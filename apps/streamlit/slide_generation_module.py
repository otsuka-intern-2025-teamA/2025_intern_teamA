# slide_generation_module.py
# ---------------------------------------------------------
# ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆãƒšãƒ¼ã‚¸ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼è¿½åŠ ç‰ˆ Ã— ã‚«ãƒ¼ãƒ‰UIï¼‰
# - ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ­ã‚´ï¼æ¡ˆä»¶ä¸€è¦§ã¸æˆ»ã‚‹ï¼ˆå·¦ä¸‹å›ºå®šï¼‰ï¼ä¼æ¥­åï¼ææ¡ˆä»¶æ•°ï¼å±¥æ­´å‚ç…§ä»¶æ•°ï¼å•†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠï¼ã‚¯ãƒªã‚¢
# - æœ¬æ–‡ï¼šä¸Šæ®µãƒ˜ãƒƒãƒ€ï¼ˆå·¦ï¼è¦‹å‡ºã—ï¼å³ï¼å€™è£œå–å¾—ãƒœã‚¿ãƒ³ï¼‰ã€
#          1æ®µç›®ï¼å•†è«‡è©³ç´°ï¼ˆå¤§ï¼‰ï¼†å‚è€ƒè³‡æ–™ï¼ˆæ¨ªä¸¦ã³ï¼‰ã€
#          2æ®µç›®ï¼å·¦ï¼šèª²é¡Œåˆ†æï¼ˆè‡ªå‹•ï¼‰ï¼å³ï¼šå€™è£œã‚«ãƒ¼ãƒ‰
#          ä¸‹æ®µï¼ç”Ÿæˆã¨ãƒ‰ãƒ©ãƒ•ãƒˆJSON
# - ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ï¼šCSVâ†’ç²—é¸å®šâ†’LLMã§Top-Ké¸æŠœâ†’LLMã§80å­—è¦ç´„ï¼ˆå¤±æ•—æ™‚ã¯çŸ­ç¸®ï¼‰
# ---------------------------------------------------------

from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import os, re, json
from datetime import datetime

import streamlit as st
import pandas as pd
import numpy as np

# å…±é€šã‚¹ã‚¿ã‚¤ãƒ«
from lib.styles import (
    apply_main_styles,
    apply_title_styles,
    apply_company_analysis_page_styles,   # ã‚µã‚¤ãƒ‰ãƒãƒ¼åœ§ç¸®/ãƒ­ã‚´ã‚«ãƒ¼ãƒ‰/ä¸‹å¯„ã›CSSã‚’æµç”¨
    apply_slide_generation_page_styles,
    render_sidebar_logo_card,
    render_slide_generation_title,        # ã‚¿ã‚¤ãƒˆãƒ«æç”»ï¼ˆh1.slide-generation-titleï¼‰
)

# APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆä¼æ¥­åˆ†æã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´å–å¾—ã«ä½¿ç”¨ï¼‰
from lib.api import get_api_client, api_available, APIError

# ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from lib.slide_generator import SlideGenerator

# LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆAzure / OpenAI ã©ã¡ã‚‰ã§ã‚‚OKï¼‰
from openai import OpenAI, AzureOpenAI

# ç”»åƒ/ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
PROJECT_ROOT = Path(__file__).parent.parent.parent
LOGO_PATH = PROJECT_ROOT / "data" / "images" / "otsuka_logo.jpg"
ICON_PATH = PROJECT_ROOT / "data" / "images" / "otsuka_icon.png"
PRODUCTS_DIR = PROJECT_ROOT / "data" / "csv" / "products"
PLACEHOLDER_IMG = PROJECT_ROOT / "data" / "images" / "product_placeholder.png"

# --- ã‚¹ã‚¿ã‚¤ãƒ« / ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆæ—¢å­˜ã®è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«åˆã‚ã›ã¦ï¼‰
from lib.api import api_available, get_api_client
from lib.new_slide_generator import NewSlideGenerator
from lib.styles import (
    apply_company_analysis_page_styles,
    apply_main_styles,
    apply_slide_generation_page_styles,
    apply_title_styles,
    render_sidebar_logo_card,
    render_slide_generation_title,
)


# =========================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
# =========================
def _ensure_session_defaults() -> None:
    ss = st.session_state
    ss.setdefault("selected_project", None)       # æ¡ˆä»¶ä¸€è¦§ã‹ã‚‰é·ç§»æ™‚ã«å…¥ã‚‹
    ss.setdefault("api_error", None)
    ss.setdefault("slide_meeting_notes", "")
    ss.setdefault("uploaded_files_store", [])     # file_uploaderã®ä¿å­˜ç”¨
    ss.setdefault("product_candidates", [])       # è¡¨ç¤ºç”¨ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®é…åˆ—
    ss.setdefault("slide_outline", None)
    ss.setdefault("slide_overview", "")
    ss.setdefault("slide_history_reference_count", 3)  # ç›´è¿‘Nå¾€å¾©å‚ç…§ï¼ˆãƒ‡ãƒ•ã‚©3ï¼‰
    ss.setdefault("slide_top_k", 10)                   # ææ¡ˆä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©10ï¼‰
    ss.setdefault("slide_products_dataset", "Auto")    # å•†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
    ss.setdefault("slide_use_tavily_api", True)        # TAVILY APIä½¿ç”¨ãƒ•ãƒ©ã‚°
    ss.setdefault("slide_use_gpt_api", True)           # GPT APIä½¿ç”¨ãƒ•ãƒ©ã‚°
    ss.setdefault("slide_tavily_uses", 2)              # è£½å“ã‚ãŸã‚Šã®TAVILY APIå‘¼ã³å‡ºã—å›æ•°
    # åŸ‹ã‚è¾¼ã¿æ¤œç´¢ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    ss.setdefault("_emb_cache", {})
    # è¡¨ç¤ºç”¨ï¼šèª²é¡Œåˆ†æçµæœï¼ˆUIã§è¦‹ã›ã‚‹ã ã‘ã€‚é¸å®šãƒ­ã‚¸ãƒƒã‚¯ã¯å¾“æ¥é€šã‚Šï¼‰
    ss.setdefault("analyzed_issues", [])


# =========================
# LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæº–å‚™ï¼ˆAzure / OpenAI è‡ªå‹•åˆ¤å®šï¼‰
# =========================
def _get_chat_client():
    """
    - Azure: USE_AZURE=true or AZURE_OPENAI_ENDPOINT ãŒã‚ã‚Œã°ä½¿ç”¨
      å¿…é ˆ: AZURE_OPENAI_API_KEY, AZURE_OPENAI_CHAT_DEPLOYMENT
      ä»»æ„: API_VERSION (default 2024-06-01)
    - OpenAI: OPENAI_API_KEY, DEFAULT_MODEL (ä»»æ„ãƒ»æ—¢å®š gpt-4o-mini)
    """
    use_azure = os.getenv("USE_AZURE", "").lower() == "true" or bool(os.getenv("AZURE_OPENAI_ENDPOINT"))
    if use_azure:
        endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        api_key = os.getenv("AZURE_OPENAI_API_KEY")
        api_version = os.getenv("API_VERSION", "2024-06-01")
        deployment = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")
        if not (endpoint and api_key and deployment):
            raise RuntimeError("Azureè¨­å®šä¸è¶³: AZURE_OPENAI_ENDPOINT / AZURE_OPENAI_API_KEY / AZURE_OPENAI_CHAT_DEPLOYMENT")
        client = AzureOpenAI(azure_endpoint=endpoint, api_key=api_key, api_version=api_version)
        model = deployment
    else:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚")
        client = OpenAI(api_key=api_key)
        model = os.getenv("DEFAULT_MODEL", "gpt-4o-mini")
    return client, model


# =========================
# ä¾¿åˆ©é–¢æ•°ï¼ˆä¾¡æ ¼ãªã©ï¼‰
# =========================
def _to_float(val):
    if val is None:
        return None
    if isinstance(val, (int, float)):
        if isinstance(val, float) and pd.isna(val):
            return None
        return float(val)
    if isinstance(val, str):
        s = val.strip().replace("Â¥", "").replace(",", "")
        if not s:
            return None
        try:
            return float(s)
        except Exception:
            return None
    return None


def _fmt_price(val) -> str:
    v = _to_float(val)
    return f"Â¥{int(round(v)):,}" if v is not None else "â€”"


# =========================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåé›†
# =========================
def _list_product_datasets() -> List[str]:
    """productsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€åã‚’åˆ—æŒ™ï¼ˆAutoã‚’å…ˆé ­ï¼‰"""
    if not PRODUCTS_DIR.exists():
        return ["Auto"]
    ds = ["Auto"]
    for p in PRODUCTS_DIR.iterdir():
        if p.is_dir():
            ds.append(p.name)
    return ds


def _gather_messages_context(item_id: Optional[str], history_n: int) -> str:
    """ä¼æ¥­åˆ†æã®ç›´è¿‘Nå¾€å¾©ï¼ˆ=2Nç™ºè¨€ï¼‰ã‚’ã¾ã¨ã‚ã¦æ–‡å­—åˆ—åŒ–"""
    if not (item_id and api_available()):
        return ""
    try:
        api = get_api_client()
        msgs = api.get_item_messages(item_id) or []
        take = min(len(msgs), history_n * 2)
        recent = msgs[-take:] if take > 0 else []
        ctx_lines = []
        for m in recent:
            role = m.get("role", "assistant")
            role_j = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if role == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
            ctx_lines.append(f"{role_j}: {m.get('content','')}")
        return "\n".join(ctx_lines)
    except Exception:
        return ""


def _load_products_from_csv(dataset: str) -> pd.DataFrame:
    """
    å•†æCSVã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆå­˜åœ¨ã‚«ãƒ©ãƒ ãŒç„¡ã‘ã‚Œã°ä½œæˆï¼‰
    æœŸå¾…ã‚«ãƒ©ãƒ : id(ç„¡ã‘ã‚Œã°ç”Ÿæˆ), name, category, price, description, tags
             ï¼‹ image_url/image/thumbnailï¼ˆä»»æ„ï¼‰, source_csvï¼ˆè¿½åŠ ï¼‰
    """
    frames: List[pd.DataFrame] = []
    if not PRODUCTS_DIR.exists():
        return pd.DataFrame()

    def _read_csvs(folder: Path):
        for csvp in folder.glob("*.csv"):
            try:
                df = pd.read_csv(csvp)
                for col in ["name", "category", "price", "description", "tags"]:
                    if col not in df.columns:
                        df[col] = None
                for col in ["image_url", "image", "thumbnail"]:
                    if col not in df.columns:
                        df[col] = None
                if "id" not in df.columns:
                    df["id"] = [f"{csvp.stem}-{i+1}" for i in range(len(df))]
                df["source_csv"] = csvp.stem
                frames.append(df[["id", "name", "category", "price", "description", "tags",
                                  "image_url", "image", "thumbnail", "source_csv"]])
            except Exception:
                continue

    if dataset == "Auto":
        for sub in PRODUCTS_DIR.iterdir():
            if sub.is_dir():
                _read_csvs(sub)
        _read_csvs(PRODUCTS_DIR)
    else:
        target = PRODUCTS_DIR / dataset
        if target.exists() and target.is_dir():
            _read_csvs(target)

    if not frames:
        return pd.DataFrame()
    return pd.concat(frames, ignore_index=True)


# =========================
# å‚è€ƒè³‡æ–™ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã€è¿½åŠ ã€‘
# =========================
def _extract_text_from_uploads(uploaded_files: List[Any], max_chars: int = 12000) -> str:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦é€£çµã—ã¦è¿”ã™ï¼ˆãƒ­ãƒã‚¹ãƒˆã«å‹•ãç°¡æ˜“å®Ÿè£…ï¼‰ã€‚
    - PDF: pypdf
    - DOCX: python-docx
    - PPTX: python-pptx
    - CSV: pandasã§å…ˆé ­æ•°è¡Œ
    - TXT: ãã®ã¾ã¾
    å¤±æ•—æ™‚ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®è¨˜éŒ²ã®ã¿ã€‚ç”»åƒ/OCRã¯æœªå¯¾å¿œã€‚
    """
    if not uploaded_files:
        return ""

    chunks: List[str] = []
    used_chars = 0

    def _append(text: str):
        nonlocal used_chars
        if not text:
            return
        remain = max_chars - used_chars
        if remain <= 0:
            return
        cut = text[:remain]
        chunks.append(cut)
        used_chars += len(cut)

    for f in uploaded_files:
        try:
            name = getattr(f, "name", "uploaded_file")
            lower = str(name).lower()
            # èª­ã¿å‡ºã—ä½ç½®ã‚’åˆæœŸåŒ–
            try:
                f.seek(0)
            except Exception:
                pass
            data = f.read()
            try:
                f.seek(0)
            except Exception:
                pass

            if lower.endswith(".pdf"):
                try:
                    from pypdf import PdfReader  # pip install pypdf
                    import io
                    reader = PdfReader(io.BytesIO(data))
                    page_limit = min(len(reader.pages), 30)
                    texts = []
                    for i in range(page_limit):
                        try:
                            texts.append(reader.pages[i].extract_text() or "")
                        except Exception:
                            continue
                    _append(f"\n[PDF:{name} æŠœç²‹]\n" + "\n".join(texts))
                except Exception:
                    _append(f"\n[PDF:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".docx"):
                try:
                    import io
                    from docx import Document  # pip install python-docx
                    doc = Document(io.BytesIO(data))
                    paras = [p.text for p in doc.paragraphs if p.text]
                    _append(f"\n[DOCX:{name} æŠœç²‹]\n" + "\n".join(paras))
                except Exception:
                    _append(f"\n[DOCX:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".pptx"):
                try:
                    import io
                    from pptx import Presentation  # pip install python-pptx
                    prs = Presentation(io.BytesIO(data))
                    slide_texts = []
                    for s in prs.slides:
                        buf = []
                        for shp in s.shapes:
                            try:
                                if hasattr(shp, "text"):
                                    t = shp.text or ""
                                    if t:
                                        buf.append(t)
                            except Exception:
                                continue
                        if buf:
                            slide_texts.append("\n".join(buf))
                    _append(f"\n[PPTX:{name} æŠœç²‹]\n" + "\n---\n".join(slide_texts))
                except Exception:
                    _append(f"\n[PPTX:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".csv"):
                try:
                    import io
                    tmp = io.BytesIO(data)
                    df = pd.read_csv(tmp)
                    head = df.head(20)
                    txt = head.to_csv(index=False)
                    _append(f"\n[CSV:{name} å…ˆé ­20è¡Œ]\n{txt}")
                except Exception:
                    _append(f"\n[CSV:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".txt"):
                try:
                    txt = data.decode("utf-8", errors="ignore")
                except Exception:
                    txt = str(data[:4000])
                _append(f"\n[TXT:{name}]\n{txt}")

            else:
                _append(f"\n[{name}]ï¼ˆæœªå¯¾å¿œ/ãƒã‚¤ãƒŠãƒªã®ãŸã‚æ¦‚è¦åæ˜ ã®ã¿ï¼‰")

        except Exception:
            continue

        if used_chars >= max_chars:
            break

    return "\n".join(chunks).strip()


# =========================
# æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç°¡æ˜“ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼‰
# =========================
def _simple_tokenize(text: str) -> List[str]:
    text = str(text or "").lower()
    text = re.sub(r"[^a-z0-9\u3040-\u30ff\u4e00-\u9fff]+", " ", text)
    toks = text.split()
    return [t for t in toks if len(t) >= 2]


def _fallback_rank_products(
    notes: str,
    messages_ctx: str,
    products_df: pd.DataFrame,
    top_pool: int
) -> List[Dict[str, Any]]:
    """å•†è«‡ãƒ¡ãƒ¢ï¼‹å±¥æ­´ã®èªå¥ä¸€è‡´ã§ç´ æœ´ã«ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° â†’ ä¸Šä½ top_pool ã‚’è¿”ã™"""
    if products_df.empty:
        return []

    query_text = (notes or "") + "\n" + (messages_ctx or "")
    q_tokens = _simple_tokenize(query_text)

    def _row_text(row) -> str:
        return " ".join([
            str(row.get("name") or ""),
            str(row.get("category") or ""),
            str(row.get("description") or ""),
            str(row.get("tags") or ""),
        ]).lower()

    scored: List[Tuple[float, Dict[str, Any]]] = []
    for _, row in products_df.iterrows():
        t = _row_text(row)
        score = 0.0
        for tok in q_tokens:
            if tok in t:
                score += 1.0
        reason = f"ä¸€è‡´èªå¥æ•°={int(score)}" if score > 0 else "ä¸€è‡´ãªã—ï¼ˆä½ã‚¹ã‚³ã‚¢ï¼‰"
        scored.append((score, {
            "id": row.get("id"),
            "name": row.get("name"),
            "category": row.get("category"),
            "price": row.get("price"),
            "description": row.get("description"),
            "tags": row.get("tags"),
            "image_url": row.get("image_url"),
            "image": row.get("image"),
            "thumbnail": row.get("thumbnail"),
            "source_csv": row.get("source_csv"),
            "score": round(float(score), 2),
            "reason": reason,
        }))

    scored.sort(key=lambda x: (x[0], str(x[1]["name"]).lower()), reverse=True)
    return [d for _, d in scored[:top_pool]]


# =========================
# LLM ã§ Top-K é¸æŠœï¼‹ç†ç”±ç”Ÿæˆ / 80å­—è¦ç´„
# =========================
def _extract_json(s: str) -> Dict[str, Any]:
    s = (s or "").strip()
    if not s:
        return {}
    if s.lstrip().startswith("{"):
        try:
            return json.loads(s)
        except Exception:
            pass
    try:
        start = s.find("{")
        end = s.rfind("}")
        if start >= 0 and end > start:
            return json.loads(s[start:end + 1])
    except Exception:
        return {}
    return {}


def _llm_pick_products(pool: List[Dict[str, Any]], top_k: int, company: str, notes: str, ctx: str, issues: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
    """
    ã‚«ã‚¿ãƒ­ã‚°ï¼ˆpoolï¼‰ã‹ã‚‰ LLM ã§ Top-K ã‚’é¸æŠœã—ã€çŸ­ã„ç†ç”±ã¨ä¿¡é ¼åº¦ã‚’ä»˜ä¸ã€‚
    issues ãŒä¸ãˆã‚‰ã‚Œã‚Œã°ã€èª²é¡ŒIDã¨ã®å¯¾å¿œä»˜ã‘ã¨æ ¹æ‹ ã‚’æ±‚ã‚ã‚‹ã€‚
    """
    if not pool:
        return []
    client, model = _get_chat_client()

    lines = []
    for p in pool:
        desc = (p.get("description") or "")[:200]
        tags = (p.get("tags") or "")[:120]
        cat = p.get("source_csv") or p.get("category") or ""
        price = p.get("price")
        price_s = f"Â¥{int(_to_float(price)):,}" if _to_float(price) is not None else "â€”"
        lines.append(f"- id:{p['id']} | name:{p.get('name','')} | category:{cat} | price:{price_s} | tags:{tags} | desc:{desc}")
    catalog = "\n".join(lines)
    # èª²é¡Œãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—åŒ–
    issues_text = ""
    if issues:
        parts = []
        for i, it in enumerate(issues):
            kw = ", ".join(it.get("keywords") or [])
            parts.append(f"[{i}] {it.get('issue')} (é‡ã¿={it.get('weight'):.2f}; ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰={kw})")
        issues_text = "\n".join(parts)
    # JSONã‚¹ã‚­ãƒ¼ãƒ
    schema = {
        "recommendations": [
            {
                "id": "<id>",
                "reason": "<120å­—ä»¥å†…>",
                "confidence": 0.0,
            }
        ]
    }
    if issues:
        schema = {
            "recommendations": [
                {
                    "id": "<id>",
                    "reason": "<120å­—ä»¥å†…>",
                    "confidence": 0.0,
                    "solved_issue_ids": [0],
                    "evidence": "<æ ¹æ‹ æŠœç²‹>"
                }
            ]
        }
    user = f"""ã‚ãªãŸã¯B2Bãƒ—ãƒªã‚»ãƒ¼ãƒ«ã‚¹ã®ææ¡ˆãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ä¼šç¤¾æƒ…å ±ã¨å•†è«‡è©³ç´°ã€ä¼šè©±æ–‡è„ˆã«åŸºã¥ã„ã¦ã€å€™è£œã‚«ã‚¿ãƒ­ã‚°ã‹ã‚‰ Top-{top_k} ã®è£½å“ã‚’é¸ã³ã€æ—¥æœ¬èªã§çŸ­ã„ç†ç”±ï¼ˆ120å­—ä»¥å†…ï¼‰ã¨ä¿¡é ¼åº¦(0-1)ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚
å¿…ãšã‚«ã‚¿ãƒ­ã‚°ã«å­˜åœ¨ã™ã‚‹ id ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯ JSON ã®ã¿ã§ã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ¼ãƒã«å¾“ã£ã¦ãã ã•ã„:
{json.dumps(schema, ensure_ascii=False)}

# ä¼šç¤¾: {company or "(ãªã—)"}
# å•†è«‡è©³ç´°:
{notes or "(ãªã—)"}

# ä¼šè©±æ–‡è„ˆ:
{ctx or "(ãªã—)"}

# èª²é¡Œä¸€è¦§:
{issues_text or "(ãªã—)"}

# å€™è£œã‚«ã‚¿ãƒ­ã‚°:
{catalog}
"""
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ­£ç¢ºã§ç°¡æ½”ãªæ—¥æœ¬èªã§å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": user},
            ],
            response_format={"type": "json_object"},
        )
        txt = resp.choices[0].message.content or ""
    except Exception:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ­£ç¢ºã§ç°¡æ½”ãªæ—¥æœ¬èªã§å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": user},
            ],
        )
        txt = resp.choices[0].message.content or ""

    data = _extract_json(txt)
    recs = data.get("recommendations", []) if isinstance(data, dict) else []
    if not recs:
        return []
    pool_map = {str(p["id"]): p for p in pool}
    out: List[Dict[str, Any]] = []
    for r in recs:
        pid = str(r.get("id", "")).strip()
        if not pid or pid not in pool_map:
            continue
        src = pool_map[pid]
        # reason / confidence
        reason = (r.get("reason") or "").strip() or src.get("reason")
        conf = float(r.get("confidence", 0.0)) if r.get("confidence") is not None else float(src.get("score", 0.0))
        solved_ids = r.get("solved_issue_ids") if isinstance(r.get("solved_issue_ids"), list) else []
        evidence = (r.get("evidence") or "").strip()
        out.append({
            **src,
            "reason": reason,
            "score": conf,
            "solved_issue_ids": solved_ids,
            "evidence": evidence,
        })
        if len(out) >= top_k:
            break
    return out


def _summarize_overviews_llm(cands: List[Dict[str, Any]]) -> None:
    """å„è£½å“ã®æ¦‚è¦ã‚’ 80å­—ä»¥å†…ã§ LLM è¦ç´„ï¼ˆå¤±æ•—æ™‚ã¯èª¬æ˜ã‚’çŸ­ç¸®ï¼‰"""
    items = []
    has_any = False
    for c in cands:
        mat = c.get("description") or c.get("tags") or c.get("name") or ""
        if mat:
            has_any = True
        items.append({"id": str(c.get("id") or ""), "name": c.get("name") or "", "material": str(mat)[:600]})

    if not has_any:
        for c in cands:
            c["overview"] = "â€”"
        return

    try:
        client, model = _get_chat_client()
        payload = "\n".join([f"- id:{it['id']} / åç§°:{it['name']}\n  å†…å®¹:{it['material']}" for it in items])
        prompt = f"""å„è£½å“ã®ã€Œè£½å“æ¦‚è¦ã€ã‚’æ—¥æœ¬èªã§1ã€œ2æ–‡ã€æœ€å¤§80å­—ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚äº‹å®Ÿã®è¿½åŠ ãƒ»èª‡å¼µã¯ç¦æ­¢ã€‚
å‡ºåŠ›ã¯ JSON ã®ã¿:
{{"summaries":[{{"id":"<id>","overview":"<80å­—ä»¥å†…>"}}]}}
å…¥åŠ›:
{payload}"""
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ç°¡æ½”ã§æ­£ç¢ºãªæ—¥æœ¬èªã®è¦ç´„ã‚’ä½œã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                response_format={"type": "json_object"},
            )
            txt = resp.choices[0].message.content or ""
        except Exception:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ç°¡æ½”ã§æ­£ç¢ºãªæ—¥æœ¬èªã®è¦ç´„ã‚’ä½œã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": prompt},
                ],
            )
            txt = resp.choices[0].message.content or ""

        data = _extract_json(txt)
        mp = {}
        if isinstance(data, dict):
            for s in data.get("summaries", []) or []:
                pid = str(s.get("id") or "")
                ov = (s.get("overview") or "").strip()
                if pid and ov:
                    mp[pid] = ov

        for c in cands:
            pid = str(c.get("id") or "")
            base = c.get("description") or c.get("tags") or ""
            fallback = (base[:80] + ("â€¦" if base and len(base) > 80 else "")) if base else "â€”"
            c["overview"] = mp.get(pid, fallback)
    except Exception:
        for c in cands:
            base = c.get("description") or c.get("tags") or ""
            c["overview"] = (base[:80] + ("â€¦" if base and len(base) > 80 else "")) if base else "â€”"


def _resolve_product_image_src(rec: Dict[str, Any]) -> Optional[str]:
    for key in ("image_url", "image", "thumbnail"):
        v = rec.get(key)
        if not v:
            continue
        s = str(v).strip()
        if s.startswith("http://") or s.startswith("https://"):
            return s
        p = (PROJECT_ROOT / s).resolve()
        if p.exists():
            return str(p)
    if PLACEHOLDER_IMG.exists():
        return str(PLACEHOLDER_IMG)
    return None

# =========================
# è¿½åŠ : èª²é¡Œåˆ†æãƒ»åŸ‹ã‚è¾¼ã¿ç´¢å¼•ãƒ»é¡ä¼¼æ¤œç´¢
# =========================

def _analyze_pain_points(notes: str, messages_ctx: str, uploads_text: str = "") -> List[Dict[str, Any]]:
    """
    å•†è«‡ãƒ¡ãƒ¢ãƒ»ä¼šè©±æ–‡è„ˆãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ï¼ˆæŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‹ã‚‰èª²é¡Œã‚’æŠ½å‡ºã€‚
    è¿”ã‚Šå€¤: [{"issue": str, "weight": float, "keywords": List[str]}, ...]
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯æ±ç”¨çš„ãªèª²é¡Œã‚’è¿”ã™ã€‚
    """
    issues: List[Dict[str, Any]] = []
    try:
        client, chat_model = _get_chat_client()
        sys = "ã‚ãªãŸã¯B2Bææ¡ˆã®èª²é¡Œåˆ†æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬èªã§JSONã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"

        uploads_section = f"\n\nè³‡æ–™æŠœç²‹:\n{uploads_text}" if uploads_text else ""

        user = f"""ä»¥ä¸‹ã®æƒ…å ±ã‹ã‚‰ã€è§£æ±ºã—ãŸã„èª²é¡Œã‚’3ã€œ6ä»¶æŠ½å‡ºã—ã€å„èª²é¡Œã«é‡ã¿(0ã€œ1)ã¨é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(3ã€œ6èª)ã‚’ä»˜ã‘ã¦JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
- èª²é¡Œã¯å…·ä½“çš„ã«è¡¨ç¾ã™ã‚‹
- é‡ã¿ã¯åˆè¨ˆãŒç´„1ã«ãªã‚‹ã‚ˆã†ç›¸å¯¾èª¿æ•´
- å¼•ç”¨å¯èƒ½ãªã‚‰è³‡æ–™ç”±æ¥ã®è¦³ç‚¹ã‚‚åæ˜ ï¼ˆãŸã ã—æ©Ÿå¯†ã‚„å€‹äººæƒ…å ±ã¯æŠ½è±¡åŒ–ï¼‰

å•†è«‡ãƒ¡ãƒ¢:
{notes}

ä¼šè©±æ–‡è„ˆ:
{messages_ctx}{uploads_section}
"""
        resp = client.chat.completions.create(
            model=chat_model,
            messages=[
                {"role": "system", "content": sys},
                {"role": "user", "content": user},
            ],
            response_format={"type": "json_object"},
            temperature=0.2,
        )
        txt = resp.choices[0].message.content or ""
        data = _extract_json(txt)
        cand = data.get("issues") if isinstance(data, dict) else data
        if isinstance(cand, list):
            for it in cand[:6]:
                issue = str(it.get("issue") or "").strip()
                if not issue:
                    continue
                weight = float(it.get("weight", 0.0))
                keywords = [str(k).strip() for k in (it.get("keywords") or []) if str(k).strip()]
                issues.append({"issue": issue[:80], "weight": max(0.0, min(1.0, weight)), "keywords": keywords[:6]})
    except Exception:
        pass
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not issues:
        issues = [
            {"issue": "æƒ…å ±å…±æœ‰ã®æ”¹å–„", "weight": 0.34, "keywords": ["ãƒŠãƒ¬ãƒƒã‚¸å…±æœ‰", "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"]},
            {"issue": "ã‚³ã‚¹ãƒˆæœ€é©åŒ–", "weight": 0.33, "keywords": ["è²»ç”¨å‰Šæ¸›", "åŠ¹ç‡åŒ–", "è‡ªå‹•åŒ–"]},
            {"issue": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–", "weight": 0.33, "keywords": ["ã‚¢ã‚¯ã‚»ã‚¹ç®¡ç†", "ç›£æŸ»", "æ¨©é™"]},
        ]
    # æ­£è¦åŒ–
    s = sum(x["weight"] for x in issues) or 1.0
    for x in issues:
        x["weight"] = float(x["weight"] / s)
    return issues


def _normalize_concat_row(row: pd.Series) -> str:
    """name, category, tags, description ã‚’é€£çµã—ã¦æ­£è¦åŒ–"""
    s = f"{row.get('name','')} {row.get('category','')} {row.get('tags','')} {row.get('description','')}"
    s = s.lower()
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _embed_texts(client, texts: List[str], embed_model: str, is_azure: bool) -> np.ndarray:
    """
    Embedding API ã‚’å‘¼ã³å‡ºã—ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™ã€‚å¤±æ•—æ™‚ã¯ä¾‹å¤–ã‚’é€å‡ºã€‚
    """
    try:
        # Azure ã§ã‚‚ OpenAI ã§ã‚‚ embeddings.create ã¯åŒã˜å½¢ã§å‘¼ã¹ã‚‹
        resp = client.embeddings.create(model=embed_model, input=texts)
        vecs = np.array([d.embedding for d in resp.data], dtype="float32")
        return vecs
    except Exception as e:
        raise RuntimeError(f"embedding failed: {e}")


def _build_products_index(dataset: str, df: pd.DataFrame, client, embed_model: str, is_azure: bool) -> Dict[str, Any]:
    """
    products DataFrame ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ç´¢å¼•ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã€‚
    Cache key: dataset_name + length + embed_model
    """
    key = f"{dataset}:{len(df)}:{embed_model}"
    cache = st.session_state.get("_emb_cache", {})
    if key in cache:
        return cache[key]
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    texts = [_normalize_concat_row(row) for _, row in df.iterrows()]
    try:
        vecs = _embed_texts(client, texts, embed_model, is_azure)
        index = {"vecs": vecs, "ids": df["id"].astype(str).tolist(), "df": df, "model": embed_model}
    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: TF-IDF
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            vectorizer = TfidfVectorizer(min_df=1)
            vecs = vectorizer.fit_transform(texts)
            index = {"vecs": vecs, "ids": df["id"].astype(str).tolist(), "df": df, "model": "tfidf", "vectorizer": vectorizer}
        except Exception:
            index = {"vecs": None, "ids": [], "df": df, "model": None}
    cache[key] = index
    st.session_state._emb_cache = cache
    return index


def _retrieve_by_issues(index: Dict[str, Any], issues: List[Dict[str, Any]], client, embed_model: str, is_azure: bool, top_pool: int) -> List[Dict[str, Any]]:
    """
    èª²é¡Œã®é‡ã¿ä»˜ããƒ™ã‚¯ãƒˆãƒ«ã§é¡ä¼¼æ¤œç´¢ã—ã€ä¸Šä½ top_pool ä»¶ã‚’è¿”ã™ã€‚
    index["vecs"] ãŒ None ã¾ãŸã¯ TF-IDF ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    if not issues or not index or index.get("vecs") is None:
        return []
    vecs = index["vecs"]
    # TF-IDF ã®å ´åˆã¯é¡ä¼¼æ¤œç´¢ã‚’è¡Œã‚ãªã„
    if hasattr(vecs, "toarray") or index.get("model") == "tfidf":
        return []
    # ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«
    queries = [f"{it['issue']} {' '.join(it.get('keywords') or [])}".strip() for it in issues]
    weights = np.array([float(it.get("weight", 0.0)) for it in issues], dtype="float32")
    try:
        q_embs = _embed_texts(client, queries, embed_model, is_azure)
        q = (weights[:, None] * q_embs).sum(axis=0, keepdims=True)
        # æ­£è¦åŒ–
        v_norm = vecs / (np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-9)
        q_norm = q / (np.linalg.norm(q, axis=1, keepdims=True) + 1e-9)
        sims = np.dot(q_norm, v_norm.T).ravel()
        order = sims.argsort()[::-1][:max(1, top_pool)]
        out = []
        for idx_pos in order:
            rid = index["ids"][idx_pos]
            row = index["df"].iloc[idx_pos].to_dict()
            out.append({
                "id": row.get("id"),
                "name": row.get("name"),
                "category": row.get("category"),
                "price": row.get("price"),
                "description": row.get("description"),
                "tags": row.get("tags"),
                "image_url": row.get("image_url"),
                "image": row.get("image"),
                "thumbnail": row.get("thumbnail"),
                "source_csv": row.get("source_csv"),
                "score": float(sims[idx_pos]),
                "reason": f"èª²é¡Œã¨é«˜é¡ä¼¼ ({sims[idx_pos]:.3f})",
            })
        return out
    except Exception:
        return []


# =========================
# å€™è£œæ¤œç´¢ï¼ˆCSVâ†’ç²—é¸å®šâ†’LLMé¸æŠœâ†’LLMè¦ç´„ï¼‰
# =========================
def _search_product_candidates(
    company: str,
    item_id: Optional[str],
    meeting_notes: str,
    top_k: int,
    history_n: int,
    dataset: str,
    uploaded_files: List[Any],   # ã“ã“ã‚’æ´»ç”¨ï¼ˆè³‡æ–™ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼‰
) -> List[Dict[str, Any]]:
    # ä¼æ¥­åˆ†æã®æ–‡è„ˆ
    ctx = _gather_messages_context(item_id, history_n)

    # CSV èª­ã¿è¾¼ã¿
    df = _load_products_from_csv(dataset)
    if df.empty:
        return []

    # â˜… è¿½åŠ ï¼šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    uploads_text = _extract_text_from_uploads(uploaded_files) if uploaded_files else ""

    # â˜† å¤‰æ›´ï¼šèª²é¡ŒæŠ½å‡ºã« uploads_text ã‚’æ¸¡ã™
    issues = _analyze_pain_points(meeting_notes or "", ctx or "", uploads_text)

    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¨ç’°å¢ƒåˆ¤å®š
    use_azure = os.getenv("USE_AZURE", "").lower() == "true" or bool(os.getenv("AZURE_OPENAI_ENDPOINT"))
    embed_model = os.getenv("AZURE_OPENAI_EMBED_DEPLOYMENT") if use_azure else os.getenv("EMBED_MODEL", "text-embedding-3-small")

    # LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå–å¾—
    client, _ = _get_chat_client()

    # åŸ‹ã‚è¾¼ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    index = _build_products_index(dataset, df, client, embed_model, use_azure)

    # èª²é¡Œãƒ‰ãƒªãƒ–ãƒ³ç²—å€™è£œæ¤œç´¢ï¼ˆTop-poolï¼‰
    top_pool = max(40, top_k * 4)
    pool = _retrieve_by_issues(index, issues, client, embed_model, use_azure, top_pool)
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šèªå¥ä¸€è‡´ã§ç²—é¸å®š
    if not pool:
        pool = _fallback_rank_products(meeting_notes, ctx, df, top_pool=max(40, top_k * 3))

    # LLM ã§ç²¾é¸ï¼ˆèª²é¡Œä»˜ï¼‰
    try:
        selected = _llm_pick_products(pool, top_k, company, meeting_notes, ctx, issues)
        if not selected:
            selected = pool[:top_k]
    except Exception:
        selected = pool[:top_k]

    # å„è£½å“ã® 80å­—æ¦‚è¦ã‚’ LLM è¦ç´„ï¼ˆå¤±æ•—æ™‚ã¯çŸ­ç¸®ï¼‰
    _summarize_overviews_llm(selected)
    return selected


# =========================
# ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆ
# =========================
def _make_outline_preview(company: str, meeting_notes: str, selected_products: List[Dict[str, Any]], overview: str) -> Dict[str, Any]:
    return {
        "title": f"{company} å‘ã‘ææ¡ˆè³‡æ–™ï¼ˆãƒ‰ãƒ©ãƒ•ãƒˆï¼‰",
        "overview": overview,
        "sections": [
            {"h2": "1. ã‚¢ã‚¸ã‚§ãƒ³ãƒ€", "bullets": ["èƒŒæ™¯", "èª²é¡Œæ•´ç†", "ææ¡ˆæ¦‚è¦", "å°å…¥åŠ¹æœ", "å°å…¥è¨ˆç”»", "æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"]},
            {"h2": "2. èƒŒæ™¯", "bullets": [f"{company}ã®äº‹æ¥­æ¦‚è¦ï¼ˆè¦ç´„ï¼‰", "å¸‚å ´å‹•å‘ãƒ»ç«¶åˆçŠ¶æ³ï¼ˆæŠœç²‹ï¼‰"]},
            {"h2": "3. ç¾çŠ¶ã®èª²é¡Œï¼ˆä»®èª¬ï¼‰", "bullets": ["ç”Ÿç”£æ€§/ã‚³ã‚¹ãƒˆ/å“è³ª/ã‚¹ãƒ”ãƒ¼ãƒ‰ã®è¦³ç‚¹ã‹ã‚‰3ã€œ5ç‚¹"]},
            {"h2": "4. ææ¡ˆæ¦‚è¦", "bullets": ["æœ¬ææ¡ˆã®ç›®çš„ï¼ç‹™ã„", "å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆé«˜ãƒ¬ãƒ™ãƒ«ï¼‰"]},
            {"h2": "5. æ¨å¥¨ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå€™è£œï¼‰", "bullets": [f"{len(selected_products)}ä»¶ã®å•†æå€™è£œã‚’æ•´ç†ãƒ»æ¯”è¼ƒ"]},
            {"h2": "6. å°å…¥åŠ¹æœï¼ˆå®šé‡/å®šæ€§ï¼‰", "bullets": ["KPIè¦‹è¾¼ã¿ / åŠ¹æœè©¦ç®—ã®æ–¹é‡"]},
            {"h2": "7. å°å…¥ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ¡ˆ", "bullets": ["PoC â†’ æœ¬å°å…¥ / ä½“åˆ¶ãƒ»å½¹å‰²åˆ†æ‹…"]},
        ],
        "meeting_notes_digest": meeting_notes[:300] + ("..." if len(meeting_notes) > 300 else ""),
        "products": [
            {
                "id": p.get("id"),
                "name": p.get("name"),
                "category": p.get("source_csv") or p.get("category"),
                "price": p.get("price"),
                "reason": p.get("reason"),
                "overview": p.get("overview"),
            }
            for p in selected_products
        ],
    }


# =========================
# ãƒ¡ã‚¤ãƒ³æç”»ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆæ”¹ä¿®ã®ã¿ï¼‰
# =========================
def render_slide_generation_page():
    """ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆãƒšãƒ¼ã‚¸ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆé…ç½®ã‚’å¤‰æ›´ï¼šä¸Šæ®µï¼å•†è«‡è©³ç´°ï¼†è³‡æ–™ã€ä¸‹æ®µï¼èª²é¡Œåˆ†æï¼†å€™è£œã‚«ãƒ¼ãƒ‰ï¼‰"""
    _ensure_session_defaults()

    try:
        st.set_page_config(
            page_title="ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ",
            page_icon=str(ICON_PATH),
            layout="wide",
            initial_sidebar_state="expanded",
        )
    except Exception:
        pass

    # ã‚¹ã‚¿ã‚¤ãƒ«
    apply_main_styles(hide_sidebar=False, hide_header=True)
    apply_title_styles()
    apply_company_analysis_page_styles()  # ã‚µã‚¤ãƒ‰ãƒãƒ¼å…±é€š
    apply_slide_generation_page_styles()  # ã‚¿ã‚¤ãƒˆãƒ«ä½ç½®ãªã©

    # æ¡ˆä»¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    pj = st.session_state.get("selected_project")
    if pj:
        title_text = f"ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ - {pj['title']} / {pj['company']}"
        company_internal = pj.get("company", "")
        item_id = pj.get("id")
    else:
        title_text = "ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ"
        company_internal = ""
        item_id = None

    # ---------- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ----------
    with st.sidebar:
        render_sidebar_logo_card(LOGO_PATH)

        st.markdown("### è¨­å®š")
        st.text_input("ä¼æ¥­å", value=company_internal, key="slide_company_input", disabled=True)

        st.session_state.slide_top_k = st.number_input(
            "ææ¡ˆä»¶æ•°",
            min_value=3, max_value=20, value=st.session_state.slide_top_k, step=1,
            key="slide_top_k_input",
        )

        st.session_state.slide_history_reference_count = st.selectbox(
            "å±¥æ­´å‚ç…§ä»¶æ•°ï¼ˆå¾€å¾©ï¼‰",
            options=list(range(1, 11)),
            index=max(0, st.session_state.slide_history_reference_count - 1),
            key="slide_history_count_select",
            help="ä¼æ¥­åˆ†æã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ç›´è¿‘Nå¾€å¾©ã‚’æ–‡è„ˆã¨ã—ã¦ä½¿ç”¨",
        )

        datasets = _list_product_datasets()
        st.session_state.slide_products_dataset = st.selectbox(
            "å•†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            options=datasets,
            index=datasets.index(st.session_state.slide_products_dataset) if st.session_state.slide_products_dataset in datasets else 0,
            key="slide_products_dataset_select",
            help="data/csv/products/ é…ä¸‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã€‚Autoã¯è‡ªå‹•é¸æŠã€‚",
        )

        st.markdown("---")
        st.markdown("### AIè¨­å®š")
        
        st.session_state.slide_use_gpt_api = st.checkbox(
            "GPT APIä½¿ç”¨",
            value=st.session_state.slide_use_gpt_api,
            key="slide_use_gpt_api_checkbox",
            help="Azure OpenAI GPT-5-miniã‚’ä½¿ç”¨ã—ã¦ä¼æ¥­èª²é¡Œåˆ†æã¨è£½å“æƒ…å ±è¦ç´„ã‚’è¡Œã„ã¾ã™"
        )
        
        st.session_state.slide_use_tavily_api = st.checkbox(
            "TAVILY APIä½¿ç”¨",
            value=st.session_state.slide_use_tavily_api,
            key="slide_use_tavily_api_checkbox",
            help="TAVILY APIã‚’ä½¿ç”¨ã—ã¦è£½å“æƒ…å ±ã®ã‚¦ã‚§ãƒ–æ¤œç´¢ã‚’è¡Œã„ã¾ã™"
        )
        
        if st.session_state.slide_use_tavily_api:
            st.session_state.slide_tavily_uses = st.number_input(
                "TAVILY APIå‘¼ã³å‡ºã—å›æ•°ï¼ˆè£½å“ã‚ãŸã‚Šï¼‰",
                min_value=1, max_value=5, value=st.session_state.slide_tavily_uses, step=1,
                key="slide_tavily_uses_input",
                help="å„è£½å“ã«å¯¾ã—ã¦TAVILY APIã‚’ä½•å›å‘¼ã³å‡ºã™ã‹ã‚’æŒ‡å®šã—ã¾ã™"
            )

        sidebar_clear = st.button("ã‚¯ãƒªã‚¢", use_container_width=True, help="å€™è£œã‚’ç”»é¢å†…ã§ã‚¯ãƒªã‚¢")

        st.markdown("<div class='sidebar-bottom'>", unsafe_allow_html=True)
        if st.button("â† æ¡ˆä»¶ä¸€è¦§ã«æˆ»ã‚‹", use_container_width=True):
            st.session_state.current_page = "æ¡ˆä»¶ä¸€è¦§"
            st.session_state.page_changed = True
            st.rerun()
        st.markdown("</div>", unsafe_allow_html=True)

    # ---------- ã‚¿ã‚¤ãƒˆãƒ« ----------
    render_slide_generation_title(title_text)

    # ---------- è¦‹å‡ºã—è¡Œï¼ˆå·¦ï¼è¦‹å‡ºã— / å³ï¼å€™è£œå–å¾—ãƒœã‚¿ãƒ³ï¼‰ ----------
    head_l, head_r = st.columns([8, 2])
    with head_l:
        st.subheader("1. å•†å“ææ¡ˆ")
    with head_r:
        search_btn = st.button("å€™è£œã‚’å–å¾—", use_container_width=True)

    # ====================== ä¸Šæ®µï¼šå•†è«‡è©³ç´°ï¼ˆå¤§ï¼‰ï¼†å‚è€ƒè³‡æ–™ï¼ˆæ¨ªä¸¦ã³ï¼‰ ======================
    top_l, top_r = st.columns([3, 2], gap="large")  # å•†è«‡è©³ç´°ã‚’å¤§ãã‚ã«
    with top_l:
        st.markdown("**â— å•†è«‡ã®è©³ç´°**")
        st.text_area(
            label="å•†è«‡ã®è©³ç´°",
            key="slide_meeting_notes",
            height=200,
            label_visibility="collapsed",
            placeholder="ä¾‹ï¼šæ¥æœŸã®éœ€è¦äºˆæ¸¬ç²¾åº¦å‘ä¸Šã¨åœ¨åº«æœ€é©åŒ–ã€‚PoCã‹ã‚‰æ®µéšå°å…¥â€¦ ãªã©",
        )
    with top_r:
        st.markdown("**â— å‚è€ƒè³‡æ–™**")
        uploads = st.file_uploader(
            label="å‚è€ƒè³‡æ–™ï¼ˆä»»æ„ï¼‰",
            type=["pdf", "pptx", "docx", "csv", "png", "jpg", "jpeg", "txt"],
            accept_multiple_files=True,
            key="slide_uploader",
            label_visibility="collapsed",
            help="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã¯ç‰¹å¾´æŠ½å‡º/è¦ç´„ã«åˆ©ç”¨ï¼ˆä»Šå›ã®ä¿®æ­£ã§èª²é¡ŒæŠ½å‡ºã«åæ˜ ã•ã‚Œã¾ã™ï¼‰ã€‚",
        )
        if uploads:
            st.session_state.uploaded_files_store = uploads
            st.success(f"{len(uploads)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸã€‚")
        elif st.session_state.uploaded_files_store:
            st.caption(f"å‰å›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {len(st.session_state.uploaded_files_store)} ãƒ•ã‚¡ã‚¤ãƒ«")

    # ã€Œå€™è£œã‚’å–å¾—ã€æŠ¼ä¸‹æ™‚ï¼šå¾“æ¥ã®å€™è£œæ¤œç´¢ï¼ˆãƒãƒƒã‚¯ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—ï¼‰ï¼‹ UIè¡¨ç¤ºç”¨ã®èª²é¡Œåˆ†æçµæœã‚’è¨ˆç®—
    if search_btn:
        if not company_internal.strip():
            st.error("ä¼æ¥­ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ¡ˆä»¶ä¸€è¦§ã‹ã‚‰ä¼æ¥­ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")
        else:
            with st.spinner("å€™è£œã‚’æ¤œç´¢ä¸­â€¦"):
                candidates = _search_product_candidates(
                    company=company_internal,
                    item_id=item_id,
                    meeting_notes=st.session_state.slide_meeting_notes or "",
                    top_k=int(st.session_state.slide_top_k),
                    history_n=int(st.session_state.slide_history_reference_count),
                    dataset=st.session_state.slide_products_dataset,
                    uploaded_files=st.session_state.uploaded_files_store,
                )
            st.session_state.product_candidates = candidates

            # è¡¨ç¤ºç”¨ï¼šèª²é¡Œåˆ†æï¼ˆâ€»å€™è£œé¸å®šãƒ­ã‚¸ãƒƒã‚¯ã«ã¯å½±éŸ¿ã—ãªã„ï¼‰
            ctx_for_view = _gather_messages_context(item_id, int(st.session_state.slide_history_reference_count))
            uploads_text_for_view = _extract_text_from_uploads(st.session_state.uploaded_files_store) if st.session_state.uploaded_files_store else ""
            st.session_state.analyzed_issues = _analyze_pain_points(
                st.session_state.slide_meeting_notes or "",
                ctx_for_view or "",
                uploads_text_for_view or ""
            )

    if sidebar_clear:
        st.session_state.product_candidates = []
        st.session_state.analyzed_issues = []
        st.info("å€™è£œã¨èª²é¡Œåˆ†æè¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

    st.divider()

    # ====================== ä¸‹æ®µï¼šå·¦ï¼èª²é¡Œåˆ†æï¼å³ï¼å€™è£œã‚«ãƒ¼ãƒ‰ ======================
    bottom_l, bottom_r = st.columns([5, 7], gap="large")

    with bottom_l:
        st.markdown("**â— èª²é¡Œåˆ†æï¼ˆè‡ªå‹•ï¼‰**")
        issues = st.session_state.get("analyzed_issues") or []
        if not issues:
            st.caption("ã€å€™è£œã‚’å–å¾—ã€ã‚’æŠ¼ã™ã¨ã€å•†è«‡ãƒ¡ãƒ¢ãƒ»å±¥æ­´ãƒ»å‚è€ƒè³‡æ–™ã‹ã‚‰èª²é¡Œã‚’è‡ªå‹•æŠ½å‡ºã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")
        else:
            for i, it in enumerate(issues, start=1):
                with st.container(border=True):
                    st.markdown(f"**{i}. {it.get('issue','â€”')}**")
                    st.caption(f"é‡ã¿: {it.get('weight',0):.2f}")
                    kws = it.get("keywords") or []
                    if kws:
                        st.markdown("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: " + " / ".join(kws))

    with bottom_r:
        st.markdown("**â— å€™è£œï¼ˆã‚«ãƒ¼ãƒ‰è¡¨ç¤ºï¼‰**")
        recs = st.session_state.product_candidates or []
        if not recs:
            st.info("å€™è£œãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€å€™è£œã‚’å–å¾—ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        else:
            for r in recs:
                pid = str(r.get("id") or "")
                name = str(r.get("name") or "")
                cat_src = r.get("source_csv") or r.get("category") or "â€”"
                price_s = _fmt_price(r.get("price"))
                reason = r.get("reason") or "â€”"
                overview = r.get("overview") or "â€”"

                with st.container(border=True):
                    c1, c2 = st.columns([1, 3], gap="medium")
                    with c1:
                        img_src = _resolve_product_image_src(r)
                        if img_src and (img_src.startswith("http") or os.path.exists(img_src)):
                            st.image(img_src, use_container_width=True)
                        else:
                            st.markdown(
                                "<div style='width:100%;height:120px;border:1px solid #eee;border-radius:10px;"
                                "background:#f6f7f9;display:flex;align-items:center;justify-content:center;"
                                "color:#999;font-size:12px;'>ç”»åƒãªã—</div>",
                                unsafe_allow_html=True
                            )
                    with c2:
                        st.markdown(f"**{name}**")
                        st.caption(f"ã‚«ãƒ†ã‚´ãƒª: {cat_src} ï¼ ä¾¡æ ¼: {price_s} ï¼ ID: `{pid}`")
                        st.markdown(f"**ææ¡ˆç†ç”±**ï¼š{reason}")
                        st.markdown(f"**è£½å“æ¦‚è¦**ï¼š{overview}")

    st.divider()

    # ====================== 2. ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ ======================
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±ã®è¡¨ç¤º
    if st.button("ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±ã‚’è¡¨ç¤º", help="ä½¿ç”¨ã™ã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™"):
        try:
            generator = NewSlideGenerator()
            template_info = generator.get_template_info()
            st.json(template_info)
        except Exception as e:
            st.error(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±ã®å–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    st.subheader("2. ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ")

    row_l, row_r = st.columns([8, 2], vertical_alignment="center")
    with row_l:
        st.session_state.slide_overview = st.text_input(
            "æ¦‚èª¬ï¼ˆä»»æ„ï¼‰",
            value=st.session_state.slide_overview or "",
            placeholder="ä¾‹ï¼šåœ¨åº«æœ€é©åŒ–ã‚’ä¸­å¿ƒã«ã€éœ€è¦äºˆæ¸¬ã¨è£œå……è¨ˆç”»ã®é€£æºã‚’ææ¡ˆâ€¦",
        )
    with row_r:
        gen_btn = st.button("ç”Ÿæˆ", type="primary", use_container_width=True)

    if gen_btn:
        if not company_internal.strip():
            st.error("ä¼æ¥­ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        elif not st.session_state.product_candidates:
            st.error("è£½å“å€™è£œãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã€Œå€™è£œã‚’å–å¾—ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        else:
            selected = list(st.session_state.product_candidates or [])  # å…¨å€™è£œã‚’æ¡ç”¨
            
            # ä¸‹æ›¸ãã®ä½œæˆ
            outline = _make_outline_preview(
                company_internal,
                st.session_state.slide_meeting_notes or "",
                selected,
                st.session_state.slide_overview or "",
            )
            st.session_state.slide_outline = outline
            
            # ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
            with st.spinner("AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆä¸­..."):
                try:
                    print(f"ğŸš€ Streamlit: ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆé–‹å§‹")
                    print(f"  ä¼æ¥­å: {company_internal}")
                    print(f"  è£½å“æ•°: {len(selected)}")
                    print(f"  GPT API: {st.session_state.slide_use_gpt_api}")
                    print(f"  TAVILY API: {st.session_state.slide_use_tavily_api}")
                    print(f"  TAVILYä½¿ç”¨å›æ•°: {st.session_state.slide_tavily_uses}")
                    
                    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®å–å¾—
                    print("ğŸ“š ãƒãƒ£ãƒƒãƒˆå±¥æ­´å–å¾—ä¸­...")
                    chat_history = _gather_messages_context(
                        item_id, 
                        st.session_state.slide_history_reference_count
                    )
                    print(f"  ãƒãƒ£ãƒƒãƒˆå±¥æ­´é•·: {len(chat_history)}æ–‡å­—")
                    
                    print("ğŸ¤– NewSlideGeneratoråˆæœŸåŒ–ä¸­...")
                    generator = NewSlideGenerator()
                    print("âœ… NewSlideGeneratoråˆæœŸåŒ–å®Œäº†")
                    
                    print("ğŸ¯ ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Ÿè¡Œä¸­...")
                    pptx_data = generator.create_presentation(
                        project_name=company_internal,  # æ¡ˆä»¶åã¨ã—ã¦ä¼æ¥­åã‚’ä½¿ç”¨
                        company_name=company_internal,
                        meeting_notes=st.session_state.slide_meeting_notes or "",
                        chat_history=chat_history,
                        products=selected,
                        use_tavily=st.session_state.slide_use_tavily_api,
                        use_gpt=st.session_state.slide_use_gpt_api,
                        tavily_uses=st.session_state.slide_tavily_uses
                    )
                    
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®è¡¨ç¤º
                    print("âœ… ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Œäº†")
                    print(f"  ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(pptx_data)} ãƒã‚¤ãƒˆ")
                    
                    st.success("ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"{company_internal}_ææ¡ˆæ›¸_{timestamp}.pptx"
                    
                    st.download_button(
                        label="ğŸ“¥ ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=pptx_data,
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                        use_container_width=True,
                        type="primary"
                    )
                    
                except Exception as e:
                    print(f"âŒ Streamlit: ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
                    st.error(f"ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.info("ä¸‹æ›¸ãã®ã¿ä½œæˆã•ã‚Œã¾ã—ãŸã€‚")

    if st.session_state.slide_outline:
        with st.expander("ä¸‹æ›¸ããƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆJSONï¼‰", expanded=True):
            st.json(st.session_state.slide_outline)
