{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4879bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc31481",
   "metadata": {},
   "source": [
    "# API Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17d3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.envからAPIキーを読む準備\n",
    "load_dotenv('./.env', override=True)\n",
    "API_VERSION = \"2024-12-01-preview\" #Azure openAI API version\n",
    "'''\n",
    "使用可能なモデル\n",
    "gpt-5-mini: reasoning(high), input(text), output(text,image), description(https://platform.openai.com/docs/models/gpt-5-mini)\n",
    "text-embedding-3-large: embedding model(https://platform.openai.com/docs/models/text-embedding-3-large)\n",
    "'''\n",
    "model_list = ['gpt-5-mini', 'text-embedding-3-large']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d66197",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c30fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Azure openAI API クライアントの作成\n",
    "client = AzureOpenAI(\n",
    "    api_version=API_VERSION,\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1b554",
   "metadata": {},
   "source": [
    "# GPT5-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf7c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GPT5-miniに渡す入力\n",
    "System prompt\n",
    "Userの入力(text,image)\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful and professional data scientist.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\":\"Explain me the GPT-oss model.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\":{\n",
    "                        \"url\": \"https://substackcdn.com/image/fetch/$s_!PKaP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe804b20e-7196-4529-9ca1-13a946123c7c_1589x734.png\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0faea029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a clear, practical explanation of the GPT‑OSS models shown in your diagram (the 20B and 120B variants), what makes them different from a dense Transformer, and why those design choices matter.\n",
      "\n",
      "High‑level idea\n",
      "- GPT‑OSS uses a standard decoder‑only Transformer backbone but adds two key efficiency/capacity features:\n",
      "  - Mixture‑of‑Experts (MoE) layers replacing (or augmenting) dense feed‑forward networks to give huge parameter capacity without linearly increasing compute.\n",
      "  - Grouped Query Attention (GQA) to reduce attention memory and compute costs required for many heads.\n",
      "- It also uses modern component choices: Rotary Positional Embeddings (RoPE) for long contexts, RMSNorm instead of LayerNorm, and SwiGLU (SwiLU + GLU style) feed‑forward nonlinearity.\n",
      "\n",
      "Core architecture and numbers (from the diagram)\n",
      "- Two model sizes shown:\n",
      "  - GPT‑OSS 20B\n",
      "    - ~20 billion parameters total\n",
      "    - 24 Transformer blocks\n",
      "    - Embedding dimension: 2,880\n",
      "    - 64 attention heads (implemented with grouped query attention)\n",
      "    - MoE: 32 experts total (per MoE layer), but only 4 experts are active per token\n",
      "    - Supported context length: 131k tokens\n",
      "    - Active parameters per token at inference: ~3.6B (because only a fraction of experts are executed)\n",
      "  - GPT‑OSS 120B\n",
      "    - ~120 billion parameters total\n",
      "    - 36 Transformer blocks\n",
      "    - Embedding dimension: 2,880 (same as the 20B)\n",
      "    - 64 attention heads (GQA)\n",
      "    - MoE: 128 experts total, 4 experts active per token\n",
      "    - Same long context: 131k tokens\n",
      "    - Active parameters per token at inference: ~5.1B\n",
      "- Vocabulary size: ~200k tokens (supports large subword vocabularies useful for long/technical content)\n",
      "\n",
      "What the MoE and router do\n",
      "- Mixture‑of‑Experts: the usual feed‑forward (FFN) is replaced by a set of independent “expert” FFNs. A lightweight router picks a small number (here 4) of experts to process each token.\n",
      "- Benefit: model capacity grows with number of experts (more parameters) while compute per token grows only with number of active experts → enables very large models without proportional inference cost.\n",
      "- Tradeoffs: sparse activation leads to complexities in training (router stability, load balancing across experts), possible quality/consistency issues, and more complicated distributed implementations.\n",
      "\n",
      "Grouped Query Attention (GQA)\n",
      "- Instead of computing attention with one query per head, GQA groups heads to share some computation, reducing memory and compute required for high head counts.\n",
      "- This helps scale to many heads and long contexts (131k tokens in this design) without exploding resource needs.\n",
      "\n",
      "Other component choices\n",
      "- RoPE (Rotary Positional Embeddings): enables extrapolating to very long contexts and is widely used for long‑range attention.\n",
      "- RMSNorm: lighter alternative to LayerNorm (no centering), simpler / slightly cheaper numerically and used by many recent Transformer variants.\n",
      "- SwiGLU (SwiLU + gated linear unit form): an efficient, higher‑quality activation function for the FFN pathway.\n",
      "\n",
      "Practical consequences and use cases\n",
      "- Long‑context applications: summarization of huge documents, multi‑document QA, retrieval‑augmented generation with long windows, codebases, logs, meeting transcripts.\n",
      "- Large capacity with lower inference cost: you get benefits of “120B scale” parameters while only executing a fraction of them at inference time.\n",
      "- Good for tasks benefiting from big model capacity but where running full dense 120B compute would be infeasible.\n",
      "\n",
      "Limitations and caveats\n",
      "- Implementation complexity: MoE + routing requires engineering support in training and serving (specialized parallelism, careful batching).\n",
      "- Training stability and fairness: routers must be trained to balance load; otherwise some experts can become underutilized.\n",
      "- Sparse models can produce different failure modes (e.g., routing jitter between tokens, variable per‑token quality).\n",
      "- Fine‑tuning or low‑resource adaptation can be more complicated for MoE architectures.\n",
      "\n",
      "If you want\n",
      "- I can compare GPT‑OSS to a dense model of similar active compute (what you’d gain/lose in quality), or\n",
      "- Show the math/diagram for how the router selects experts and how active parameter counts are computed, or\n",
      "- Explain deployment considerations (hardware, parallelism strategies, batching, memory).\n"
     ]
    }
   ],
   "source": [
    "#Azure openAI API を呼び出す\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages, #入力\n",
    "    max_completion_tokens=12800, #最大トークン数\n",
    "    model=model_list[0] #モデル選択\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "793011b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completion_tokens=1575, prompt_tokens=1178, total_tokens=2753\n",
      "reasoning_tokens=640\n"
     ]
    }
   ],
   "source": [
    "print(f'completion_tokens={response.usage.completion_tokens}, prompt_tokens={response.usage.prompt_tokens}, total_tokens={response.usage.total_tokens}')\n",
    "print(f'reasoning_tokens={response.usage.completion_tokens_details.reasoning_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe7a32",
   "metadata": {},
   "source": [
    "# text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de452b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: length=1024, [0.030616212636232376, -0.0028326271567493677, ..., -0.005696623120456934, 0.018194060772657394]\n",
      "data[1]: length=1024, [0.016034524887800217, 0.00731195043772459, ..., 0.004383934661746025, 0.023255884647369385]\n",
      "data[2]: length=1024, [0.02254379168152809, -0.002585632260888815, ..., -0.005732203833758831, 0.012679222971200943]\n",
      "Usage(prompt_tokens=6, total_tokens=6)\n",
      "embeddings shape: (3, 1024)\n"
     ]
    }
   ],
   "source": [
    "dimensions = 1024 #最大の埋め込み次元数\n",
    "input_text = [\"first phrase\",\"second phrase\",\"third phrase\"] #インプット\n",
    "response = client.embeddings.create(\n",
    "    input=input_text,\n",
    "    dimensions=dimensions,\n",
    "    model=model_list[1] #モデル選択\n",
    ")\n",
    "\n",
    "embeddings = np.zeros((len(input_text),dimensions)) #Embedding vectorを入れるためのnumpy配列\n",
    "\n",
    "#埋め込み結果を表示\n",
    "for i,item in enumerate(response.data):\n",
    "    length = len(item.embedding)\n",
    "    embeddings[i,:] = item.embedding\n",
    "    print(\n",
    "        f\"data[{item.index}]: length={length}, \"\n",
    "        f\"[{item.embedding[0]}, {item.embedding[1]}, \"\n",
    "        f\"..., {item.embedding[length-2]}, {item.embedding[length-1]}]\"\n",
    "    )\n",
    "print(response.usage)\n",
    "print(f'embeddings shape: {embeddings.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (otsuka-env)",
   "language": "python",
   "name": "otsuka-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
