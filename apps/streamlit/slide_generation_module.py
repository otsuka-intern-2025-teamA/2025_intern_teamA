# slide_generation_module.py
# ---------------------------------------------------------
# ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆãƒšãƒ¼ã‚¸ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š / æœ¬æ–‡ï¼å…¥åŠ›â†’çµæœ / ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆï¼‰
# - ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ­ã‚´ï¼æ¡ˆä»¶ä¸€è¦§ã¸æˆ»ã‚‹ï¼ä¼æ¥­å(è¡¨ç¤ºã®ã¿)ï¼å•†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼AIè¨­å®šï¼ã‚¯ãƒªã‚¢
# - æœ¬æ–‡ï¼šä¸Šæ®µï¼å•†è«‡ãƒ¡ãƒ¢ã‚’å…¥åŠ› + å‚è€ƒè³‡æ–™ã‚’å…¥åŠ› + è©³ç´°è¨­å®š(Top-K/å±¥æ­´å‚ç…§)
#          ä¸­æ®µï¼èª²é¡Œã®è¦ç´„ï¼ˆçµæœï¼‰ï¼ææ¡ˆå€™è£œã®ä¸€è¦§ï¼ˆçµæœï¼‰
#          ä¸‹æ®µï¼ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬æ·»ä»˜ â†’ ç”Ÿæˆ â†’ DLï¼‰
# ---------------------------------------------------------

from __future__ import annotations

import json
import os
import re
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Any

import pandas as pd

# Load environment variables from .env file
from dotenv import load_dotenv

load_dotenv(".env", override=True)

# APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆä¼æ¥­åˆ†æã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´å–å¾—ã«ä½¿ç”¨ï¼‰
from lib.api import api_available, get_api_client

# ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« / ã‚¹ã‚¿ã‚¤ãƒ«
from lib.styles import (
    apply_company_analysis_page_styles,  # ã‚µã‚¤ãƒ‰ãƒãƒ¼åœ§ç¸®/ãƒ­ã‚´ã‚«ãƒ¼ãƒ‰/ä¸‹å¯„ã›CSS
    apply_main_styles,
    apply_slide_generation_page_styles,
    apply_title_styles,
    render_sidebar_logo_card,
    render_slide_generation_title,  # ã‚¿ã‚¤ãƒˆãƒ«æç”»ï¼ˆh1.slide-generation-titleï¼‰
)

import streamlit as st

# ç”»åƒ/ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
PROJECT_ROOT = Path(__file__).parent.parent.parent
LOGO_PATH = PROJECT_ROOT / "data" / "images" / "otsuka_logo.jpg"
ICON_PATH = PROJECT_ROOT / "data" / "images" / "otsuka_icon.png"
PRODUCTS_DIR = PROJECT_ROOT / "data" / "csv" / "products"
PLACEHOLDER_IMG = PROJECT_ROOT / "data" / "images" / "product_placeholder.png"

# --- æ–°ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ  ---
from lib.new_slide_generator import NewSlideGenerator


# =========================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
# =========================
def _ensure_session_defaults() -> None:
    ss = st.session_state
    ss.setdefault("selected_project", None)       # æ¡ˆä»¶ä¸€è¦§ã‹ã‚‰é·ç§»æ™‚ã«å…¥ã‚‹
    ss.setdefault("api_error", None)
    ss.setdefault("slide_meeting_notes", "")
    ss.setdefault("uploaded_files_store", [])     # file_uploaderã®ä¿å­˜ç”¨
    ss.setdefault("product_candidates", [])       # è¡¨ç¤ºç”¨ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®é…åˆ—
    ss.setdefault("slide_outline", None)
    ss.setdefault("slide_overview", "")
    ss.setdefault("slide_history_reference_count", 3)  # â† è©³ç´°è¨­å®šã«ç§»å‹•ï¼ˆæœ¬æ–‡ï¼‰
    ss.setdefault("slide_top_k", 1)                   # â† è©³ç´°è¨­å®šã«ç§»å‹•ï¼ˆæœ¬æ–‡ï¼‰
    ss.setdefault("slide_products_dataset", "Auto")    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰
    ss.setdefault("slide_use_tavily_api", True)        # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰
    ss.setdefault("slide_use_gpt_api", True)           # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰
    ss.setdefault("slide_tavily_uses", 1)              # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰
    # åŸ‹ã‚è¾¼ã¿æ¤œç´¢ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    ss.setdefault("_emb_cache", {})
    # è¡¨ç¤ºç”¨ï¼šèª²é¡Œåˆ†æçµæœï¼ˆUIã§è¦‹ã›ã‚‹ã ã‘ã€‚é¸å®šãƒ­ã‚¸ãƒƒã‚¯ã¯å¾“æ¥é€šã‚Šï¼‰
    ss.setdefault("analyzed_issues", [])
    # ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆä»»æ„ï¼‰
    ss.setdefault("slide_template_bytes", None)
    ss.setdefault("slide_template_name", None)


# =========================
# DBã‹ã‚‰èª²é¡Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—
# =========================
def _get_proposal_issues_from_db(proposal_id: str) -> list[dict[str, Any]]:
    try:
        import sqlite3
        db_path = PROJECT_ROOT / "data" / "sqlite" / "app.db"
        if not db_path.exists():
            return []
        with sqlite3.connect(db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT idx, issue, weight, keywords_json
                FROM proposal_issues
                WHERE proposal_id = ?
                ORDER BY idx
            """, (proposal_id,))
            rows = cursor.fetchall()
            issues = []
            for row in rows:
                import json as _j
                keywords = _j.loads(row[3]) if row[3] else []
                issues.append({"issue": row[1], "weight": row[2], "keywords": keywords})
            return issues
    except Exception as e:
        print(f"ææ¡ˆèª²é¡Œå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []


# =========================
# ä¾¿åˆ©é–¢æ•°ï¼ˆä¾¡æ ¼ãªã©ï¼‰
# =========================
def _to_float(val):
    if val is None:
        return None
    if isinstance(val, (int, float)):
        if isinstance(val, float) and pd.isna(val):
            return None
        return float(val)
    if isinstance(val, str):
        s = val.strip().replace("Â¥", "").replace(",", "")
        if not s:
            return None
        try:
            return float(s)
        except Exception:
            return None
    return None


def _fmt_price(val) -> str:
    v = _to_float(val)
    return f"Â¥{int(round(v)):,}" if v is not None else "â€”"


# =========================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåé›†
# =========================
def _list_product_datasets() -> list[str]:
    """productsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€åã‚’åˆ—æŒ™ï¼ˆAutoã‚’å…ˆé ­ï¼‰"""
    if not PRODUCTS_DIR.exists():
        return ["Auto"]
    ds = ["Auto"]
    for p in PRODUCTS_DIR.iterdir():
        if p.is_dir():
            ds.append(p.name)
    return ds


def _gather_messages_context(item_id: str | None, history_n: int) -> str:
    """ä¼æ¥­åˆ†æã®ç›´è¿‘Nå¾€å¾©ï¼ˆ=2Nç™ºè¨€ï¼‰ã‚’ã¾ã¨ã‚ã¦æ–‡å­—åˆ—åŒ–"""
    if not (item_id and api_available()):
        return ""
    try:
        api = get_api_client()
        msgs = api.get_item_messages(item_id) or []
        take = min(len(msgs), history_n * 2)
        recent = msgs[-take:] if take > 0 else []
        ctx_lines = []
        for m in recent:
            role = m.get("role", "assistant")
            role_j = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if role == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
            ctx_lines.append(f"{role_j}: {m.get('content','')}")
        return "\n".join(ctx_lines)
    except Exception:
        return ""


def _load_products_from_csv(dataset: str) -> pd.DataFrame:
    """
    å•†æCSVã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆå­˜åœ¨ã‚«ãƒ©ãƒ ãŒç„¡ã‘ã‚Œã°ä½œæˆï¼‰
    æœŸå¾…ã‚«ãƒ©ãƒ : id(ç„¡ã‘ã‚Œã°ç”Ÿæˆ), name, category, price, description, tags
             ï¼‹ image_url/image/thumbnailï¼ˆä»»æ„ï¼‰, source_csvï¼ˆè¿½åŠ ï¼‰
    """
    frames: list[pd.DataFrame] = []
    if not PRODUCTS_DIR.exists():
        return pd.DataFrame()

    def _read_csvs(folder: Path):
        for csvp in folder.glob("*.csv"):
            try:
                df = pd.read_csv(csvp)
                for col in ["name", "category", "price", "description", "tags"]:
                    if col not in df.columns:
                        df[col] = None
                for col in ["image_url", "image", "thumbnail"]:
                    if col not in df.columns:
                        df[col] = None
                if "id" not in df.columns:
                    df["id"] = [f"{csvp.stem}-{i+1}" for i in range(len(df))]
                df["source_csv"] = csvp.stem
                frames.append(df[["id", "name", "category", "price", "description", "tags",
                                  "image_url", "image", "thumbnail", "source_csv"]])
            except Exception:
                continue

    if dataset == "Auto":
        for sub in PRODUCTS_DIR.iterdir():
            if sub.is_dir():
                _read_csvs(sub)
        _read_csvs(PRODUCTS_DIR)
    else:
        target = PRODUCTS_DIR / dataset
        if target.exists() and target.is_dir():
            _read_csvs(target)

    if not frames:
        return pd.DataFrame()
    return pd.concat(frames, ignore_index=True)


# =========================
# å‚è€ƒè³‡æ–™ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
# =========================
def _extract_text_from_uploads(uploaded_files: list[Any], max_chars: int = 12000) -> str:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦é€£çµã—ã¦è¿”ã™ã€‚
    å¤±æ•—æ™‚ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã€‚ç”»åƒ/OCRã¯æœªå¯¾å¿œã€‚
    """
    if not uploaded_files:
        return ""

    chunks: list[str] = []
    used_chars = 0

    def _append(text: str):
        nonlocal used_chars
        if not text:
            return
        remain = max_chars - used_chars
        if remain <= 0:
            return
        cut = text[:remain]
        chunks.append(cut)
        used_chars += len(cut)

    for f in uploaded_files:
        try:
            name = getattr(f, "name", "uploaded_file")
            lower = str(name).lower()
            try:
                f.seek(0)
            except Exception:
                pass
            data = f.read()
            try:
                f.seek(0)
            except Exception:
                pass

            if lower.endswith(".pdf"):
                try:
                    import io

                    from pypdf import PdfReader
                    reader = PdfReader(io.BytesIO(data))
                    page_limit = min(len(reader.pages), 30)
                    texts = []
                    for i in range(page_limit):
                        try:
                            texts.append(reader.pages[i].extract_text() or "")
                        except Exception:
                            continue
                    _append(f"\n[PDF:{name} æŠœç²‹]\n" + "\n".join(texts))
                except Exception:
                    _append(f"\n[PDF:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".docx"):
                try:
                    import io

                    from docx import Document
                    doc = Document(io.BytesIO(data))
                    paras = [p.text for p in doc.paragraphs if p.text]
                    _append(f"\n[DOCX:{name} æŠœç²‹]\n" + "\n".join(paras))
                except Exception:
                    _append(f"\n[DOCX:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".pptx"):
                try:
                    import io

                    from pptx import Presentation
                    prs = Presentation(io.BytesIO(data))
                    slide_texts = []
                    for s in prs.slides:
                        buf = []
                        for shp in s.shapes:
                            try:
                                if hasattr(shp, "text"):
                                    t = shp.text or ""
                                    if t:
                                        buf.append(t)
                            except Exception:
                                continue
                        if buf:
                            slide_texts.append("\n".join(buf))
                    _append(f"\n[PPTX:{name} æŠœç²‹]\n" + "\n---\n".join(slide_texts))
                except Exception:
                    _append(f"\n[PPTX:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".csv"):
                try:
                    import io
                    tmp = io.BytesIO(data)
                    df = pd.read_csv(tmp)
                    head = df.head(20)
                    txt = head.to_csv(index=False)
                    _append(f"\n[CSV:{name} å…ˆé ­20è¡Œ]\n{txt}")
                except Exception:
                    _append(f"\n[CSV:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".txt"):
                try:
                    txt = data.decode("utf-8", errors="ignore")
                except Exception:
                    txt = str(data[:4000])
                _append(f"\n[TXT:{name}]\n{txt}")

            else:
                _append(f"\n[{name}]ï¼ˆæœªå¯¾å¿œ/ãƒã‚¤ãƒŠãƒªã®ãŸã‚æ¦‚è¦åæ˜ ã®ã¿ï¼‰")

        except Exception:
            continue

        if used_chars >= max_chars:
            break

    return "\n".join(chunks).strip()


# =========================
# æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç°¡æ˜“ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼‰
# =========================
def _simple_tokenize(text: str) -> list[str]:
    text = str(text or "").lower()
    text = re.sub(r"[^a-z0-9\u3040-\u30ff\u4e00-\u9fff]+", " ", text)
    toks = text.split()
    return [t for t in toks if len(t) >= 2]


def _fallback_rank_products(
    notes: str,
    messages_ctx: str,
    products_df: pd.DataFrame,
    top_pool: int
) -> list[dict[str, Any]]:
    """å•†è«‡ãƒ¡ãƒ¢ï¼‹å±¥æ­´ã®èªå¥ä¸€è‡´ã§ç´ æœ´ã«ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° â†’ ä¸Šä½ top_pool ã‚’è¿”ã™"""
    if products_df.empty:
        return []

    query_text = (notes or "") + "\n" + (messages_ctx or "")
    q_tokens = _simple_tokenize(query_text)

    def _row_text(row) -> str:
        return " ".join([
            str(row.get("name") or ""),
            str(row.get("category") or ""),
            str(row.get("description") or ""),
            str(row.get("tags") or ""),
        ]).lower()

    scored: list[tuple[float, dict[str, Any]]] = []
    for _, row in products_df.iterrows():
        t = _row_text(row)
        score = 0.0
        for tok in q_tokens:
            if tok in t:
                score += 1.0
        reason = f"ä¸€è‡´èªå¥æ•°={int(score)}" if score > 0 else "ä¸€è‡´ãªã—ï¼ˆä½ã‚¹ã‚³ã‚¢ï¼‰"
        scored.append((score, {
            "id": row.get("id"),
            "name": row.get("name"),
            "category": row.get("category"),
            "price": row.get("price"),
            "description": row.get("description"),
            "tags": row.get("tags"),
            "image_url": row.get("image_url"),
            "image": row.get("image"),
            "thumbnail": row.get("thumbnail"),
            "source_csv": row.get("source_csv"),
            "score": round(float(score), 2),
            "reason": reason,
        }))

    scored.sort(key=lambda x: (x[0], str(x[1]["name"]).lower()), reverse=True)
    return [d for _, d in scored[:top_pool]]


# =========================
# è£½å“æ¦‚è¦ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
# =========================
def _extract_json(s: str) -> dict[str, Any]:
    s = (s or "").strip()
    if not s:
        return {}
    if s.lstrip().startswith("{"):
        try:
            return json.loads(s)
        except Exception:
            pass
    try:
        start = s.find("{")
        end = s.rfind("}")
        if start >= 0 and end > start:
            return json.loads(s[start:end + 1])
    except Exception:
        return {}
    return {}


def _llm_pick_products(pool: list[dict[str, Any]], top_k: int, company: str, notes: str, ctx: str, issues: list[dict[str, Any]] | None = None) -> list[dict[str, Any]]:
    """
    ã‚«ã‚¿ãƒ­ã‚°ï¼ˆpoolï¼‰ã‹ã‚‰ LLM ã§ Top-K ã‚’é¸æŠœã—ã€çŸ­ã„ç†ç”±ã¨ä¿¡é ¼åº¦ã‚’ä»˜ä¸ã€‚
    issues ãŒä¸ãˆã‚‰ã‚Œã‚Œã°ã€èª²é¡ŒIDã¨ã®å¯¾å¿œä»˜ã‘ã¨æ ¹æ‹ ã‚’æ±‚ã‚ã‚‹ã€‚
    """
    if not pool:
        return []
    # ã“ã“ã§ã¯å®Ÿè¡Œã—ãªã„ï¼ˆç°¡æ˜“ãƒ­ã‚¸ãƒƒã‚¯é‹ç”¨ï¼‰ã€‚å¿…è¦ãªã‚‰ _get_chat_client ã‚’å®Ÿè£…ã—ã¦ä½¿ç”¨ã€‚

    return []


def _summarize_overviews_llm(cands: list[dict[str, Any]]) -> None:
    """ï¼ˆæœªä½¿ç”¨ï¼‰å„è£½å“ã®æ¦‚è¦ã‚’ LLM ã§è¦ç´„ã™ã‚‹å ´åˆã®ãƒ•ãƒƒã‚¯"""
    pass


def _resolve_product_image_src(rec: dict[str, Any]) -> str | None:
    for key in ("image_url", "image", "thumbnail"):
        v = rec.get(key)
        if not v:
            continue
        s = str(v).strip()
        if s.startswith("http://") or s.startswith("https://"):
            return s
        p = (PROJECT_ROOT / s).resolve()
        if p.exists():
            return str(p)
    if PLACEHOLDER_IMG.exists():
        return str(PLACEHOLDER_IMG)
    return None


# =========================
# ç°¡æ˜“èª²é¡ŒæŠ½å‡º
# =========================
def _analyze_pain_points_simple(notes: str, messages_ctx: str, uploads_text: str = "") -> list[dict[str, Any]]:
    issues = [
        {"issue": "æ¥­å‹™åŠ¹ç‡åŒ–", "weight": 0.4, "keywords": ["åŠ¹ç‡åŒ–", "è‡ªå‹•åŒ–", "ç”Ÿç”£æ€§"]},
        {"issue": "ã‚³ã‚¹ãƒˆæœ€é©åŒ–", "weight": 0.3, "keywords": ["è²»ç”¨å‰Šæ¸›", "æœ€é©åŒ–", "ã‚³ã‚¹ãƒˆ"]},
        {"issue": "æƒ…å ±ç®¡ç†æ”¹å–„", "weight": 0.3, "keywords": ["æƒ…å ±å…±æœ‰", "ç®¡ç†", "ã‚·ã‚¹ãƒ†ãƒ "]},
    ]
    all_text = f"{notes} {messages_ctx} {uploads_text}".lower()
    if "åŠ¹ç‡" in all_text or "ç”Ÿç”£æ€§" in all_text:
        issues[0]["weight"], issues[1]["weight"], issues[2]["weight"] = 0.5, 0.25, 0.25
    elif "ã‚³ã‚¹ãƒˆ" in all_text or "è²»ç”¨" in all_text:
        issues[0]["weight"], issues[1]["weight"], issues[2]["weight"] = 0.25, 0.5, 0.25
    elif "æƒ…å ±" in all_text or "ç®¡ç†" in all_text:
        issues[0]["weight"], issues[1]["weight"], issues[2]["weight"] = 0.25, 0.25, 0.5
    return issues

# æ—¢å­˜ã‚³ãƒ¼ãƒ‰äº’æ›ï¼ˆä»–æ‰€ã®å‘¼ã³å‡ºã—åã‚’å¸åï¼‰
_analyze_pain_points = _analyze_pain_points_simple


# =========================
# å€™è£œæ¤œç´¢ï¼ˆç°¡æ˜“ç‰ˆï¼‰
# =========================
def _search_product_candidates(
    company: str,
    item_id: str | None,
    meeting_notes: str,
    top_k: int,
    history_n: int,
    dataset: str,
    uploaded_files: list[Any],
    issues_precomputed: list[dict[str, Any]] | None = None,
) -> list[dict[str, Any]]:
    # ä¼æ¥­åˆ†æã®æ–‡è„ˆ
    ctx = _gather_messages_context(item_id, history_n)

    # CSV èª­ã¿è¾¼ã¿
    df = _load_products_from_csv(dataset)
    if df.empty:
        return []

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    uploads_text = _extract_text_from_uploads(uploaded_files) if uploaded_files else ""

    # äº‹å‰ã«è¨ˆç®—æ¸ˆã¿ã®èª²é¡ŒãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€‚ãªã‘ã‚Œã°ã“ã“ã§æŠ½å‡ºã€‚
    issues = issues_precomputed if issues_precomputed is not None else _analyze_pain_points(meeting_notes or "", ctx or "", uploads_text)

    # èªå¥ä¸€è‡´ã§ç²—é¸å®š
    top_pool = max(40, top_k * 4)
    pool = _fallback_rank_products(meeting_notes, ctx, df, top_pool=top_pool)

    # ä¸Šä½Kä»¶ã‚’é¸æŠ
    selected = pool[:top_k]

    # å„è£½å“ã®æ¦‚è¦ï¼ˆã“ã“ã§ã¯ç°¡æ˜“ã« description ãªã©ã‹ã‚‰åˆ‡ã‚Šå‡ºã™ï¼‰
    for product in selected:
        base = product.get("description") or product.get("tags") or product.get("name") or ""
        product["overview"] = (base[:80] + ("â€¦" if base and len(base) > 80 else "")) if base else "â€”"

    return selected


# =========================
# ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆ
# =========================
def _make_outline_preview(company: str, meeting_notes: str, selected_products: list[dict[str, Any]], overview: str) -> dict[str, Any]:
    return {
        "title": f"{company} å‘ã‘ææ¡ˆè³‡æ–™ï¼ˆãƒ‰ãƒ©ãƒ•ãƒˆï¼‰",
        "overview": overview,
        "sections": [
            {"h2": "1. ã‚¢ã‚¸ã‚§ãƒ³ãƒ€", "bullets": ["èƒŒæ™¯", "èª²é¡Œæ•´ç†", "ææ¡ˆæ¦‚è¦", "å°å…¥åŠ¹æœ", "å°å…¥è¨ˆç”»", "æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"]},
            {"h2": "2. èƒŒæ™¯", "bullets": [f"{company}ã®äº‹æ¥­æ¦‚è¦ï¼ˆè¦ç´„ï¼‰", "å¸‚å ´å‹•å‘ãƒ»ç«¶åˆçŠ¶æ³ï¼ˆæŠœç²‹ï¼‰"]},
            {"h2": "3. ç¾çŠ¶ã®èª²é¡Œï¼ˆä»®èª¬ï¼‰", "bullets": ["ç”Ÿç”£æ€§/ã‚³ã‚¹ãƒˆ/å“è³ª/ã‚¹ãƒ”ãƒ¼ãƒ‰ã®è¦³ç‚¹ã‹ã‚‰3ã€œ5ç‚¹"]},
            {"h2": "4. ææ¡ˆæ¦‚è¦", "bullets": ["æœ¬ææ¡ˆã®ç›®çš„ï¼ç‹™ã„", "å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆé«˜ãƒ¬ãƒ™ãƒ«ï¼‰"]},
            {"h2": "5. æ¨å¥¨ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå€™è£œï¼‰", "bullets": [f"{len(selected_products)}ä»¶ã®å•†æå€™è£œã‚’æ•´ç†ãƒ»æ¯”è¼ƒ"]},
            {"h2": "6. å°å…¥åŠ¹æœï¼ˆå®šé‡/å®šæ€§ï¼‰", "bullets": ["KPIè¦‹è¾¼ã¿ / åŠ¹æœè©¦ç®—ã®æ–¹é‡"]},
            {"h2": "7. å°å…¥ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ¡ˆ", "bullets": ["PoC â†’ æœ¬å°å…¥ / ä½“åˆ¶ãƒ»å½¹å‰²åˆ†æ‹…"]},
        ],
        "meeting_notes_digest": meeting_notes[:300] + ("..." if len(meeting_notes) > 300 else ""),
        "products": [
            {
                "id": p.get("id"),
                "name": p.get("name"),
                "category": p.get("source_csv") or p.get("category"),
                "price": p.get("price"),
                "reason": p.get("reason"),
                "overview": p.get("overview"),
            }
            for p in selected_products
        ],
    }


# --- çµæœãƒ¬ãƒ³ãƒ€ãƒ©ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’åˆ†é›¢ï¼‰ ---
def _render_issues_body(issues: list[dict[str, Any]], body_ph):
    body_ph.empty()
    with body_ph.container():
        if not issues:
            st.caption("ã€å•†å“ææ¡ˆã‚’ä½œæˆã€ã‚’æŠ¼ã™ã¨ã€å•†è«‡ãƒ¡ãƒ¢ãƒ»å±¥æ­´ãƒ»å‚è€ƒè³‡æ–™ã‹ã‚‰èª²é¡Œã‚’è‡ªå‹•æŠ½å‡ºã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")
            return
        for i, it in enumerate(issues, start=1):
            with st.container(border=True):
                st.markdown(f"**{i}. {it.get('issue','â€”')}**")
                st.caption(f"é‡ã¿: {it.get('weight',0):.2f}")
                kws = it.get("keywords") or []
                if kws:
                    st.markdown("é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: " + " / ".join(kws))


def _render_candidates_body(recs: list[dict[str, Any]], body_ph):
    body_ph.empty()
    with body_ph.container():
        if not recs:
            st.info("ææ¡ˆå€™è£œãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€å•†å“ææ¡ˆã‚’ä½œæˆã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
            return
        for r in recs:
            pid = str(r.get("id") or "")
            name = str(r.get("name") or "")
            cat_src = r.get("source_csv") or r.get("category") or "â€”"
            price_s = _fmt_price(r.get("price"))
            reason = r.get("reason") or "â€”"
            overview = r.get("overview") or "â€”"
            with st.container(border=True):
                c1, c2 = st.columns([1, 3], gap="medium")
                with c1:
                    img_src = _resolve_product_image_src(r)
                    if img_src and (img_src.startswith("http") or os.path.exists(img_src)):
                        st.image(img_src, use_container_width=True)
                    else:
                        st.markdown(
                            "<div style='width:100%;height:120px;border:1px solid #eee;border-radius:10px;"
                            "background:#f6f7f9;display:flex;align-items:center;justify-content:center;"
                            "color:#999;font-size:12px;'>ç”»åƒãªã—</div>",
                            unsafe_allow_html=True
                        )
                with c2:
                    st.markdown(f"**{name}**")
                    st.caption(f"ã‚«ãƒ†ã‚´ãƒª: {cat_src} ï¼ ä¾¡æ ¼: {price_s} ï¼ ID: `{pid}`")
                    st.markdown(f"**ææ¡ˆç†ç”±**ï¼š{reason}")
                    st.markdown(f"**è£½å“æ¦‚è¦**ï¼š{overview}")


# --- ææ¡ˆä¿å­˜ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆæ—¢å­˜ app.db ã‚’æ´»ç”¨ï¼‰ ---
import json as _json
import sqlite3
import uuid

DB_PATH = PROJECT_ROOT / "data" / "sqlite" / "app.db"

def _init_db_for_proposals():
    DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("""CREATE TABLE IF NOT EXISTS proposals(
            id TEXT PRIMARY KEY,
            project_item_id TEXT,
            company TEXT NOT NULL,
            meeting_notes TEXT,
            overview TEXT,
            created_at TEXT NOT NULL
        )""")
        c.execute("""CREATE TABLE IF NOT EXISTS proposal_issues(
            proposal_id TEXT NOT NULL,
            idx INTEGER NOT NULL,
            issue TEXT NOT NULL,
            weight REAL,
            keywords_json TEXT,
            FOREIGN KEY(proposal_id) REFERENCES proposals(id)
        )""")
        c.execute("""CREATE TABLE IF NOT EXISTS proposal_products(
            proposal_id TEXT NOT NULL,
            rank INTEGER NOT NULL,
            product_id TEXT,
            name TEXT,
            category TEXT,
            price TEXT,
            reason TEXT,
            overview TEXT,
            score REAL,
            source_csv TEXT,
            image_url TEXT,
            FOREIGN KEY(proposal_id) REFERENCES proposals(id)
        )""")
        conn.commit()

def _save_proposal_to_db(
    project_item_id: str | None,
    company: str,
    meeting_notes: str,
    overview: str,
    issues: list[dict[str, Any]],
    products: list[dict[str, Any]],
    created_at_iso: str
) -> str:
    """ææ¡ˆã²ã¨ã¾ã¨ã¾ã‚Šã‚’ä¿å­˜ã—ã€proposal_id ã‚’è¿”ã™ã€‚"""
    _init_db_for_proposals()
    pid = str(uuid.uuid4())
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute(
            "INSERT INTO proposals(id, project_item_id, company, meeting_notes, overview, created_at) VALUES(?,?,?,?,?,?)",
            (pid, project_item_id, company, meeting_notes, overview, created_at_iso)
        )
        for i, it in enumerate(issues or []):
            c.execute(
                "INSERT INTO proposal_issues(proposal_id, idx, issue, weight, keywords_json) VALUES(?,?,?,?,?)",
                (pid, i+1, it.get('issue',''), float(it.get('weight') or 0.0), _json.dumps(it.get('keywords') or [], ensure_ascii=False))
            )
        for r, p in enumerate(products or []):
            c.execute(
                """INSERT INTO proposal_products(
                    proposal_id, rank, product_id, name, category, price, reason, overview, score, source_csv, image_url
                ) VALUES(?,?,?,?,?,?,?,?,?,?,?)""",
                (
                    pid, r+1,
                    str(p.get('id','')) or None,
                    p.get('name',''),
                    p.get('source_csv') or p.get('category',''),
                    str(p.get('price','')),
                    p.get('reason',''),
                    p.get('overview',''),
                    float(p.get('score') or 0.0),
                    p.get('source_csv') or '',
                    p.get('image_url') or ''
                )
            )
        conn.commit()
    return pid


# =========================
# ãƒ¡ã‚¤ãƒ³æç”»ï¼ˆæ”¹ä¿®å¾Œï¼‰
# =========================
def render_slide_generation_page():
    """ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆãƒšãƒ¼ã‚¸ï¼ˆå…¥åŠ›ã¨çµæœã‚’ç©ºé–“åˆ†é›¢ / ã‚µã‚¤ãƒ‰ãƒãƒ¼ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å‰æã®ã¿ï¼‰"""
    _ensure_session_defaults()

    try:
        st.set_page_config(
            page_title="ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ",
            page_icon=str(ICON_PATH),
            layout="wide",
            initial_sidebar_state="expanded",
        )
    except Exception:
        pass

    # ã‚¹ã‚¿ã‚¤ãƒ«
    apply_main_styles(hide_sidebar=False, hide_header=True)
    apply_title_styles()
    apply_company_analysis_page_styles()  # ã‚µã‚¤ãƒ‰ãƒãƒ¼å…±é€š
    apply_slide_generation_page_styles()  # ã‚¿ã‚¤ãƒˆãƒ«ä½ç½®ãªã©

    # æ¡ˆä»¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    pj = st.session_state.get("selected_project")
    if pj:
        title_text = f"ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ - {pj['title']} / {pj['company']}"
        company_internal = pj.get("company", "")
        item_id = pj.get("id")
    else:
        title_text = "ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ"
        company_internal = ""
        item_id = None

    # ---------- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å‰æã®ã¿ï¼‰ ----------
    with st.sidebar:
        render_sidebar_logo_card(LOGO_PATH)

        st.markdown("### è¨­å®š")
        st.text_input("ä¼æ¥­å", value=company_internal, key="slide_company_input", disabled=True)

        st.selectbox(
            "å•†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            options=_list_product_datasets(),
            key="slide_products_dataset",
            help="data/csv/products/ é…ä¸‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã€‚Autoã¯è‡ªå‹•é¸æŠã€‚"
        )

        st.markdown("---")
        st.markdown("### AIè¨­å®š")
        st.checkbox("GPT APIä½¿ç”¨", key="slide_use_gpt_api")
        st.checkbox("TAVILY APIä½¿ç”¨", key="slide_use_tavily_api")
        if st.session_state.slide_use_tavily_api:
            st.selectbox("TAVILY APIå‘¼ã³å‡ºã—å›æ•°ï¼ˆè£½å“ã‚ãŸã‚Šï¼‰", options=list(range(1, 6)), key="slide_tavily_uses")

        sidebar_clear = st.button("ã‚¯ãƒªã‚¢", use_container_width=True, help="ææ¡ˆå€™è£œã¨èª²é¡Œã®è¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢")
        st.markdown("<div class='sidebar-bottom'>", unsafe_allow_html=True)
        if st.button("â† æ¡ˆä»¶ä¸€è¦§ã«æˆ»ã‚‹", use_container_width=True):
            st.session_state.current_page = "æ¡ˆä»¶ä¸€è¦§"
            st.session_state.page_changed = True
            st.rerun()
        st.markdown("</div>", unsafe_allow_html=True)

    # ---------- ã‚¿ã‚¤ãƒˆãƒ« ----------
    render_slide_generation_title(title_text)

    # ---------- è¦‹å‡ºã—è¡Œï¼ˆå·¦ï¼è¦‹å‡ºã— / å³ï¼CTAï¼‰ ----------
    head_l, head_r = st.columns([8, 2])
    with head_l:
        st.subheader("1. å•†å“ææ¡ˆã‚’ä½œæˆ")
    with head_r:
        search_btn = st.button("å•†å“ææ¡ˆä½œæˆ", type="primary", use_container_width=True)
    st.divider()

    # ====================== ä¸Šæ®µï¼šå…¥åŠ›ã‚¨ãƒªã‚¢ï¼ˆå•†è«‡ãƒ¡ãƒ¢ï¼‹å‚è€ƒè³‡æ–™ï¼‹è©³ç´°è¨­å®šï¼‰ ======================
    top_l, top_r = st.columns([3, 2], gap="large")  # å•†è«‡ãƒ¡ãƒ¢ã‚’åºƒã‚ã«
    with top_l:
        st.markdown("**â— å•†è«‡ãƒ¡ãƒ¢ã‚’å…¥åŠ›**")
        st.text_area(
            label="å•†è«‡ãƒ¡ãƒ¢ã‚’å…¥åŠ›",
            key="slide_meeting_notes",
            height=154,
            label_visibility="collapsed",
            placeholder="ä¾‹ï¼šæ¥æœŸã®éœ€è¦äºˆæ¸¬ç²¾åº¦å‘ä¸Šã¨åœ¨åº«æœ€é©åŒ–ã€‚PoCã‹ã‚‰æ®µéšå°å…¥â€¦ ãªã©",
        )
    with top_r:
        st.markdown("**â— å‚è€ƒè³‡æ–™ã‚’å…¥åŠ›ï¼ˆä»»æ„ï¼‰**")
        uploads = st.file_uploader(
            label="å‚è€ƒè³‡æ–™ã‚’å…¥åŠ›ï¼ˆä»»æ„ï¼‰",
            type=["pdf", "pptx", "docx", "csv", "png", "jpg", "jpeg", "txt"],
            accept_multiple_files=True,
            key="slide_uploader",
            label_visibility="collapsed",
            help="è­°äº‹éŒ²ã‚„è¦ä»¶å®šç¾©ãªã©ã‚’æ·»ä»˜ã€‚å†…å®¹ã¯èª²é¡ŒæŠ½å‡ºãƒ»å€™è£œé¸å®šã«åæ˜ ã•ã‚Œã¾ã™ã€‚",
        )
        if uploads:
            st.session_state.uploaded_files_store = uploads
            st.success(f"{len(uploads)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸã€‚")
        elif st.session_state.uploaded_files_store:
            st.caption(f"å‰å›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {len(st.session_state.uploaded_files_store)} ãƒ•ã‚¡ã‚¤ãƒ«")

    # â–¼ è©³ç´°è¨­å®šï¼ˆTop-K / éå»ãƒ­ã‚°å‚ç…§ï¼‰
    with st.expander("è©³ç´°è¨­å®šï¼ˆå•†å“ææ¡ˆã®æ¡ä»¶ï¼‰", expanded=False):
        cols = st.columns(2)
        with cols[0]:
            st.selectbox(
                "ææ¡ˆå€™è£œã®ä»¶æ•°ï¼ˆTop-Kï¼‰",
                options=list(range(1, 11)),
                key="slide_top_k",
                help="è¡¨ç¤ºã™ã‚‹ææ¡ˆå€™è£œã®ä»¶æ•°ã€‚"
            )
        with cols[1]:
            st.selectbox(
                "éå»ãƒ­ã‚°å‚ç…§ç¯„å›²ï¼ˆå¾€å¾©æ•°ï¼‰",
                options=list(range(1, 11)),
                key="slide_history_reference_count",
                help="ä¼æ¥­åˆ†æãƒãƒ£ãƒƒãƒˆã®ç›´è¿‘Nå¾€å¾©ã‚’æ–‡è„ˆã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚"
            )

    # ====================== ä¸­æ®µï¼šçµæœã‚¨ãƒªã‚¢ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ç›´ä¸‹ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸â†’æœ¬æ–‡ï¼‰ ======================
    bottom_l, bottom_r = st.columns([5, 7], gap="large")

    # å·¦ã‚«ãƒ©ãƒ ï¼šèª²é¡Œ
    with bottom_l:
        st.markdown("**â— èª²é¡Œã®è¦ç´„ï¼ˆçµæœï¼‰**")
        issues_msg_ph = st.empty()   # â† ã‚¿ã‚¤ãƒˆãƒ«ç›´ä¸‹ã®é€²è¡Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        issues_body_ph = st.empty()  # â† æœ¬æ–‡ï¼ˆãƒªã‚¹ãƒˆï¼‰

    # å³ã‚«ãƒ©ãƒ ï¼šå€™è£œ
    with bottom_r:
        st.markdown("**â— ææ¡ˆå€™è£œã®ä¸€è¦§ï¼ˆçµæœï¼‰**")
        candidates_msg_ph = st.empty()   # â† ã‚¿ã‚¤ãƒˆãƒ«ç›´ä¸‹ã®é€²è¡Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        candidates_body_ph = st.empty()  # â† æœ¬æ–‡ï¼ˆã‚«ãƒ¼ãƒ‰ç¾¤ï¼‰

    # åˆæœŸè¡¨ç¤ºï¼ˆå‰å›ã®çŠ¶æ…‹ã‚’åæ˜ ï¼‰
    _render_issues_body(st.session_state.get("analyzed_issues") or [], issues_body_ph)
    _render_candidates_body(st.session_state.get("product_candidates") or [], candidates_body_ph)

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã€Œã‚¯ãƒªã‚¢ã€
    if sidebar_clear:
        st.session_state.product_candidates = []
        st.session_state.analyzed_issues = []
        st.session_state.slide_outline = None
        # é€²è¡Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚ç©ºã«
        issues_msg_ph.empty()
        candidates_msg_ph.empty()
        # æœ¬æ–‡ã‚’ç©ºã«ï¼ˆåˆæœŸã®æ¡ˆå†…æ–‡ã«æˆ»ã™ï¼‰
        _render_issues_body([], issues_body_ph)
        _render_candidates_body([], candidates_body_ph)
        st.success("ææ¡ˆå€™è£œã¨èª²é¡Œã®è¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
        st.stop()  # â† ã“ã®ã‚¿ãƒ¼ãƒ³ã¯ã“ã“ã§çµ‚äº†ã—ã€å³æ™‚åæ˜ 

    # ææ¡ˆç”ŸæˆæŠ¼ä¸‹
    if search_btn:
        if not company_internal.strip():
            st.error("ä¼æ¥­ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ¡ˆä»¶ä¸€è¦§ã‹ã‚‰ä¼æ¥­ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")
        else:
            with issues_msg_ph.container():                 # â† å ´æ‰€ã‚’å›ºå®š
                with st.spinner("1/2 èª²é¡Œã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™..."):
                    # 1/2 èª²é¡ŒæŠ½å‡ºï¼ˆå·¦ã‚¿ã‚¤ãƒˆãƒ«ç›´ä¸‹ã«é€²æ—ã‚’è¡¨ç¤ºï¼‰
                    issues_body_ph.empty()  # ã„ã£ãŸã‚“æœ¬æ–‡ã¯ç©ºã«
                    ctx_for_view = _gather_messages_context(item_id, int(st.session_state.slide_history_reference_count))
                    uploads_text_for_view = _extract_text_from_uploads(st.session_state.uploaded_files_store) if st.session_state.uploaded_files_store else ""
                    issues_early = _analyze_pain_points(
                        st.session_state.slide_meeting_notes or "",
                        ctx_for_view or "",
                        uploads_text_for_view or ""
                    )
                    st.session_state.analyzed_issues = issues_early
            pass
            # çµæœã‚’æç”» â†’ é€²æ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯æ¶ˆã™
            issues_msg_ph.empty()
            _render_issues_body(issues_early, issues_body_ph)

            with candidates_msg_ph.container():                 # â† å ´æ‰€ã‚’å›ºå®š
                with st.spinner("2/2 ã‚«ã‚¿ãƒ­ã‚°ã¨ç…§åˆã—ã¦ææ¡ˆå€™è£œã‚’é¸å®šä¸­..."):
                    # 2/2 å€™è£œé¸å®šï¼ˆå³ã‚¿ã‚¤ãƒˆãƒ«ç›´ä¸‹ã§é€²æ—ã‚’é †ã«è¡¨ç¤ºï¼‰
                    candidates_body_ph.empty()
                    candidates = _search_product_candidates(
                        company=company_internal,
                        item_id=item_id,
                        meeting_notes=st.session_state.slide_meeting_notes or "",
                        top_k=int(st.session_state.slide_top_k),
                        history_n=int(st.session_state.slide_history_reference_count),
                        dataset=st.session_state.slide_products_dataset,
                        uploaded_files=st.session_state.uploaded_files_store,
                        issues_precomputed=issues_early,
                    )
                    st.session_state.product_candidates = candidates
            pass
            # æç”»ä¸­
            candidates_msg_ph.empty()
            _render_candidates_body(candidates, candidates_body_ph)

            # ãƒ‰ãƒ©ãƒ•ãƒˆä¿å­˜ï¼ˆå¾“æ¥ã©ãŠã‚Šï¼‰
            try:
                proposal_id = _save_proposal_to_db(
                    project_item_id=item_id,
                    company=company_internal,
                    meeting_notes=st.session_state.slide_meeting_notes or "",
                    overview=st.session_state.slide_overview or "",
                    issues=st.session_state.analyzed_issues or [],
                    products=st.session_state.product_candidates or [],
                    created_at_iso=datetime.now().isoformat(timespec="seconds")
                )
                st.session_state["last_proposal_id"] = proposal_id
            except Exception as e:
                st.warning(f"ãƒ‰ãƒ©ãƒ•ãƒˆä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    # ====================== 2. ææ¡ˆã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ ======================
    st.subheader("2. ææ¡ˆã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ")
    st.divider()

    tmpl_file = st.file_uploader(
        "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆ.pptxï¼‰ã‚’æ·»ä»˜ï¼ˆä»»æ„ï¼‰",
        type=["pptx"],
        key="slide_template_uploader",
        help="æœªæ·»ä»˜ã®å ´åˆã¯æ—¢å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™"
    )

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¸ä¿æŒ
    if tmpl_file is not None:
        st.session_state.slide_template_bytes = tmpl_file.getvalue()
        st.session_state.slide_template_name = tmpl_file.name
        st.success(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸï¼š{tmpl_file.name}")
    else:
        current = st.session_state.get("slide_template_name")
        if current:
            st.caption(f"ç¾åœ¨ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼š{current}ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‚’ä½¿ç”¨ï¼‰")
        else:
            st.caption("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæœªæ·»ä»˜ï¼šæ—¢å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")

    row_l, row_r = st.columns([8, 2], vertical_alignment="center")
    with row_l:
        st.session_state.slide_overview = st.text_input(
            "æ¦‚èª¬ï¼ˆä»»æ„ï¼‰",
            value=st.session_state.slide_overview or "",
            placeholder="ä¾‹ï¼šåœ¨åº«æœ€é©åŒ–ã‚’ä¸­å¿ƒã«ã€éœ€è¦äºˆæ¸¬ã¨è£œå……è¨ˆç”»ã®é€£æºã‚’ææ¡ˆâ€¦",
            label_visibility="collapsed",
        )
    with row_r:
        gen_btn = st.button("ç”Ÿæˆ", type="primary", use_container_width=True)

    if gen_btn:
        if not company_internal.strip():
            st.error("ä¼æ¥­ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        elif not st.session_state.product_candidates:
            st.error("ææ¡ˆå€™è£œãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã€å•†å“ææ¡ˆã‚’ä½œæˆã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        else:
            selected = list(st.session_state.product_candidates or [])  # å…¨å€™è£œã‚’æ¡ç”¨

            # ä¸‹æ›¸ãã®ä½œæˆ
            outline = _make_outline_preview(
                company_internal,
                st.session_state.slide_meeting_notes or "",
                selected,
                st.session_state.slide_overview or "",
            )
            st.session_state.slide_outline = outline

            # ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
            with st.spinner("AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆä¸­..."):
                try:
                    print("ğŸš€ Streamlit: ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆé–‹å§‹")
                    print(f"  ä¼æ¥­å: {company_internal}")
                    print(f"  è£½å“æ•°: {len(selected)}")
                    print(f"  GPT API: {st.session_state.slide_use_gpt_api}")
                    print(f"  TAVILY API: {st.session_state.slide_use_tavily_api}")
                    print(f"  TAVILYä½¿ç”¨å›æ•°: {st.session_state.slide_tavily_uses}")

                    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®å–å¾—
                    print("ğŸ“š ãƒãƒ£ãƒƒãƒˆå±¥æ­´å–å¾—ä¸­...")
                    chat_history = _gather_messages_context(
                        item_id,
                        st.session_state.slide_history_reference_count
                    )
                    print(f"  ãƒãƒ£ãƒƒãƒˆå±¥æ­´é•·: {len(chat_history)}æ–‡å­—")
                    print("ğŸ¤– NewSlideGeneratoråˆæœŸåŒ–ä¸­.")

                    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ†ãƒ³ãƒ—ãƒ¬ãŒã‚ã‚Œã°ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ä½¿ç”¨
                    uploaded_template_path = None
                    try:
                        if st.session_state.get("slide_template_bytes"):
                            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pptx")
                            tmp.write(st.session_state["slide_template_bytes"])
                            tmp.flush()
                            tmp.close()
                            uploaded_template_path = tmp.name
                            print(f"  ğŸ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ä½¿ç”¨: {uploaded_template_path}")

                        generator = (NewSlideGenerator(template_path=uploaded_template_path)
                                     if uploaded_template_path else NewSlideGenerator())
                        print("âœ… NewSlideGeneratoråˆæœŸåŒ–å®Œäº†")
                    finally:
                        if uploaded_template_path and os.path.exists(uploaded_template_path):
                            try:
                                os.remove(uploaded_template_path)
                            except Exception:
                                pass

                    # ææ¡ˆèª²é¡Œã®å–å¾—
                    print("ğŸ” ææ¡ˆèª²é¡Œå–å¾—ä¸­...")
                    proposal_issues = []
                    if st.session_state.get("last_proposal_id"):
                        proposal_issues = _get_proposal_issues_from_db(st.session_state["last_proposal_id"])
                    else:
                        proposal_issues = st.session_state.get("analyzed_issues", [])
                    print(f"  ææ¡ˆèª²é¡Œæ•°: {len(proposal_issues)}")

                    print("ğŸ¯ ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Ÿè¡Œä¸­...")
                    pptx_data = generator.create_presentation(
                        project_name=company_internal,  # æ¡ˆä»¶åã¨ã—ã¦ä¼æ¥­åã‚’ä½¿ç”¨
                        company_name=company_internal,
                        meeting_notes=st.session_state.slide_meeting_notes or "",
                        chat_history=chat_history,
                        products=selected,
                        proposal_issues=proposal_issues,
                        use_tavily=st.session_state.slide_use_tavily_api,
                        use_gpt=st.session_state.slide_use_gpt_api,
                        tavily_uses=st.session_state.slide_tavily_uses
                    )

                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®è¡¨ç¤º
                    print("âœ… ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Œäº†")
                    print(f"  ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(pptx_data)} ãƒã‚¤ãƒˆ")
                    st.success("ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")

                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"{company_internal}_ææ¡ˆæ›¸_{timestamp}.pptx"
                    st.download_button(
                        label="ğŸ“¥ ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=pptx_data,
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                        use_container_width=True,
                        type="primary"
                    )

                except Exception as e:
                    print(f"âŒ Streamlit: ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
                    st.error(f"ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.info("ä¸‹æ›¸ãã®ã¿ä½œæˆã•ã‚Œã¾ã—ãŸã€‚")

    # ä¸‹æ›¸ããƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    if st.session_state.slide_outline:
        with st.expander("ä¸‹æ›¸ããƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆJSONï¼‰", expanded=True):
            st.json(st.session_state.slide_outline)
