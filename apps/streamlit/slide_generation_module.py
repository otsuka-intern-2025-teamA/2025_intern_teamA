# slide_generation_module.py
# ---------------------------------------------------------
# ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆãƒšãƒ¼ã‚¸ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼è¿½åŠ ç‰ˆ Ã— ã‚«ãƒ¼ãƒ‰UIï¼‰
# - ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ­ã‚´ï¼æ¡ˆä»¶ä¸€è¦§ã¸æˆ»ã‚‹ï¼ˆå·¦ä¸‹å›ºå®šï¼‰ï¼ä¼æ¥­åï¼ææ¡ˆä»¶æ•°ï¼å±¥æ­´å‚ç…§ä»¶æ•°ï¼å•†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠï¼ã‚¯ãƒªã‚¢
# - æœ¬æ–‡ï¼šä¸Šæ®µãƒ˜ãƒƒãƒ€ï¼ˆå·¦ï¼è¦‹å‡ºã—ï¼å³ï¼å€™è£œå–å¾—ãƒœã‚¿ãƒ³ï¼‰ã€
#          1æ®µç›®ï¼å•†è«‡è©³ç´°ï¼ˆå¤§ï¼‰ï¼†å‚è€ƒè³‡æ–™ï¼ˆæ¨ªä¸¦ã³ï¼‰ã€
#          2æ®µç›®ï¼å·¦ï¼šèª²é¡Œåˆ†æï¼ˆè‡ªå‹•ï¼‰ï¼å³ï¼šå€™è£œã‚«ãƒ¼ãƒ‰
#          ä¸‹æ®µï¼ç”Ÿæˆã¨ãƒ‰ãƒ©ãƒ•ãƒˆJSON
# - ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ï¼šCSVâ†’ç²—é¸å®šâ†’LLMã§Top-Ké¸æŠœâ†’LLMã§80å­—è¦ç´„ï¼ˆå¤±æ•—æ™‚ã¯çŸ­ç¸®ï¼‰
# ---------------------------------------------------------

from __future__ import annotations

import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Any
import tempfile

import numpy as np
import pandas as pd

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv(".env", override=True)

# APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆä¼æ¥­åˆ†æã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´å–å¾—ã«ä½¿ç”¨ï¼‰
from lib.api import api_available, get_api_client

# ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# å…±é€šã‚¹ã‚¿ã‚¤ãƒ«
from lib.styles import (
    apply_company_analysis_page_styles,  # ã‚µã‚¤ãƒ‰ãƒãƒ¼åœ§ç¸®/ãƒ­ã‚´ã‚«ãƒ¼ãƒ‰/ä¸‹å¯„ã›CSSã‚’æµç”¨
    apply_main_styles,
    apply_slide_generation_page_styles,
    apply_title_styles,
    render_sidebar_logo_card,
    render_slide_generation_title,  # ã‚¿ã‚¤ãƒˆãƒ«æç”»ï¼ˆh1.slide-generation-titleï¼‰
)

import streamlit as st

# ç”»åƒ/ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
PROJECT_ROOT = Path(__file__).parent.parent.parent
LOGO_PATH = PROJECT_ROOT / "data" / "images" / "otsuka_logo.jpg"
ICON_PATH = PROJECT_ROOT / "data" / "images" / "otsuka_icon.png"
PRODUCTS_DIR = PROJECT_ROOT / "data" / "csv" / "products"
PLACEHOLDER_IMG = PROJECT_ROOT / "data" / "images" / "product_placeholder.png"

# --- æ–°ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ  ---
from lib.new_slide_generator import NewSlideGenerator


# =========================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
# =========================
def _ensure_session_defaults() -> None:
    ss = st.session_state
    ss.setdefault("selected_project", None)       # æ¡ˆä»¶ä¸€è¦§ã‹ã‚‰é·ç§»æ™‚ã«å…¥ã‚‹
    ss.setdefault("api_error", None)
    ss.setdefault("slide_meeting_notes", "")
    ss.setdefault("uploaded_files_store", [])     # file_uploaderã®ä¿å­˜ç”¨
    ss.setdefault("product_candidates", [])       # è¡¨ç¤ºç”¨ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®é…åˆ—
    ss.setdefault("slide_outline", None)
    ss.setdefault("slide_overview", "")
    ss.setdefault("slide_history_reference_count", 3)  # ç›´è¿‘Nå¾€å¾©å‚ç…§ï¼ˆãƒ‡ãƒ•ã‚©3ï¼‰
    ss.setdefault("slide_top_k", 1)                   # ææ¡ˆä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©1ï¼‰
    ss.setdefault("slide_products_dataset", "Auto")    # å•†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
    ss.setdefault("slide_use_tavily_api", True)        # TAVILY APIä½¿ç”¨ãƒ•ãƒ©ã‚°
    ss.setdefault("slide_use_gpt_api", True)           # GPT APIä½¿ç”¨ãƒ•ãƒ©ã‚°
    ss.setdefault("slide_tavily_uses", 1)              # è£½å“ã‚ãŸã‚Šã®TAVILY APIå‘¼ã³å‡ºã—å›æ•°
    # åŸ‹ã‚è¾¼ã¿æ¤œç´¢ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    ss.setdefault("_emb_cache", {})
    # è¡¨ç¤ºç”¨ï¼šèª²é¡Œåˆ†æçµæœï¼ˆUIã§è¦‹ã›ã‚‹ã ã‘ã€‚é¸å®šãƒ­ã‚¸ãƒƒã‚¯ã¯å¾“æ¥é€šã‚Šï¼‰
    ss.setdefault("analyzed_issues", [])
    ss.setdefault("slide_template_bytes", None)
    ss.setdefault("slide_template_name", None)


# =========================
# æ–°ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =========================
def _get_proposal_issues_from_db(proposal_id: str) -> list[dict[str, Any]]:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ææ¡ˆèª²é¡Œã‚’å–å¾—"""
    try:
        import sqlite3
        db_path = PROJECT_ROOT / "data" / "sqlite" / "app.db"
        if not db_path.exists():
            return []
        
        with sqlite3.connect(db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT idx, issue, weight, keywords_json 
                FROM proposal_issues 
                WHERE proposal_id = ? 
                ORDER BY idx
            """, (proposal_id,))
            
            rows = cursor.fetchall()
            issues = []
            for row in rows:
                import json
                keywords = json.loads(row[3]) if row[3] else []
                issues.append({
                    "issue": row[1],
                    "weight": row[2],
                    "keywords": keywords
                })
            
            return issues
    except Exception as e:
        print(f"ææ¡ˆèª²é¡Œå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []


# =========================
# ä¾¿åˆ©é–¢æ•°ï¼ˆä¾¡æ ¼ãªã©ï¼‰
# =========================
def _to_float(val):
    if val is None:
        return None
    if isinstance(val, (int, float)):
        if isinstance(val, float) and pd.isna(val):
            return None
        return float(val)
    if isinstance(val, str):
        s = val.strip().replace("Â¥", "").replace(",", "")
        if not s:
            return None
        try:
            return float(s)
        except Exception:
            return None
    return None


def _fmt_price(val) -> str:
    v = _to_float(val)
    return f"Â¥{int(round(v)):,}" if v is not None else "â€”"


# =========================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåé›†
# =========================
def _list_product_datasets() -> list[str]:
    """productsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€åã‚’åˆ—æŒ™ï¼ˆAutoã‚’å…ˆé ­ï¼‰"""
    if not PRODUCTS_DIR.exists():
        return ["Auto"]
    ds = ["Auto"]
    for p in PRODUCTS_DIR.iterdir():
        if p.is_dir():
            ds.append(p.name)
    return ds


def _gather_messages_context(item_id: str | None, history_n: int) -> str:
    """ä¼æ¥­åˆ†æã®ç›´è¿‘Nå¾€å¾©ï¼ˆ=2Nç™ºè¨€ï¼‰ã‚’ã¾ã¨ã‚ã¦æ–‡å­—åˆ—åŒ–"""
    if not (item_id and api_available()):
        return ""
    try:
        api = get_api_client()
        msgs = api.get_item_messages(item_id) or []
        take = min(len(msgs), history_n * 2)
        recent = msgs[-take:] if take > 0 else []
        ctx_lines = []
        for m in recent:
            role = m.get("role", "assistant")
            role_j = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if role == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
            ctx_lines.append(f"{role_j}: {m.get('content','')}")
        return "\n".join(ctx_lines)
    except Exception:
        return ""


def _load_products_from_csv(dataset: str) -> pd.DataFrame:
    """
    å•†æCSVã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆå­˜åœ¨ã‚«ãƒ©ãƒ ãŒç„¡ã‘ã‚Œã°ä½œæˆï¼‰
    æœŸå¾…ã‚«ãƒ©ãƒ : id(ç„¡ã‘ã‚Œã°ç”Ÿæˆ), name, category, price, description, tags
             ï¼‹ image_url/image/thumbnailï¼ˆä»»æ„ï¼‰, source_csvï¼ˆè¿½åŠ ï¼‰
    """
    frames: list[pd.DataFrame] = []
    if not PRODUCTS_DIR.exists():
        return pd.DataFrame()

    def _read_csvs(folder: Path):
        for csvp in folder.glob("*.csv"):
            try:
                df = pd.read_csv(csvp)
                for col in ["name", "category", "price", "description", "tags"]:
                    if col not in df.columns:
                        df[col] = None
                for col in ["image_url", "image", "thumbnail"]:
                    if col not in df.columns:
                        df[col] = None
                if "id" not in df.columns:
                    df["id"] = [f"{csvp.stem}-{i+1}" for i in range(len(df))]
                df["source_csv"] = csvp.stem
                frames.append(df[["id", "name", "category", "price", "description", "tags",
                                  "image_url", "image", "thumbnail", "source_csv"]])
            except Exception:
                continue

    if dataset == "Auto":
        for sub in PRODUCTS_DIR.iterdir():
            if sub.is_dir():
                _read_csvs(sub)
        _read_csvs(PRODUCTS_DIR)
    else:
        target = PRODUCTS_DIR / dataset
        if target.exists() and target.is_dir():
            _read_csvs(target)

    if not frames:
        return pd.DataFrame()
    return pd.concat(frames, ignore_index=True)


# =========================
# å‚è€ƒè³‡æ–™ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã€è¿½åŠ ã€‘
# =========================
def _extract_text_from_uploads(uploaded_files: list[Any], max_chars: int = 12000) -> str:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦é€£çµã—ã¦è¿”ã™ï¼ˆãƒ­ãƒã‚¹ãƒˆã«å‹•ãç°¡æ˜“å®Ÿè£…ï¼‰ã€‚
    - PDF: pypdf
    - DOCX: python-docx
    - PPTX: python-pptx
    - CSV: pandasã§å…ˆé ­æ•°è¡Œ
    - TXT: ãã®ã¾ã¾
    å¤±æ•—æ™‚ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®è¨˜éŒ²ã®ã¿ã€‚ç”»åƒ/OCRã¯æœªå¯¾å¿œã€‚
    """
    if not uploaded_files:
        return ""

    chunks: list[str] = []
    used_chars = 0

    def _append(text: str):
        nonlocal used_chars
        if not text:
            return
        remain = max_chars - used_chars
        if remain <= 0:
            return
        cut = text[:remain]
        chunks.append(cut)
        used_chars += len(cut)

    for f in uploaded_files:
        try:
            name = getattr(f, "name", "uploaded_file")
            lower = str(name).lower()
            # èª­ã¿å‡ºã—ä½ç½®ã‚’åˆæœŸåŒ–
            try:
                f.seek(0)
            except Exception:
                pass
            data = f.read()
            try:
                f.seek(0)
            except Exception:
                pass

            if lower.endswith(".pdf"):
                try:
                    import io

                    from pypdf import PdfReader  # pip install pypdf
                    reader = PdfReader(io.BytesIO(data))
                    page_limit = min(len(reader.pages), 30)
                    texts = []
                    for i in range(page_limit):
                        try:
                            texts.append(reader.pages[i].extract_text() or "")
                        except Exception:
                            continue
                    _append(f"\n[PDF:{name} æŠœç²‹]\n" + "\n".join(texts))
                except Exception:
                    _append(f"\n[PDF:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".docx"):
                try:
                    import io

                    from docx import Document  # pip install python-docx
                    doc = Document(io.BytesIO(data))
                    paras = [p.text for p in doc.paragraphs if p.text]
                    _append(f"\n[DOCX:{name} æŠœç²‹]\n" + "\n".join(paras))
                except Exception:
                    _append(f"\n[DOCX:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".pptx"):
                try:
                    import io

                    from pptx import Presentation  # pip install python-pptx
                    prs = Presentation(io.BytesIO(data))
                    slide_texts = []
                    for s in prs.slides:
                        buf = []
                        for shp in s.shapes:
                            try:
                                if hasattr(shp, "text"):
                                    t = shp.text or ""
                                    if t:
                                        buf.append(t)
                            except Exception:
                                continue
                        if buf:
                            slide_texts.append("\n".join(buf))
                    _append(f"\n[PPTX:{name} æŠœç²‹]\n" + "\n---\n".join(slide_texts))
                except Exception:
                    _append(f"\n[PPTX:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".csv"):
                try:
                    import io
                    tmp = io.BytesIO(data)
                    df = pd.read_csv(tmp)
                    head = df.head(20)
                    txt = head.to_csv(index=False)
                    _append(f"\n[CSV:{name} å…ˆé ­20è¡Œ]\n{txt}")
                except Exception:
                    _append(f"\n[CSV:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".txt"):
                try:
                    txt = data.decode("utf-8", errors="ignore")
                except Exception:
                    txt = str(data[:4000])
                _append(f"\n[TXT:{name}]\n{txt}")

            else:
                _append(f"\n[{name}]ï¼ˆæœªå¯¾å¿œ/ãƒã‚¤ãƒŠãƒªã®ãŸã‚æ¦‚è¦åæ˜ ã®ã¿ï¼‰")

        except Exception:
            continue

        if used_chars >= max_chars:
            break

    return "\n".join(chunks).strip()


# =========================
# æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç°¡æ˜“ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼‰
# =========================
def _simple_tokenize(text: str) -> list[str]:
    text = str(text or "").lower()
    text = re.sub(r"[^a-z0-9\u3040-\u30ff\u4e00-\u9fff]+", " ", text)
    toks = text.split()
    return [t for t in toks if len(t) >= 2]


def _fallback_rank_products(
    notes: str,
    messages_ctx: str,
    products_df: pd.DataFrame,
    top_pool: int
) -> list[dict[str, Any]]:
    """å•†è«‡ãƒ¡ãƒ¢ï¼‹å±¥æ­´ã®èªå¥ä¸€è‡´ã§ç´ æœ´ã«ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° â†’ ä¸Šä½ top_pool ã‚’è¿”ã™"""
    if products_df.empty:
        return []

    query_text = (notes or "") + "\n" + (messages_ctx or "")
    q_tokens = _simple_tokenize(query_text)

    def _row_text(row) -> str:
        return " ".join([
            str(row.get("name") or ""),
            str(row.get("category") or ""),
            str(row.get("description") or ""),
            str(row.get("tags") or ""),
        ]).lower()

    scored: list[tuple[float, dict[str, Any]]] = []
    for _, row in products_df.iterrows():
        t = _row_text(row)
        score = 0.0
        for tok in q_tokens:
            if tok in t:
                score += 1.0
        reason = f"ä¸€è‡´èªå¥æ•°={int(score)}" if score > 0 else "ä¸€è‡´ãªã—ï¼ˆä½ã‚¹ã‚³ã‚¢ï¼‰"
        scored.append((score, {
            "id": row.get("id"),
            "name": row.get("name"),
            "category": row.get("category"),
            "price": row.get("price"),
            "description": row.get("description"),
            "tags": row.get("tags"),
            "image_url": row.get("image_url"),
            "image": row.get("image"),
            "thumbnail": row.get("thumbnail"),
            "source_csv": row.get("source_csv"),
            "score": round(float(score), 2),
            "reason": reason,
        }))

    scored.sort(key=lambda x: (x[0], str(x[1]["name"]).lower()), reverse=True)
    return [d for _, d in scored[:top_pool]]


# =========================
# è£½å“æ¦‚è¦ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
# =========================
def _generate_product_overview(product: dict[str, Any]) -> str:
    """è£½å“ã®æ¦‚è¦ã‚’ç”Ÿæˆï¼ˆ80å­—ä»¥å†…ï¼‰"""
    description = product.get("description") or ""
    tags = product.get("tags") or ""
    name = product.get("name") or ""
    
    # èª¬æ˜æ–‡ã‹ã‚‰æ¦‚è¦ã‚’ç”Ÿæˆ
    if description:
        overview = description[:80]
        if len(description) > 80:
            overview += "..."
        return overview
    
    # ã‚¿ã‚°ã‹ã‚‰æ¦‚è¦ã‚’ç”Ÿæˆ
    if tags:
        overview = f"{name}: {tags[:60]}"
        if len(tags) > 60:
            overview += "..."
        return overview
    
    # åå‰ã®ã¿
    return name if name else "è£½å“ã®è©³ç´°æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“"


def _resolve_product_image_src(rec: dict[str, Any]) -> str | None:
    for key in ("image_url", "image", "thumbnail"):
        v = rec.get(key)
        if not v:
            continue
        s = str(v).strip()
        if s.startswith("http://") or s.startswith("https://"):
            return s
        p = (PROJECT_ROOT / s).resolve()
        if p.exists():
            return str(p)
    if PLACEHOLDER_IMG.exists():
        return str(PLACEHOLDER_IMG)
    return None

# =========================
# æ–°ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆç¶šãï¼‰
# =========================
def _analyze_pain_points_simple(notes: str, messages_ctx: str, uploads_text: str = "") -> list[dict[str, Any]]:
    """
    ç°¡æ˜“ç‰ˆèª²é¡Œåˆ†æï¼ˆLLMã‚’ä½¿ç”¨ã—ãªã„ï¼‰
    å•†è«‡ãƒ¡ãƒ¢ãƒ»ä¼šè©±æ–‡è„ˆãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã‹ã‚‰åŸºæœ¬çš„ãªèª²é¡Œã‚’æŠ½å‡º
    """
    issues = []
    
    # åŸºæœ¬çš„ãªèª²é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³
    basic_issues = [
        {"issue": "æ¥­å‹™åŠ¹ç‡åŒ–", "weight": 0.4, "keywords": ["åŠ¹ç‡åŒ–", "è‡ªå‹•åŒ–", "ç”Ÿç”£æ€§"]},
        {"issue": "ã‚³ã‚¹ãƒˆæœ€é©åŒ–", "weight": 0.3, "keywords": ["è²»ç”¨å‰Šæ¸›", "æœ€é©åŒ–", "ã‚³ã‚¹ãƒˆ"]},
        {"issue": "æƒ…å ±ç®¡ç†æ”¹å–„", "weight": 0.3, "keywords": ["æƒ…å ±å…±æœ‰", "ç®¡ç†", "ã‚·ã‚¹ãƒ†ãƒ "]}
    ]
    
    # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã«åŸºã¥ã„ã¦é‡ã¿ã‚’èª¿æ•´
    all_text = f"{notes} {messages_ctx} {uploads_text}".lower()
    
    if "åŠ¹ç‡" in all_text or "ç”Ÿç”£æ€§" in all_text:
        basic_issues[0]["weight"] = 0.5
        basic_issues[1]["weight"] = 0.25
        basic_issues[2]["weight"] = 0.25
    elif "ã‚³ã‚¹ãƒˆ" in all_text or "è²»ç”¨" in all_text:
        basic_issues[0]["weight"] = 0.25
        basic_issues[1]["weight"] = 0.5
        basic_issues[2]["weight"] = 0.25
    elif "æƒ…å ±" in all_text or "ç®¡ç†" in all_text:
        basic_issues[0]["weight"] = 0.25
        basic_issues[1]["weight"] = 0.25
        basic_issues[2]["weight"] = 0.5
    
    return basic_issues


# =========================
# å€™è£œæ¤œç´¢ï¼ˆç°¡æ˜“ç‰ˆï¼‰
# =========================
def _search_product_candidates(
    company: str,
    item_id: str | None,
    meeting_notes: str,
    top_k: int,
    history_n: int,
    dataset: str,
    uploaded_files: list[Any],
) -> list[dict[str, Any]]:
    # ä¼æ¥­åˆ†æã®æ–‡è„ˆ
    ctx = _gather_messages_context(item_id, history_n)

    # CSV èª­ã¿è¾¼ã¿
    df = _load_products_from_csv(dataset)
    if df.empty:
        return []

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    uploads_text = _extract_text_from_uploads(uploaded_files) if uploaded_files else ""

    # ç°¡æ˜“ç‰ˆèª²é¡Œåˆ†æ
    issues = _analyze_pain_points_simple(meeting_notes or "", ctx or "", uploads_text)

    # èªå¥ä¸€è‡´ã§ç²—é¸å®š
    top_pool = max(40, top_k * 4)
    pool = _fallback_rank_products(meeting_notes, ctx, df, top_pool=top_pool)

    # ä¸Šä½Kä»¶ã‚’é¸æŠ
    selected = pool[:top_k]
    
    # å„è£½å“ã®æ¦‚è¦ã‚’ç”Ÿæˆ
    for product in selected:
        product["overview"] = _generate_product_overview(product)
    
    return selected


# =========================
# ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆ
# =========================
def _make_outline_preview(company: str, meeting_notes: str, selected_products: list[dict[str, Any]], overview: str) -> dict[str, Any]:
    return {
        "title": f"{company} å‘ã‘ææ¡ˆè³‡æ–™ï¼ˆãƒ‰ãƒ©ãƒ•ãƒˆï¼‰",
        "overview": overview,
        "sections": [
            {"h2": "1. ã‚¢ã‚¸ã‚§ãƒ³ãƒ€", "bullets": ["èƒŒæ™¯", "èª²é¡Œæ•´ç†", "ææ¡ˆæ¦‚è¦", "å°å…¥åŠ¹æœ", "å°å…¥è¨ˆç”»", "æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"]},
            {"h2": "2. èƒŒæ™¯", "bullets": [f"{company}ã®äº‹æ¥­æ¦‚è¦ï¼ˆè¦ç´„ï¼‰", "å¸‚å ´å‹•å‘ãƒ»ç«¶åˆçŠ¶æ³ï¼ˆæŠœç²‹ï¼‰"]},
            {"h2": "3. ç¾çŠ¶ã®èª²é¡Œï¼ˆä»®èª¬ï¼‰", "bullets": ["ç”Ÿç”£æ€§/ã‚³ã‚¹ãƒˆ/å“è³ª/ã‚¹ãƒ”ãƒ¼ãƒ‰ã®è¦³ç‚¹ã‹ã‚‰3ã€œ5ç‚¹"]},
            {"h2": "4. ææ¡ˆæ¦‚è¦", "bullets": ["æœ¬ææ¡ˆã®ç›®çš„ï¼ç‹™ã„", "å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆé«˜ãƒ¬ãƒ™ãƒ«ï¼‰"]},
            {"h2": "5. æ¨å¥¨ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå€™è£œï¼‰", "bullets": [f"{len(selected_products)}ä»¶ã®å•†æå€™è£œã‚’æ•´ç†ãƒ»æ¯”è¼ƒ"]},
            {"h2": "6. å°å…¥åŠ¹æœï¼ˆå®šé‡/å®šæ€§ï¼‰", "bullets": ["KPIè¦‹è¾¼ã¿ / åŠ¹æœè©¦ç®—ã®æ–¹é‡"]},
            {"h2": "7. å°å…¥ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ¡ˆ", "bullets": ["PoC â†’ æœ¬å°å…¥ / ä½“åˆ¶ãƒ»å½¹å‰²åˆ†æ‹…"]},
        ],
        "meeting_notes_digest": meeting_notes[:300] + ("..." if len(meeting_notes) > 300 else ""),
        "products": [
            {
                "id": p.get("id"),
                "name": p.get("name"),
                "category": p.get("source_csv") or p.get("category"),
                "price": p.get("price"),
                "reason": p.get("reason"),
                "overview": p.get("overview"),
            }
            for p in selected_products
        ],
    }

# --- ææ¡ˆä¿å­˜ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆæ—¢å­˜ app.db ã‚’æ´»ç”¨ï¼‰ ---
import sqlite3, uuid, json as _json
DB_PATH = PROJECT_ROOT / "data" / "sqlite" / "app.db"  # â† ã“ã“ã ã‘ãƒ‘ã‚¹å¤‰æ›´

def _init_db_for_proposals():
    DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("""CREATE TABLE IF NOT EXISTS proposals(
            id TEXT PRIMARY KEY,
            project_item_id TEXT,
            company TEXT NOT NULL,
            meeting_notes TEXT,
            overview TEXT,
            created_at TEXT NOT NULL
        )""")
        c.execute("""CREATE TABLE IF NOT EXISTS proposal_issues(
            proposal_id TEXT NOT NULL,
            idx INTEGER NOT NULL,
            issue TEXT NOT NULL,
            weight REAL,
            keywords_json TEXT,
            FOREIGN KEY(proposal_id) REFERENCES proposals(id)
        )""")
        c.execute("""CREATE TABLE IF NOT EXISTS proposal_products(
            proposal_id TEXT NOT NULL,
            rank INTEGER NOT NULL,
            product_id TEXT,
            name TEXT,
            category TEXT,
            price TEXT,
            reason TEXT,
            overview TEXT,
            score REAL,
            source_csv TEXT,
            image_url TEXT,
            FOREIGN KEY(proposal_id) REFERENCES proposals(id)
        )""")
        conn.commit()

def _save_proposal_to_db(
    project_item_id: str | None,
    company: str,
    meeting_notes: str,
    overview: str,
    issues: list[dict[str, Any]],
    products: list[dict[str, Any]],
    created_at_iso: str
) -> str:
    """ææ¡ˆã²ã¨ã¾ã¨ã¾ã‚Šã‚’ä¿å­˜ã—ã€proposal_id ã‚’è¿”ã™ï¼ˆæ—¢å­˜ app.db ã«è¿½è¨˜ï¼‰ã€‚"""
    _init_db_for_proposals()
    pid = str(uuid.uuid4())
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute(
            "INSERT INTO proposals(id, project_item_id, company, meeting_notes, overview, created_at) VALUES(?,?,?,?,?,?)",
            (pid, project_item_id, company, meeting_notes, overview, created_at_iso)
        )
        # èª²é¡Œã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        for i, it in enumerate(issues or []):
            c.execute(
                "INSERT INTO proposal_issues(proposal_id, idx, issue, weight, keywords_json) VALUES(?,?,?,?,?)",
                (pid, i+1, it.get('issue',''), float(it.get('weight') or 0.0), _json.dumps(it.get('keywords') or [], ensure_ascii=False))
            )
        # æ¡ç”¨è£½å“ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        for r, p in enumerate(products or []):
            c.execute(
                """INSERT INTO proposal_products(
                    proposal_id, rank, product_id, name, category, price, reason, overview, score, source_csv, image_url
                ) VALUES(?,?,?,?,?,?,?,?,?,?,?)""",
                (
                    pid, r+1,
                    str(p.get('id','')) or None,
                    p.get('name',''),
                    p.get('source_csv') or p.get('category',''),
                    str(p.get('price','')),
                    p.get('reason',''),
                    p.get('overview',''),
                    float(p.get('score') or 0.0),
                    p.get('source_csv') or '',
                    p.get('image_url') or ''
                )
            )
        conn.commit()
    return pid




# =========================
# ãƒ¡ã‚¤ãƒ³æç”»ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆæ”¹ä¿®ã®ã¿ï¼‰
# =========================
def render_slide_generation_page():
    """ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆãƒšãƒ¼ã‚¸ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆé…ç½®ã‚’å¤‰æ›´ï¼šä¸Šæ®µï¼å•†è«‡è©³ç´°ï¼†è³‡æ–™ã€ä¸‹æ®µï¼èª²é¡Œåˆ†æï¼†å€™è£œã‚«ãƒ¼ãƒ‰ï¼‰"""
    _ensure_session_defaults()

    try:
        st.set_page_config(
            page_title="ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ",
            page_icon=str(ICON_PATH),
            layout="wide",
            initial_sidebar_state="expanded",
        )
    except Exception:
        pass

    # ã‚¹ã‚¿ã‚¤ãƒ«
    apply_main_styles(hide_sidebar=False, hide_header=True)
    apply_title_styles()
    apply_company_analysis_page_styles()  # ã‚µã‚¤ãƒ‰ãƒãƒ¼å…±é€š
    apply_slide_generation_page_styles()  # ã‚¿ã‚¤ãƒˆãƒ«ä½ç½®ãªã©

    # æ¡ˆä»¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    pj = st.session_state.get("selected_project")
    if pj:
        title_text = f"ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ - {pj['title']} / {pj['company']}"
        company_internal = pj.get("company", "")
        item_id = pj.get("id")
    else:
        title_text = "ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ"
        company_internal = ""
        item_id = None

    # ---------- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ----------
    with st.sidebar:
        render_sidebar_logo_card(LOGO_PATH)

        st.markdown("### è¨­å®š")
        st.text_input("ä¼æ¥­å", value=company_internal, key="slide_company_input", disabled=True)

        # --- ã“ã“ã‹ã‚‰â€œæ•°å­—ã¯ã™ã¹ã¦ä¸€è¦§é¸æŠå‹â€ & Session State æ—¢å®šå€¤ã®ã¿ä½¿ç”¨ ---
        st.selectbox("ææ¡ˆä»¶æ•°", options=list(range(1, 11)), key="slide_top_k")

        st.selectbox(
            "å±¥æ­´å‚ç…§ä»¶æ•°ï¼ˆå¾€å¾©ï¼‰",
            options=list(range(1, 11)),
            key="slide_history_reference_count",
            help="ä¼æ¥­åˆ†æã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ç›´è¿‘Nå¾€å¾©ã‚’æ–‡è„ˆã¨ã—ã¦ä½¿ç”¨",
        )

        st.selectbox(
            "å•†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            options=_list_product_datasets(),
            key="slide_products_dataset",
            help="data/csv/products/ é…ä¸‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã€‚Autoã¯è‡ªå‹•é¸æŠã€‚",
        )

        st.markdown("---")
        st.markdown("### AIè¨­å®š")
        
        st.checkbox("GPT APIä½¿ç”¨", key="slide_use_gpt_api")
        st.checkbox("TAVILY APIä½¿ç”¨", key="slide_use_tavily_api")

        if st.session_state.slide_use_tavily_api:
            st.selectbox("TAVILY APIå‘¼ã³å‡ºã—å›æ•°ï¼ˆè£½å“ã‚ãŸã‚Šï¼‰", options=list(range(1, 6)), key="slide_tavily_uses")

        sidebar_clear = st.button("ã‚¯ãƒªã‚¢", use_container_width=True, help="å€™è£œã‚’ç”»é¢å†…ã§ã‚¯ãƒªã‚¢")

        st.markdown("<div class='sidebar-bottom'>", unsafe_allow_html=True)
        if st.button("â† æ¡ˆä»¶ä¸€è¦§ã«æˆ»ã‚‹", use_container_width=True):
            st.session_state.current_page = "æ¡ˆä»¶ä¸€è¦§"
            st.session_state.page_changed = True
            st.rerun()
        st.markdown("</div>", unsafe_allow_html=True)

    # ---------- ã‚¿ã‚¤ãƒˆãƒ« ----------
    render_slide_generation_title(title_text)

    # ---------- è¦‹å‡ºã—è¡Œï¼ˆå·¦ï¼è¦‹å‡ºã— / å³ï¼å€™è£œå–å¾—ãƒœã‚¿ãƒ³ï¼‰ ----------
    head_l, head_r = st.columns([8, 2])
    with head_l:
        st.subheader("1. å•†å“ææ¡ˆ")
    with head_r:
        search_btn = st.button("å€™è£œã‚’å–å¾—", use_container_width=True)

    # ====================== ä¸Šæ®µï¼šå•†è«‡è©³ç´°ï¼ˆå¤§ï¼‰ï¼†å‚è€ƒè³‡æ–™ï¼ˆæ¨ªä¸¦ã³ï¼‰ ======================
    top_l, top_r = st.columns([3, 2], gap="large")  # å•†è«‡è©³ç´°ã‚’å¤§ãã‚ã«
    with top_l:
        st.markdown("**â— å•†è«‡ã®è©³ç´°**")
        st.text_area(
            label="å•†è«‡ã®è©³ç´°",
            key="slide_meeting_notes",
            height=154,
            label_visibility="collapsed",
            placeholder="ä¾‹ï¼šæ¥æœŸã®éœ€è¦äºˆæ¸¬ç²¾åº¦å‘ä¸Šã¨åœ¨åº«æœ€é©åŒ–ã€‚PoCã‹ã‚‰æ®µéšå°å…¥â€¦ ãªã©",
        )
    with top_r:
        st.markdown("**â— å‚è€ƒè³‡æ–™**")
        uploads = st.file_uploader(
            label="å‚è€ƒè³‡æ–™ï¼ˆä»»æ„ï¼‰",
            type=["pdf", "pptx", "docx", "csv", "png", "jpg", "jpeg", "txt"],
            accept_multiple_files=True,
            key="slide_uploader",
            label_visibility="collapsed",
            help="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã¯ç‰¹å¾´æŠ½å‡º/è¦ç´„ã«åˆ©ç”¨ï¼ˆä»Šå›ã®ä¿®æ­£ã§èª²é¡ŒæŠ½å‡ºã«åæ˜ ã•ã‚Œã¾ã™ï¼‰ã€‚",
        )
        if uploads:
            st.session_state.uploaded_files_store = uploads
            st.success(f"{len(uploads)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸã€‚")
        elif st.session_state.uploaded_files_store:
            st.caption(f"å‰å›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {len(st.session_state.uploaded_files_store)} ãƒ•ã‚¡ã‚¤ãƒ«")

    # ã€Œå€™è£œã‚’å–å¾—ã€æŠ¼ä¸‹æ™‚ï¼šå¾“æ¥ã®å€™è£œæ¤œç´¢ï¼ˆãƒãƒƒã‚¯ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—ï¼‰ï¼‹ UIè¡¨ç¤ºç”¨ã®èª²é¡Œåˆ†æçµæœã‚’è¨ˆç®—
    if search_btn:
        if not company_internal.strip():
            st.error("ä¼æ¥­ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ¡ˆä»¶ä¸€è¦§ã‹ã‚‰ä¼æ¥­ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")
        else:
            with st.spinner("å€™è£œã‚’æ¤œç´¢ä¸­â€¦"):
                candidates = _search_product_candidates(
                    company=company_internal,
                    item_id=item_id,
                    meeting_notes=st.session_state.slide_meeting_notes or "",
                    top_k=int(st.session_state.slide_top_k),
                    history_n=int(st.session_state.slide_history_reference_count),
                    dataset=st.session_state.slide_products_dataset,
                    uploaded_files=st.session_state.uploaded_files_store,
                )
            st.session_state.product_candidates = candidates

            # è¡¨ç¤ºç”¨ï¼šèª²é¡Œåˆ†æï¼ˆâ€»å€™è£œé¸å®šãƒ­ã‚¸ãƒƒã‚¯ã«ã¯å½±éŸ¿ã—ãªã„ï¼‰
            ctx_for_view = _gather_messages_context(item_id, int(st.session_state.slide_history_reference_count))
            uploads_text_for_view = _extract_text_from_uploads(st.session_state.uploaded_files_store) if st.session_state.uploaded_files_store else ""
            st.session_state.analyzed_issues = _analyze_pain_points_simple(
                st.session_state.slide_meeting_notes or "",
                ctx_for_view or "",
                uploads_text_for_view or ""
            )
        # ï¼ˆä»»æ„ï¼‰å€™è£œæ¤œç´¢ç›´å¾Œã«â€œãƒ‰ãƒ©ãƒ•ãƒˆâ€ä¿å­˜
        try:
            proposal_id = _save_proposal_to_db(
                project_item_id=item_id,
                company=company_internal,
                meeting_notes=st.session_state.slide_meeting_notes or "",
                overview=st.session_state.slide_overview or "",
                issues=st.session_state.analyzed_issues or [],
                products=st.session_state.product_candidates or [],
                created_at_iso=datetime.now().isoformat(timespec="seconds")
            )
            st.session_state["last_proposal_id"] = proposal_id
            st.info(f"ãƒ‰ãƒ©ãƒ•ãƒˆææ¡ˆã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ˆID: {proposal_id[:8]}â€¦ï¼‰")
        except Exception as e:
            st.warning(f"ãƒ‰ãƒ©ãƒ•ãƒˆä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


    if sidebar_clear:
        st.session_state.product_candidates = []
        st.session_state.analyzed_issues = []
        st.info("å€™è£œã¨èª²é¡Œåˆ†æè¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

    st.divider()

    # ====================== ä¸‹æ®µï¼šå·¦ï¼èª²é¡Œåˆ†æï¼å³ï¼å€™è£œã‚«ãƒ¼ãƒ‰ ======================
    bottom_l, bottom_r = st.columns([5, 7], gap="large")

    with bottom_l:
        st.markdown("**â— èª²é¡Œåˆ†æï¼ˆè‡ªå‹•ï¼‰**")
        issues = st.session_state.get("analyzed_issues") or []
        if not issues:
            st.caption("ã€å€™è£œã‚’å–å¾—ã€ã‚’æŠ¼ã™ã¨ã€å•†è«‡ãƒ¡ãƒ¢ãƒ»å±¥æ­´ãƒ»å‚è€ƒè³‡æ–™ã‹ã‚‰èª²é¡Œã‚’è‡ªå‹•æŠ½å‡ºã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")
        else:
            for i, it in enumerate(issues, start=1):
                with st.container(border=True):
                    st.markdown(f"**{i}. {it.get('issue','â€”')}**")
                    st.caption(f"é‡ã¿: {it.get('weight',0):.2f}")
                    kws = it.get("keywords") or []
                    if kws:
                        st.markdown("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: " + " / ".join(kws))

    with bottom_r:
        st.markdown("**â— å€™è£œï¼ˆã‚«ãƒ¼ãƒ‰è¡¨ç¤ºï¼‰**")
        recs = st.session_state.product_candidates or []
        if not recs:
            st.info("å€™è£œãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€å€™è£œã‚’å–å¾—ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        else:
            for r in recs:
                pid = str(r.get("id") or "")
                name = str(r.get("name") or "")
                cat_src = r.get("source_csv") or r.get("category") or "â€”"
                price_s = _fmt_price(r.get("price"))
                reason = r.get("reason") or "â€”"
                overview = r.get("overview") or "â€”"

                with st.container(border=True):
                    c1, c2 = st.columns([1, 3], gap="medium")
                    with c1:
                        img_src = _resolve_product_image_src(r)
                        if img_src and (img_src.startswith("http") or os.path.exists(img_src)):
                            st.image(img_src, use_container_width=True)
                        else:
                            st.markdown(
                                "<div style='width:100%;height:120px;border:1px solid #eee;border-radius:10px;"
                                "background:#f6f7f9;display:flex;align-items:center;justify-content:center;"
                                "color:#999;font-size:12px;'>ç”»åƒãªã—</div>",
                                unsafe_allow_html=True
                            )
                    with c2:
                        st.markdown(f"**{name}**")
                        st.caption(f"ã‚«ãƒ†ã‚´ãƒª: {cat_src} ï¼ ä¾¡æ ¼: {price_s} ï¼ ID: `{pid}`")
                        st.markdown(f"**ææ¡ˆç†ç”±**ï¼š{reason}")
                        st.markdown(f"**è£½å“æ¦‚è¦**ï¼š{overview}")

    st.divider()

    # ====================== 2. ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ ======================
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æ·»ä»˜
    tmpl_file = st.file_uploader(
        "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆ.pptxï¼‰ã‚’æ·»ä»˜ï¼ˆä»»æ„ï¼‰",
        type=["pptx"],
        key="slide_template_uploader",
        help="æ·»ä»˜ãŒç„¡ã„å ´åˆã¯æ—¢å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™"
    )

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¸ä¿æŒ
    if tmpl_file is not None:
        st.session_state.slide_template_bytes = tmpl_file.getvalue()
        st.session_state.slide_template_name = tmpl_file.name
        st.success(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸï¼š{tmpl_file.name}")
    else:
        # æ—¢ã«å‰å›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰åå‰ã ã‘è¡¨ç¤º
        current = st.session_state.get("slide_template_name")
        if current:
            st.caption(f"ç¾åœ¨ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼š{current}ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‚’ä½¿ç”¨ï¼‰")
        else:
            st.caption("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæœªæ·»ä»˜ï¼šæ—¢å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")

    st.subheader("2. ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ")

    row_l, row_r = st.columns([8, 2], vertical_alignment="center")
    with row_l:
        st.session_state.slide_overview = st.text_input(
            "æ¦‚èª¬ï¼ˆä»»æ„ï¼‰",
            value=st.session_state.slide_overview or "",
            placeholder="ä¾‹ï¼šåœ¨åº«æœ€é©åŒ–ã‚’ä¸­å¿ƒã«ã€éœ€è¦äºˆæ¸¬ã¨è£œå……è¨ˆç”»ã®é€£æºã‚’ææ¡ˆâ€¦",
        )
    with row_r:
        gen_btn = st.button("ç”Ÿæˆ", type="primary", use_container_width=True)

    if gen_btn:
        if not company_internal.strip():
            st.error("ä¼æ¥­ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        elif not st.session_state.product_candidates:
            st.error("è£½å“å€™è£œãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã€Œå€™è£œã‚’å–å¾—ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        else:
            selected = list(st.session_state.product_candidates or [])  # å…¨å€™è£œã‚’æ¡ç”¨
            
            # ä¸‹æ›¸ãã®ä½œæˆ
            outline = _make_outline_preview(
                company_internal,
                st.session_state.slide_meeting_notes or "",
                selected,
                st.session_state.slide_overview or "",
            )
            st.session_state.slide_outline = outline
            
            # ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
            with st.spinner("AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆä¸­..."):
                try:
                    print("ğŸš€ Streamlit: ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆé–‹å§‹")
                    print(f"  ä¼æ¥­å: {company_internal}")
                    print(f"  è£½å“æ•°: {len(selected)}")
                    print(f"  GPT API: {st.session_state.slide_use_gpt_api}")
                    print(f"  TAVILY API: {st.session_state.slide_use_tavily_api}")
                    print(f"  TAVILYä½¿ç”¨å›æ•°: {st.session_state.slide_tavily_uses}")
                    
                    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®å–å¾—
                    print("ğŸ“š ãƒãƒ£ãƒƒãƒˆå±¥æ­´å–å¾—ä¸­...")
                    chat_history = _gather_messages_context(
                        item_id, 
                        st.session_state.slide_history_reference_count
                    )
                    print(f"  ãƒãƒ£ãƒƒãƒˆå±¥æ­´é•·: {len(chat_history)}æ–‡å­—")
                    print("ğŸ¤– NewSlideGeneratoråˆæœŸåŒ–ä¸­.")

                    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ†ãƒ³ãƒ—ãƒ¬ãŒã‚ã‚Œã°ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ä½¿ç”¨
                    uploaded_template_path = None
                    try:
                        if st.session_state.get("slide_template_bytes"):
                            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pptx")
                            tmp.write(st.session_state["slide_template_bytes"])
                            tmp.flush()
                            tmp.close()
                            uploaded_template_path = tmp.name
                            print(f"  ğŸ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ä½¿ç”¨: {uploaded_template_path}")

                        generator = (NewSlideGenerator(template_path=uploaded_template_path) if uploaded_template_path else NewSlideGenerator())
                        print("âœ… NewSlideGeneratoråˆæœŸåŒ–å®Œäº†")
                    finally:
                        if uploaded_template_path and os.path.exists(uploaded_template_path):
                            try:
                                os.remove(uploaded_template_path)
                            except Exception:
                                pass
                    
                    # ææ¡ˆèª²é¡Œã®å–å¾—
                    print("ğŸ” ææ¡ˆèª²é¡Œå–å¾—ä¸­...")
                    proposal_issues = []
                    if st.session_state.get("last_proposal_id"):
                        proposal_issues = _get_proposal_issues_from_db(st.session_state["last_proposal_id"])
                    else:
                        proposal_issues = st.session_state.get("analyzed_issues", [])
                    
                    print(f"  ææ¡ˆèª²é¡Œæ•°: {len(proposal_issues)}")
                    
                    print("ğŸ¯ ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Ÿè¡Œä¸­...")
                    pptx_data = generator.create_presentation(
                        project_name=company_internal,  # æ¡ˆä»¶åã¨ã—ã¦ä¼æ¥­åã‚’ä½¿ç”¨
                        company_name=company_internal,
                        meeting_notes=st.session_state.slide_meeting_notes or "",
                        chat_history=chat_history,
                        products=selected,
                        proposal_issues=proposal_issues,
                        use_tavily=st.session_state.slide_use_tavily_api,
                        use_gpt=st.session_state.slide_use_gpt_api,
                        tavily_uses=st.session_state.slide_tavily_uses
                    )
                    
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®è¡¨ç¤º
                    print("âœ… ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Œäº†")
                    print(f"  ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(pptx_data)} ãƒã‚¤ãƒˆ")
                    
                    st.success("ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"{company_internal}_ææ¡ˆæ›¸_{timestamp}.pptx"
                    
                    st.download_button(
                        label="ğŸ“¥ ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=pptx_data,
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                        use_container_width=True,
                        type="primary"
                    )
                    
                except Exception as e:
                    print(f"âŒ Streamlit: ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
                    print(f"âŒ Streamlit: ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
                    st.error(f"ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.info("ä¸‹æ›¸ãã®ã¿ä½œæˆã•ã‚Œã¾ã—ãŸã€‚")

    if st.session_state.slide_outline:
        with st.expander("ä¸‹æ›¸ããƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆJSONï¼‰", expanded=True):
            st.json(st.session_state.slide_outline)
