# slide_generation_module.py
# ---------------------------------------------------------
# ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆãƒšãƒ¼ã‚¸ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼è¿½åŠ ç‰ˆ Ã— ã‚«ãƒ¼ãƒ‰UIï¼‰
# - ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ­ã‚´ï¼æ¡ˆä»¶ä¸€è¦§ã¸æˆ»ã‚‹ï¼ˆå·¦ä¸‹å›ºå®šï¼‰ï¼ä¼æ¥­åï¼ææ¡ˆä»¶æ•°ï¼å±¥æ­´å‚ç…§ä»¶æ•°ï¼å•†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠï¼ã‚¯ãƒªã‚¢
# - æœ¬æ–‡ï¼šä¸Šæ®µãƒ˜ãƒƒãƒ€ï¼ˆå·¦ï¼è¦‹å‡ºã—ï¼å³ï¼ææ¡ˆå•†å“å–å¾—ãƒœã‚¿ãƒ³ï¼‰ã€
#          1æ®µç›®ï¼å•†è«‡è©³ç´°ï¼ˆå¤§ï¼‰ï¼†å‚è€ƒè³‡æ–™ï¼ˆæ¨ªä¸¦ã³ï¼‰ã€
#          2æ®µç›®ï¼å·¦ï¼šèª²é¡Œåˆ†æçµæœï¼å³ï¼šææ¡ˆå•†å“ã‚«ãƒ¼ãƒ‰
#          ä¸‹æ®µï¼ç”Ÿæˆã¨ãƒ‰ãƒ©ãƒ•ãƒˆJSON
# - ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ï¼šCSVâ†’ç²—é¸å®šâ†’LLMã§Top-Ké¸æŠœâ†’LLMã§80å­—è¦ç´„ï¼ˆå¤±æ•—æ™‚ã¯çŸ­ç¸®ï¼‰
# ---------------------------------------------------------

from __future__ import annotations

import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Any
import tempfile

import numpy as np
import pandas as pd

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv(".env", override=True)

# APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆä¼æ¥­åˆ†æã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´å–å¾—ã«ä½¿ç”¨ï¼‰
from lib.api import api_available, get_api_client

# ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# å…±é€šã‚¹ã‚¿ã‚¤ãƒ«
from lib.styles import (
    apply_company_analysis_page_styles,  # ã‚µã‚¤ãƒ‰ãƒãƒ¼åœ§ç¸®/ãƒ­ã‚´ã‚«ãƒ¼ãƒ‰/ä¸‹å¯„ã›CSSã‚’æµç”¨
    apply_main_styles,
    apply_slide_generation_page_styles,
    apply_title_styles,
    render_sidebar_logo_card,
    render_slide_generation_title,  # ã‚¿ã‚¤ãƒˆãƒ«æç”»ï¼ˆh1.slide-generation-titleï¼‰
)

import streamlit as st

# ç”»åƒ/ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
PROJECT_ROOT = Path(__file__).parent.parent.parent
LOGO_PATH = PROJECT_ROOT / "data" / "images" / "otsuka_logo.jpg"
ICON_PATH = PROJECT_ROOT / "data" / "images" / "otsuka_icon.png"
PRODUCTS_DIR = PROJECT_ROOT / "data" / "csv" / "products"
PLACEHOLDER_IMG = PROJECT_ROOT / "data" / "images" / "product_placeholder.png"

# --- æ–°ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ  ---
from lib.new_slide_generator import NewSlideGenerator


# =========================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
# =========================
def _ensure_session_defaults() -> None:
    ss = st.session_state
    ss.setdefault("selected_project", None)       # æ¡ˆä»¶ä¸€è¦§ã‹ã‚‰é·ç§»æ™‚ã«å…¥ã‚‹
    ss.setdefault("api_error", None)
    ss.setdefault("slide_meeting_notes", "")
    ss.setdefault("uploaded_files_store", [])     # file_uploaderã®ä¿å­˜ç”¨
    ss.setdefault("product_candidates", [])       # è¡¨ç¤ºç”¨ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®é…åˆ—
    ss.setdefault("slide_outline", None)
    ss.setdefault("slide_overview", "")
    ss.setdefault("slide_history_reference_count", 3)  # ç›´è¿‘Nå¾€å¾©å‚ç…§ï¼ˆãƒ‡ãƒ•ã‚©3ï¼‰
    ss.setdefault("slide_top_k", 1)                   # ææ¡ˆä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©1ï¼‰
    ss.setdefault("slide_products_dataset", "Auto")    # å•†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
    ss.setdefault("slide_use_tavily_api", True)        # TAVILY APIä½¿ç”¨ãƒ•ãƒ©ã‚°
    ss.setdefault("slide_use_gpt_api", True)           # GPT APIä½¿ç”¨ãƒ•ãƒ©ã‚°
    ss.setdefault("slide_tavily_uses", 1)              # è£½å“ã‚ãŸã‚Šã®TAVILY APIå‘¼ã³å‡ºã—å›æ•°
    # åŸ‹ã‚è¾¼ã¿æ¤œç´¢ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    ss.setdefault("_emb_cache", {})
    # è¡¨ç¤ºç”¨ï¼šèª²é¡Œåˆ†æçµæœï¼ˆUIã§è¦‹ã›ã‚‹ã ã‘ã€‚é¸å®šãƒ­ã‚¸ãƒƒã‚¯ã¯å¾“æ¥é€šã‚Šï¼‰
    ss.setdefault("analyzed_issues", [])
    ss.setdefault("slide_template_bytes", None)
    ss.setdefault("slide_template_name", None)


# =========================
# æ–°ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =========================
def _get_proposal_issues_from_db(proposal_id: str) -> list[dict[str, Any]]:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ææ¡ˆèª²é¡Œã‚’å–å¾—"""
    try:
        import sqlite3
        db_path = PROJECT_ROOT / "data" / "sqlite" / "app.db"
        if not db_path.exists():
            return []
        
        with sqlite3.connect(db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT idx, issue, weight, keywords_json 
                FROM proposal_issues 
                WHERE proposal_id = ? 
                ORDER BY idx
            """, (proposal_id,))
            
            rows = cursor.fetchall()
            issues = []
            for row in rows:
                import json
                keywords = json.loads(row[3]) if row[3] else []
                issues.append({
                    "issue": row[1],
                    "weight": row[2],
                    "keywords": keywords
                })
            
            return issues
    except Exception as e:
        print(f"ææ¡ˆèª²é¡Œå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []


# =========================
# ä¾¿åˆ©é–¢æ•°ï¼ˆä¾¡æ ¼ãªã©ï¼‰
# =========================
def _to_float(val):
    if val is None:
        return None
    if isinstance(val, (int, float)):
        if isinstance(val, float) and pd.isna(val):
            return None
        return float(val)
    if isinstance(val, str):
        s = val.strip().replace("Â¥", "").replace(",", "")
        if not s:
            return None
        try:
            return float(s)
        except Exception:
            return None
    return None


def _fmt_price(val) -> str:
    v = _to_float(val)
    return f"Â¥{int(round(v)):,}" if v is not None else "â€”"


# =========================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåé›†
# =========================
def _list_product_datasets() -> list[str]:
    """productsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€åã‚’åˆ—æŒ™ï¼ˆAutoã‚’å…ˆé ­ï¼‰"""
    if not PRODUCTS_DIR.exists():
        return ["Auto"]
    ds = ["Auto"]
    for p in PRODUCTS_DIR.iterdir():
        if p.is_dir():
            ds.append(p.name)
    return ds


def _gather_messages_context(item_id: str | None, history_n: int) -> str:
    """ä¼æ¥­åˆ†æã®ç›´è¿‘Nå¾€å¾©ï¼ˆ=2Nç™ºè¨€ï¼‰ã‚’ã¾ã¨ã‚ã¦æ–‡å­—åˆ—åŒ–"""
    if not (item_id and api_available()):
        return ""
    try:
        api = get_api_client()
        msgs = api.get_item_messages(item_id) or []
        take = min(len(msgs), history_n * 2)
        recent = msgs[-take:] if take > 0 else []
        ctx_lines = []
        for m in recent:
            role = m.get("role", "assistant")
            role_j = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if role == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
            ctx_lines.append(f"{role_j}: {m.get('content','')}")
        return "\n".join(ctx_lines)
    except Exception:
        return ""


def _load_products_from_csv(dataset: str) -> pd.DataFrame:
    """
    å•†æCSVã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆå­˜åœ¨ã‚«ãƒ©ãƒ ãŒç„¡ã‘ã‚Œã°ä½œæˆï¼‰
    æœŸå¾…ã‚«ãƒ©ãƒ : id(ç„¡ã‘ã‚Œã°ç”Ÿæˆ), name, category, price, description, tags
             ï¼‹ image_url/image/thumbnailï¼ˆä»»æ„ï¼‰, source_csvï¼ˆè¿½åŠ ï¼‰
    """
    frames: list[pd.DataFrame] = []
    if not PRODUCTS_DIR.exists():
        return pd.DataFrame()

    def _read_csvs(folder: Path):
        for csvp in folder.glob("*.csv"):
            try:
                df = pd.read_csv(csvp)
                for col in ["name", "category", "price", "description", "tags"]:
                    if col not in df.columns:
                        df[col] = None
                for col in ["image_url", "image", "thumbnail"]:
                    if col not in df.columns:
                        df[col] = None
                if "id" not in df.columns:
                    df["id"] = [f"{csvp.stem}-{i+1}" for i in range(len(df))]
                df["source_csv"] = csvp.stem
                frames.append(df[["id", "name", "category", "price", "description", "tags",
                                  "image_url", "image", "thumbnail", "source_csv"]])
            except Exception:
                continue

    if dataset == "Auto":
        for sub in PRODUCTS_DIR.iterdir():
            if sub.is_dir():
                _read_csvs(sub)
        _read_csvs(PRODUCTS_DIR)
    else:
        target = PRODUCTS_DIR / dataset
        if target.exists() and target.is_dir():
            _read_csvs(target)

    if not frames:
        return pd.DataFrame()
    return pd.concat(frames, ignore_index=True)


# =========================
# å‚è€ƒè³‡æ–™ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã€è¿½åŠ ã€‘
# =========================
def _extract_text_from_uploads(uploaded_files: list[Any], max_chars: int = 12000) -> str:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦é€£çµã—ã¦è¿”ã™ï¼ˆãƒ­ãƒã‚¹ãƒˆã«å‹•ãç°¡æ˜“å®Ÿè£…ï¼‰ã€‚
    - PDF: pypdf
    - DOCX: python-docx
    - PPTX: python-pptx
    - CSV: pandasã§å…ˆé ­æ•°è¡Œ
    - TXT: ãã®ã¾ã¾
    å¤±æ•—æ™‚ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®è¨˜éŒ²ã®ã¿ã€‚ç”»åƒ/OCRã¯æœªå¯¾å¿œã€‚
    """
    if not uploaded_files:
        return ""

    chunks: list[str] = []
    used_chars = 0

    def _append(text: str):
        nonlocal used_chars
        if not text:
            return
        remain = max_chars - used_chars
        if remain <= 0:
            return
        cut = text[:remain]
        chunks.append(cut)
        used_chars += len(cut)

    for f in uploaded_files:
        try:
            name = getattr(f, "name", "uploaded_file")
            lower = str(name).lower()
            # èª­ã¿å‡ºã—ä½ç½®ã‚’åˆæœŸåŒ–
            try:
                f.seek(0)
            except Exception:
                pass
            data = f.read()
            try:
                f.seek(0)
            except Exception:
                pass

            if lower.endswith(".pdf"):
                try:
                    import io

                    from pypdf import PdfReader  # pip install pypdf
                    reader = PdfReader(io.BytesIO(data))
                    page_limit = min(len(reader.pages), 30)
                    texts = []
                    for i in range(page_limit):
                        try:
                            texts.append(reader.pages[i].extract_text() or "")
                        except Exception:
                            continue
                    _append(f"\n[PDF:{name} æŠœç²‹]\n" + "\n".join(texts))
                except Exception:
                    _append(f"\n[PDF:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".docx"):
                try:
                    import io

                    from docx import Document  # pip install python-docx
                    doc = Document(io.BytesIO(data))
                    paras = [p.text for p in doc.paragraphs if p.text]
                    _append(f"\n[DOCX:{name} æŠœç²‹]\n" + "\n".join(paras))
                except Exception:
                    _append(f"\n[DOCX:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".pptx"):
                try:
                    import io

                    from pptx import Presentation  # pip install python-pptx
                    prs = Presentation(io.BytesIO(data))
                    slide_texts = []
                    for s in prs.slides:
                        buf = []
                        for shp in s.shapes:
                            try:
                                if hasattr(shp, "text"):
                                    t = shp.text or ""
                                    if t:
                                        buf.append(t)
                            except Exception:
                                continue
                        if buf:
                            slide_texts.append("\n".join(buf))
                    _append(f"\n[PPTX:{name} æŠœç²‹]\n" + "\n---\n".join(slide_texts))
                except Exception:
                    _append(f"\n[PPTX:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".csv"):
                try:
                    import io
                    tmp = io.BytesIO(data)
                    df = pd.read_csv(tmp)
                    head = df.head(20)
                    txt = head.to_csv(index=False)
                    _append(f"\n[CSV:{name} å…ˆé ­20è¡Œ]\n{txt}")
                except Exception:
                    _append(f"\n[CSV:{name}]ï¼ˆæŠ½å‡ºå¤±æ•—â†’ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿åæ˜ ï¼‰")

            elif lower.endswith(".txt"):
                try:
                    txt = data.decode("utf-8", errors="ignore")
                except Exception:
                    txt = str(data[:4000])
                _append(f"\n[TXT:{name}]\n{txt}")

            else:
                _append(f"\n[{name}]ï¼ˆæœªå¯¾å¿œ/ãƒã‚¤ãƒŠãƒªã®ãŸã‚æ¦‚è¦åæ˜ ã®ã¿ï¼‰")

        except Exception:
            continue

        if used_chars >= max_chars:
            break

    return "\n".join(chunks).strip()


# =========================
# æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç°¡æ˜“ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼‰
# =========================
def _simple_tokenize(text: str) -> list[str]:
    text = str(text or "").lower()
    text = re.sub(r"[^a-z0-9\u3040-\u30ff\u4e00-\u9fff]+", " ", text)
    toks = text.split()
    return [t for t in toks if len(t) >= 2]


def _fallback_rank_products(
    notes: str,
    messages_ctx: str,
    products_df: pd.DataFrame,
    top_pool: int
) -> list[dict[str, Any]]:
    """å•†è«‡ãƒ¡ãƒ¢ï¼‹å±¥æ­´ã®èªå¥ä¸€è‡´ã§ç´ æœ´ã«ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° â†’ ä¸Šä½ top_pool ã‚’è¿”ã™"""
    if products_df.empty:
        return []

    query_text = (notes or "") + "\n" + (messages_ctx or "")
    q_tokens = _simple_tokenize(query_text)

    def _row_text(row) -> str:
        return " ".join([
            str(row.get("name") or ""),
            str(row.get("category") or ""),
            str(row.get("description") or ""),
            str(row.get("tags") or ""),
        ]).lower()

    scored: list[tuple[float, dict[str, Any]]] = []
    for _, row in products_df.iterrows():
        t = _row_text(row)
        score = 0.0
        for tok in q_tokens:
            if tok in t:
                score += 1.0
        reason = f"ä¸€è‡´èªå¥æ•°={int(score)}" if score > 0 else "ä¸€è‡´ãªã—ï¼ˆä½ã‚¹ã‚³ã‚¢ï¼‰"
        scored.append((score, {
            "id": row.get("id"),
            "name": row.get("name"),
            "category": row.get("category"),
            "price": row.get("price"),
            "description": row.get("description"),
            "tags": row.get("tags"),
            "image_url": row.get("image_url"),
            "image": row.get("image"),
            "thumbnail": row.get("thumbnail"),
            "source_csv": row.get("source_csv"),
            "score": round(float(score), 2),
            "reason": reason,
        }))

    scored.sort(key=lambda x: (x[0], str(x[1]["name"]).lower()), reverse=True)
    return [d for _, d in scored[:top_pool]]


# =========================
# è£½å“æ¦‚è¦ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
# =========================
def _extract_json(s: str) -> dict[str, Any]:
    s = (s or "").strip()
    if not s:
        return {}
    if s.lstrip().startswith("{"):
        try:
            return json.loads(s)
        except Exception:
            pass
    try:
        start = s.find("{")
        end = s.rfind("}")
        if start >= 0 and end > start:
            return json.loads(s[start:end + 1])
    except Exception:
        return {}
    return {}


def _llm_pick_products(pool: list[dict[str, Any]], top_k: int, company: str, notes: str, ctx: str, issues: list[dict[str, Any]] | None = None) -> list[dict[str, Any]]:
    """
    ã‚«ã‚¿ãƒ­ã‚°ï¼ˆpoolï¼‰ã‹ã‚‰ LLM ã§ Top-K ã‚’é¸æŠœã—ã€çŸ­ã„ç†ç”±ã¨ä¿¡é ¼åº¦ã‚’ä»˜ä¸ã€‚
    issues ãŒä¸ãˆã‚‰ã‚Œã‚Œã°ã€èª²é¡ŒIDã¨ã®å¯¾å¿œä»˜ã‘ã¨æ ¹æ‹ ã‚’æ±‚ã‚ã‚‹ã€‚
    """
    if not pool:
        return []
    client, model = _get_chat_client()

    lines = []
    for p in pool:
        desc = (p.get("description") or "")[:200]
        tags = (p.get("tags") or "")[:120]
        cat = p.get("source_csv") or p.get("category") or ""
        price = p.get("price")
        price_s = f"Â¥{int(_to_float(price)):,}" if _to_float(price) is not None else "â€”"
        lines.append(f"- id:{p['id']} | name:{p.get('name','')} | category:{cat} | price:{price_s} | tags:{tags} | desc:{desc}")
    catalog = "\n".join(lines)
    # èª²é¡Œãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—åŒ–
    issues_text = ""
    if issues:
        parts = []
        for i, it in enumerate(issues):
            kw = ", ".join(it.get("keywords") or [])
            parts.append(f"[{i}] {it.get('issue')} (é‡ã¿={it.get('weight'):.2f}; é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰={kw})")
        issues_text = "\n".join(parts)
    # JSONã‚¹ã‚­ãƒ¼ãƒ
    schema = {
        "recommendations": [
            {
                "id": "<id>",
                "reason": "<120å­—ä»¥å†…>",
                "confidence": 0.0,
            }
        ]
    }
    if issues:
        schema = {
            "recommendations": [
                {
                    "id": "<id>",
                    "reason": "<120å­—ä»¥å†…>",
                    "confidence": 0.0,
                    "solved_issue_ids": [0],
                    "evidence": "<æ ¹æ‹ æŠœç²‹>"
                }
            ]
        }
    user = f"""ã‚ãªãŸã¯B2Bãƒ—ãƒªã‚»ãƒ¼ãƒ«ã‚¹ã®ææ¡ˆãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ä¼šç¤¾æƒ…å ±ã¨å•†è«‡è©³ç´°ã€ä¼šè©±æ–‡è„ˆã«åŸºã¥ã„ã¦ã€å€™è£œã‚«ã‚¿ãƒ­ã‚°ã‹ã‚‰ Top-{top_k} ã®è£½å“ã‚’é¸ã³ã€æ—¥æœ¬èªã§çŸ­ã„ç†ç”±ï¼ˆ120å­—ä»¥å†…ï¼‰ã¨ä¿¡é ¼åº¦(0-1)ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚
å¿…ãšã‚«ã‚¿ãƒ­ã‚°ã«å­˜åœ¨ã™ã‚‹ id ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯ JSON ã®ã¿ã§ã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ¼ãƒã«å¾“ã£ã¦ãã ã•ã„:
{json.dumps(schema, ensure_ascii=False)}

# ä¼šç¤¾: {company or "(ãªã—)"}
# å•†è«‡è©³ç´°:
{notes or "(ãªã—)"}

# ä¼šè©±æ–‡è„ˆ:
{ctx or "(ãªã—)"}

# èª²é¡Œä¸€è¦§:
{issues_text or "(ãªã—)"}

# å€™è£œã‚«ã‚¿ãƒ­ã‚°:
{catalog}
"""
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ­£ç¢ºã§ç°¡æ½”ãªæ—¥æœ¬èªã§å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": user},
            ],
            response_format={"type": "json_object"},
        )
        txt = resp.choices[0].message.content or ""
    except Exception:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ­£ç¢ºã§ç°¡æ½”ãªæ—¥æœ¬èªã§å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": user},
            ],
        )
        txt = resp.choices[0].message.content or ""

    data = _extract_json(txt)
    recs = data.get("recommendations", []) if isinstance(data, dict) else []
    if not recs:
        return []
    pool_map = {str(p["id"]): p for p in pool}
    out: list[dict[str, Any]] = []
    for r in recs:
        pid = str(r.get("id", "")).strip()
        if not pid or pid not in pool_map:
            continue
        src = pool_map[pid]
        # reason / confidence
        reason = (r.get("reason") or "").strip() or src.get("reason")
        conf = float(r.get("confidence", 0.0)) if r.get("confidence") is not None else float(src.get("score", 0.0))
        solved_ids = r.get("solved_issue_ids") if isinstance(r.get("solved_issue_ids"), list) else []
        evidence = (r.get("evidence") or "").strip()
        out.append({
            **src,
            "reason": reason,
            "score": conf,
            "solved_issue_ids": solved_ids,
            "evidence": evidence,
        })
        if len(out) >= top_k:
            break
    return out


def _summarize_overviews_llm(cands: list[dict[str, Any]]) -> None:
    """å„è£½å“ã®æ¦‚è¦ã‚’ 80å­—ä»¥å†…ã§ LLM è¦ç´„ï¼ˆå¤±æ•—æ™‚ã¯èª¬æ˜ã‚’çŸ­ç¸®ï¼‰"""
    items = []
    has_any = False
    for c in cands:
        mat = c.get("description") or c.get("tags") or c.get("name") or ""
        if mat:
            has_any = True
        items.append({"id": str(c.get("id") or ""), "name": c.get("name") or "", "material": str(mat)[:600]})

    if not has_any:
        for c in cands:
            c["overview"] = "â€”"
        return

    try:
        client, model = _get_chat_client()
        payload = "\n".join([f"- id:{it['id']} / åç§°:{it['name']}\n  å†…å®¹:{it['material']}" for it in items])
        prompt = f"""å„è£½å“ã®ã€Œè£½å“æ¦‚è¦ã€ã‚’æ—¥æœ¬èªã§1ã€œ2æ–‡ã€æœ€å¤§80å­—ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚äº‹å®Ÿã®è¿½åŠ ãƒ»èª‡å¼µã¯ç¦æ­¢ã€‚
å‡ºåŠ›ã¯ JSON ã®ã¿:
{{"summaries":[{{"id":"<id>","overview":"<80å­—ä»¥å†…>"}}]}}
å…¥åŠ›:
{payload}"""
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ç°¡æ½”ã§æ­£ç¢ºãªæ—¥æœ¬èªã®è¦ç´„ã‚’ä½œã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                response_format={"type": "json_object"},
            )
            txt = resp.choices[0].message.content or ""
        except Exception:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ç°¡æ½”ã§æ­£ç¢ºãªæ—¥æœ¬èªã®è¦ç´„ã‚’ä½œã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": prompt},
                ],
            )
            txt = resp.choices[0].message.content or ""

        data = _extract_json(txt)
        mp = {}
        if isinstance(data, dict):
            for s in data.get("summaries", []) or []:
                pid = str(s.get("id") or "")
                ov = (s.get("overview") or "").strip()
                if pid and ov:
                    mp[pid] = ov

        for c in cands:
            pid = str(c.get("id") or "")
            base = c.get("description") or c.get("tags") or ""
            fallback = (base[:80] + ("â€¦" if base and len(base) > 80 else "")) if base else "â€”"
            c["overview"] = mp.get(pid, fallback)
    except Exception:
        for c in cands:
            base = c.get("description") or c.get("tags") or ""
            c["overview"] = (base[:80] + ("â€¦" if base and len(base) > 80 else "")) if base else "â€”"


def _resolve_product_image_src(rec: dict[str, Any]) -> str | None:
    for key in ("image_url", "image", "thumbnail"):
        v = rec.get(key)
        if not v:
            continue
        s = str(v).strip()
        if s.startswith("http://") or s.startswith("https://"):
            return s
        p = (PROJECT_ROOT / s).resolve()
        if p.exists():
            return str(p)
    if PLACEHOLDER_IMG.exists():
        return str(PLACEHOLDER_IMG)
    return None

# =========================
# æ–°ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆç¶šãï¼‰
# =========================
def _analyze_pain_points_simple(notes: str, messages_ctx: str, uploads_text: str = "") -> list[dict[str, Any]]:
    """
    ç°¡æ˜“ç‰ˆèª²é¡Œåˆ†æï¼ˆLLMã‚’ä½¿ç”¨ã—ãªã„ï¼‰
    å•†è«‡ãƒ¡ãƒ¢ãƒ»ä¼šè©±æ–‡è„ˆãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã‹ã‚‰åŸºæœ¬çš„ãªèª²é¡Œã‚’æŠ½å‡º
    """
    issues = []
    
    # åŸºæœ¬çš„ãªèª²é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³
    basic_issues = [
        {"issue": "æ¥­å‹™åŠ¹ç‡åŒ–", "weight": 0.4, "keywords": ["åŠ¹ç‡åŒ–", "è‡ªå‹•åŒ–", "ç”Ÿç”£æ€§"]},
        {"issue": "ã‚³ã‚¹ãƒˆæœ€é©åŒ–", "weight": 0.3, "keywords": ["è²»ç”¨å‰Šæ¸›", "æœ€é©åŒ–", "ã‚³ã‚¹ãƒˆ"]},
        {"issue": "æƒ…å ±ç®¡ç†æ”¹å–„", "weight": 0.3, "keywords": ["æƒ…å ±å…±æœ‰", "ç®¡ç†", "ã‚·ã‚¹ãƒ†ãƒ "]}
    ]
    
    # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã«åŸºã¥ã„ã¦é‡ã¿ã‚’èª¿æ•´
    all_text = f"{notes} {messages_ctx} {uploads_text}".lower()
    
    if "åŠ¹ç‡" in all_text or "ç”Ÿç”£æ€§" in all_text:
        basic_issues[0]["weight"] = 0.5
        basic_issues[1]["weight"] = 0.25
        basic_issues[2]["weight"] = 0.25
    elif "ã‚³ã‚¹ãƒˆ" in all_text or "è²»ç”¨" in all_text:
        basic_issues[0]["weight"] = 0.25
        basic_issues[1]["weight"] = 0.5
        basic_issues[2]["weight"] = 0.25
    elif "æƒ…å ±" in all_text or "ç®¡ç†" in all_text:
        basic_issues[0]["weight"] = 0.25
        basic_issues[1]["weight"] = 0.25
        basic_issues[2]["weight"] = 0.5
    
    return basic_issues


# =========================
# å€™è£œæ¤œç´¢ï¼ˆç°¡æ˜“ç‰ˆï¼‰
# =========================
def _search_product_candidates(
    company: str,
    item_id: str | None,
    meeting_notes: str,
    top_k: int,
    history_n: int,
    dataset: str,
    uploaded_files: list[Any],   # ã“ã“ã‚’æ´»ç”¨ï¼ˆè³‡æ–™ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼‰
    issues_precomputed: list[dict[str, Any]] | None = None,   # â† è¿½åŠ 
) -> list[dict[str, Any]]:
    # ä¼æ¥­åˆ†æã®æ–‡è„ˆ
    ctx = _gather_messages_context(item_id, history_n)

    # CSV èª­ã¿è¾¼ã¿
    df = _load_products_from_csv(dataset)
    if df.empty:
        return []

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    uploads_text = _extract_text_from_uploads(uploaded_files) if uploaded_files else ""

    # äº‹å‰ã«è¨ˆç®—æ¸ˆã¿ã®èª²é¡ŒãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€‚ãªã‘ã‚Œã°ã“ã“ã§æŠ½å‡ºã€‚
    if issues_precomputed is not None:
        issues = issues_precomputed
    else:
        issues = _analyze_pain_points(meeting_notes or "", ctx or "", uploads_text)

    # èªå¥ä¸€è‡´ã§ç²—é¸å®š
    top_pool = max(40, top_k * 4)
    pool = _fallback_rank_products(meeting_notes, ctx, df, top_pool=top_pool)

    # ä¸Šä½Kä»¶ã‚’é¸æŠ
    selected = pool[:top_k]
    
    # å„è£½å“ã®æ¦‚è¦ã‚’ç”Ÿæˆ
    for product in selected:
        product["overview"] = _generate_product_overview(product)
    
    return selected


# =========================
# ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆ
# =========================
def _make_outline_preview(company: str, meeting_notes: str, selected_products: list[dict[str, Any]], overview: str) -> dict[str, Any]:
    return {
        "title": f"{company} å‘ã‘ææ¡ˆè³‡æ–™ï¼ˆãƒ‰ãƒ©ãƒ•ãƒˆï¼‰",
        "overview": overview,
        "sections": [
            {"h2": "1. ã‚¢ã‚¸ã‚§ãƒ³ãƒ€", "bullets": ["èƒŒæ™¯", "èª²é¡Œæ•´ç†", "ææ¡ˆæ¦‚è¦", "å°å…¥åŠ¹æœ", "å°å…¥è¨ˆç”»", "æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"]},
            {"h2": "2. èƒŒæ™¯", "bullets": [f"{company}ã®äº‹æ¥­æ¦‚è¦ï¼ˆè¦ç´„ï¼‰", "å¸‚å ´å‹•å‘ãƒ»ç«¶åˆçŠ¶æ³ï¼ˆæŠœç²‹ï¼‰"]},
            {"h2": "3. ç¾çŠ¶ã®èª²é¡Œï¼ˆä»®èª¬ï¼‰", "bullets": ["ç”Ÿç”£æ€§/ã‚³ã‚¹ãƒˆ/å“è³ª/ã‚¹ãƒ”ãƒ¼ãƒ‰ã®è¦³ç‚¹ã‹ã‚‰3ã€œ5ç‚¹"]},
            {"h2": "4. ææ¡ˆæ¦‚è¦", "bullets": ["æœ¬ææ¡ˆã®ç›®çš„ï¼ç‹™ã„", "å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆé«˜ãƒ¬ãƒ™ãƒ«ï¼‰"]},
            {"h2": "5. æ¨å¥¨ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå€™è£œï¼‰", "bullets": [f"{len(selected_products)}ä»¶ã®å•†æå€™è£œã‚’æ•´ç†ãƒ»æ¯”è¼ƒ"]},
            {"h2": "6. å°å…¥åŠ¹æœï¼ˆå®šé‡/å®šæ€§ï¼‰", "bullets": ["KPIè¦‹è¾¼ã¿ / åŠ¹æœè©¦ç®—ã®æ–¹é‡"]},
            {"h2": "7. å°å…¥ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ¡ˆ", "bullets": ["PoC â†’ æœ¬å°å…¥ / ä½“åˆ¶ãƒ»å½¹å‰²åˆ†æ‹…"]},
        ],
        "meeting_notes_digest": meeting_notes[:300] + ("..." if len(meeting_notes) > 300 else ""),
        "products": [
            {
                "id": p.get("id"),
                "name": p.get("name"),
                "category": p.get("source_csv") or p.get("category"),
                "price": p.get("price"),
                "reason": p.get("reason"),
                "overview": p.get("overview"),
            }
            for p in selected_products
        ],
    }

# --- æ®µéšè¡¨ç¤ºç”¨ãƒ¬ãƒ³ãƒ€ãƒ© ---
def _render_issues_block(issues: list[dict[str, Any]], placeholder):
    placeholder.empty()
    with placeholder.container():
        st.markdown("**â— èª²é¡Œåˆ†æçµæœ**")
        if not issues:
            st.caption("ã€ææ¡ˆå•†å“ã‚’å‡ºåŠ›ã€ã‚’æŠ¼ã™ã¨ã€å•†è«‡ãƒ¡ãƒ¢ãƒ»å±¥æ­´ãƒ»å‚è€ƒè³‡æ–™ã‹ã‚‰èª²é¡Œã‚’è‡ªå‹•æŠ½å‡ºã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")
            return
        for i, it in enumerate(issues, start=1):
            with st.container(border=True):
                st.markdown(f"**{i}. {it.get('issue','â€”')}**")
                st.caption(f"é‡ã¿: {it.get('weight',0):.2f}")
                kws = it.get("keywords") or []
                if kws:
                    st.markdown("é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: " + " / ".join(kws))

def _render_candidates_block(recs: list[dict[str, Any]], placeholder):
    placeholder.empty()
    with placeholder.container():
        st.markdown("**â— ææ¡ˆå•†å“ä¸€è¦§**")
        if not recs:
            st.info("ææ¡ˆå•†å“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€ææ¡ˆå•†å“ã‚’å‡ºåŠ›ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
            return
        for r in recs:
            pid = str(r.get("id") or "")
            name = str(r.get("name") or "")
            cat_src = r.get("source_csv") or r.get("category") or "â€”"
            price_s = _fmt_price(r.get("price"))
            reason = r.get("reason") or "â€”"
            overview = r.get("overview") or "â€”"
            with st.container(border=True):
                c1, c2 = st.columns([1, 3], gap="medium")
                with c1:
                    img_src = _resolve_product_image_src(r)
                    if img_src and (img_src.startswith("http") or os.path.exists(img_src)):
                        st.image(img_src, use_container_width=True)
                    else:
                        st.markdown(
                            "<div style='width:100%;height:120px;border:1px solid #eee;border-radius:10px;"
                            "background:#f6f7f9;display:flex;align-items:center;justify-content:center;"
                            "color:#999;font-size:12px;'>ç”»åƒãªã—</div>",
                            unsafe_allow_html=True
                        )
                with c2:
                    st.markdown(f"**{name}**")
                    st.caption(f"ã‚«ãƒ†ã‚´ãƒª: {cat_src} ï¼ ä¾¡æ ¼: {price_s} ï¼ ID: `{pid}`")
                    st.markdown(f"**ææ¡ˆç†ç”±**ï¼š{reason}")
                    st.markdown(f"**è£½å“æ¦‚è¦**ï¼š{overview}")


# --- ææ¡ˆä¿å­˜ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆæ—¢å­˜ app.db ã‚’æ´»ç”¨ï¼‰ ---
import sqlite3, uuid, json as _json
DB_PATH = PROJECT_ROOT / "data" / "sqlite" / "app.db"  # â† ã“ã“ã ã‘ãƒ‘ã‚¹å¤‰æ›´

def _init_db_for_proposals():
    DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("""CREATE TABLE IF NOT EXISTS proposals(
            id TEXT PRIMARY KEY,
            project_item_id TEXT,
            company TEXT NOT NULL,
            meeting_notes TEXT,
            overview TEXT,
            created_at TEXT NOT NULL
        )""")
        c.execute("""CREATE TABLE IF NOT EXISTS proposal_issues(
            proposal_id TEXT NOT NULL,
            idx INTEGER NOT NULL,
            issue TEXT NOT NULL,
            weight REAL,
            keywords_json TEXT,
            FOREIGN KEY(proposal_id) REFERENCES proposals(id)
        )""")
        c.execute("""CREATE TABLE IF NOT EXISTS proposal_products(
            proposal_id TEXT NOT NULL,
            rank INTEGER NOT NULL,
            product_id TEXT,
            name TEXT,
            category TEXT,
            price TEXT,
            reason TEXT,
            overview TEXT,
            score REAL,
            source_csv TEXT,
            image_url TEXT,
            FOREIGN KEY(proposal_id) REFERENCES proposals(id)
        )""")
        conn.commit()

def _save_proposal_to_db(
    project_item_id: str | None,
    company: str,
    meeting_notes: str,
    overview: str,
    issues: list[dict[str, Any]],
    products: list[dict[str, Any]],
    created_at_iso: str
) -> str:
    """ææ¡ˆã²ã¨ã¾ã¨ã¾ã‚Šã‚’ä¿å­˜ã—ã€proposal_id ã‚’è¿”ã™ï¼ˆæ—¢å­˜ app.db ã«è¿½è¨˜ï¼‰ã€‚"""
    _init_db_for_proposals()
    pid = str(uuid.uuid4())
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute(
            "INSERT INTO proposals(id, project_item_id, company, meeting_notes, overview, created_at) VALUES(?,?,?,?,?,?)",
            (pid, project_item_id, company, meeting_notes, overview, created_at_iso)
        )
        # èª²é¡Œã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        for i, it in enumerate(issues or []):
            c.execute(
                "INSERT INTO proposal_issues(proposal_id, idx, issue, weight, keywords_json) VALUES(?,?,?,?,?)",
                (pid, i+1, it.get('issue',''), float(it.get('weight') or 0.0), _json.dumps(it.get('keywords') or [], ensure_ascii=False))
            )
        # æ¡ç”¨è£½å“ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        for r, p in enumerate(products or []):
            c.execute(
                """INSERT INTO proposal_products(
                    proposal_id, rank, product_id, name, category, price, reason, overview, score, source_csv, image_url
                ) VALUES(?,?,?,?,?,?,?,?,?,?,?)""",
                (
                    pid, r+1,
                    str(p.get('id','')) or None,
                    p.get('name',''),
                    p.get('source_csv') or p.get('category',''),
                    str(p.get('price','')),
                    p.get('reason',''),
                    p.get('overview',''),
                    float(p.get('score') or 0.0),
                    p.get('source_csv') or '',
                    p.get('image_url') or ''
                )
            )
        conn.commit()
    return pid




# =========================
# ãƒ¡ã‚¤ãƒ³æç”»ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆæ”¹ä¿®ã®ã¿ï¼‰
# =========================
def render_slide_generation_page():
    """ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆãƒšãƒ¼ã‚¸ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆé…ç½®ã‚’å¤‰æ›´ï¼šä¸Šæ®µï¼å•†è«‡è©³ç´°ï¼†è³‡æ–™ã€ä¸‹æ®µï¼èª²é¡Œåˆ†æï¼†ææ¡ˆå•†å“ã‚«ãƒ¼ãƒ‰ï¼‰"""
    _ensure_session_defaults()

    try:
        st.set_page_config(
            page_title="ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ",
            page_icon=str(ICON_PATH),
            layout="wide",
            initial_sidebar_state="expanded",
        )
    except Exception:
        pass

    # ã‚¹ã‚¿ã‚¤ãƒ«
    apply_main_styles(hide_sidebar=False, hide_header=True)
    apply_title_styles()
    apply_company_analysis_page_styles()  # ã‚µã‚¤ãƒ‰ãƒãƒ¼å…±é€š
    apply_slide_generation_page_styles()  # ã‚¿ã‚¤ãƒˆãƒ«ä½ç½®ãªã©

    # æ¡ˆä»¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    pj = st.session_state.get("selected_project")
    if pj:
        title_text = f"ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ - {pj['title']} / {pj['company']}"
        company_internal = pj.get("company", "")
        item_id = pj.get("id")
    else:
        title_text = "ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ"
        company_internal = ""
        item_id = None

    # ---------- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ----------
    with st.sidebar:
        render_sidebar_logo_card(LOGO_PATH)

        st.markdown("### è¨­å®š")
        st.text_input("ä¼æ¥­å", value=company_internal, key="slide_company_input", disabled=True)

        # --- ã“ã“ã‹ã‚‰â€œæ•°å­—ã¯ã™ã¹ã¦ä¸€è¦§é¸æŠå‹â€ & Session State æ—¢å®šå€¤ã®ã¿ä½¿ç”¨ ---
        st.selectbox("ææ¡ˆä»¶æ•°", options=list(range(1, 11)), key="slide_top_k")

        st.selectbox(
            "å±¥æ­´å‚ç…§ä»¶æ•°ï¼ˆå¾€å¾©ï¼‰",
            options=list(range(1, 11)),
            key="slide_history_reference_count",
            help="ä¼æ¥­åˆ†æã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ç›´è¿‘Nå¾€å¾©ã‚’æ–‡è„ˆã¨ã—ã¦ä½¿ç”¨",
        )

        st.selectbox(
            "å•†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            options=_list_product_datasets(),
            key="slide_products_dataset",
            help="data/csv/products/ é…ä¸‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã€‚Autoã¯è‡ªå‹•é¸æŠã€‚",
        )

        st.markdown("---")
        st.markdown("### AIè¨­å®š")
        
        st.checkbox("GPT APIä½¿ç”¨", key="slide_use_gpt_api")
        st.checkbox("TAVILY APIä½¿ç”¨", key="slide_use_tavily_api")

        if st.session_state.slide_use_tavily_api:
            st.selectbox("TAVILY APIå‘¼ã³å‡ºã—å›æ•°ï¼ˆè£½å“ã‚ãŸã‚Šï¼‰", options=list(range(1, 6)), key="slide_tavily_uses")

        sidebar_clear = st.button("ã‚¯ãƒªã‚¢", use_container_width=True, help="ææ¡ˆå•†å“ã‚’ç”»é¢å†…ã§ã‚¯ãƒªã‚¢")

        st.markdown("<div class='sidebar-bottom'>", unsafe_allow_html=True)
        if st.button("â† æ¡ˆä»¶ä¸€è¦§ã«æˆ»ã‚‹", use_container_width=True):
            st.session_state.current_page = "æ¡ˆä»¶ä¸€è¦§"
            st.session_state.page_changed = True
            st.rerun()
        st.markdown("</div>", unsafe_allow_html=True)

    # ---------- ã‚¿ã‚¤ãƒˆãƒ« ----------
    render_slide_generation_title(title_text)

    # ---------- è¦‹å‡ºã—è¡Œï¼ˆå·¦ï¼è¦‹å‡ºã— / å³ï¼ææ¡ˆå•†å“å–å¾—ãƒœã‚¿ãƒ³ï¼‰ ----------
    head_l, head_r = st.columns([8, 2])
    with head_l:
        st.subheader("1. ææ¡ˆå•†å“ã®å‡ºåŠ›")
    with head_r:
        search_btn = st.button("ææ¡ˆå•†å“ã‚’å‡ºåŠ›", use_container_width=True)

    # ====================== ä¸Šæ®µï¼šå•†è«‡è©³ç´°ï¼ˆå¤§ï¼‰ï¼†å‚è€ƒè³‡æ–™ï¼ˆæ¨ªä¸¦ã³ï¼‰ ======================
    top_l, top_r = st.columns([3, 2], gap="large")  # å•†è«‡è©³ç´°ã‚’å¤§ãã‚ã«
    with top_l:
        st.markdown("**â— å•†è«‡ãƒ¡ãƒ¢ï¼ˆç›¸æ‰‹ã®èª²é¡Œã‚„è¦æœ›ï¼‰**")
        st.text_area(
            label="å•†è«‡ãƒ¡ãƒ¢",
            key="slide_meeting_notes",
            height=154,
            label_visibility="collapsed",
            placeholder="ä¾‹ï¼šæ¥æœŸã®éœ€è¦äºˆæ¸¬ç²¾åº¦å‘ä¸Šã¨åœ¨åº«æœ€é©åŒ–ã€‚PoCã‹ã‚‰æ®µéšå°å…¥â€¦ ãªã©",
        )
    with top_r:
        st.markdown("**â— å‚è€ƒè³‡æ–™ï¼ˆè­°äº‹éŒ²ç­‰ï¼‰**")
        uploads = st.file_uploader(
            label="å‚è€ƒè³‡æ–™ï¼ˆä»»æ„ï¼‰",
            type=["pdf", "pptx", "docx", "csv", "png", "jpg", "jpeg", "txt"],
            accept_multiple_files=True,
            key="slide_uploader",
            label_visibility="collapsed",
            help="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è³‡æ–™ã¯ç‰¹å¾´æŠ½å‡º/è¦ç´„ã«åˆ©ç”¨ï¼ˆä»Šå›ã®ä¿®æ­£ã§èª²é¡ŒæŠ½å‡ºã«åæ˜ ã•ã‚Œã¾ã™ï¼‰ã€‚",
        )
        if uploads:
            st.session_state.uploaded_files_store = uploads
            st.success(f"{len(uploads)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸã€‚")
        elif st.session_state.uploaded_files_store:
            st.caption(f"å‰å›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {len(st.session_state.uploaded_files_store)} ãƒ•ã‚¡ã‚¤ãƒ«")

    # ã“ã“ã¯ã€Œä¸Šæ®µï¼šå•†è«‡è©³ç´°ï¼†å‚è€ƒè³‡æ–™ã€ã®ç›´å¾Œ
    st.divider()

    # â–¼â–¼ é€²è¡ŒçŠ¶æ³ã®è¡¨ç¤ºå ´æ‰€ã‚’â€œèª²é¡Œåˆ†æçµæœï¼ææ¡ˆå•†å“ä¸€è¦§â€ã®ç›´å‰ã«å›ºå®š
    progress_placeholder = st.empty()

    # ä¸‹æ®µã‚«ãƒ©ãƒ ï¼ˆèª²é¡Œåˆ†æçµæœï¼ææ¡ˆå•†å“ä¸€è¦§ï¼‰
    bottom_l, bottom_r = st.columns([5, 7], gap="large")
    with bottom_l:
        issues_placeholder = st.empty()
    with bottom_r:
        candidates_placeholder = st.empty()

    # åˆæœŸè¡¨ç¤ºï¼ˆå‰å›ã®çŠ¶æ…‹ã‚’åæ˜ ï¼‰
    _render_issues_block(st.session_state.get("analyzed_issues") or [], issues_placeholder)
    _render_candidates_block(st.session_state.get("product_candidates") or [], candidates_placeholder)


    if search_btn:
        if not company_internal.strip():
            st.error("ä¼æ¥­ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ¡ˆä»¶ä¸€è¦§ã‹ã‚‰ä¼æ¥­ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")
        else:
            with progress_placeholder.container():
                with st.status("å•†å“ææ¡ˆã‚’å®Ÿè¡Œä¸­â€¦", expanded=True) as status:
                    status.update(label="1/3 èª²é¡Œã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™")
                    # ã¾ãšã¯èª²é¡Œã ã‘è¨ˆç®—ã—ã¦å³è¡¨ç¤º
                    ctx_for_view = _gather_messages_context(item_id, int(st.session_state.slide_history_reference_count))
                    uploads_text_for_view = _extract_text_from_uploads(st.session_state.uploaded_files_store) if st.session_state.uploaded_files_store else ""
                    issues_early = _analyze_pain_points(
                        st.session_state.slide_meeting_notes or "",
                        ctx_for_view or "",
                        uploads_text_for_view or ""
                    )
                    st.session_state.analyzed_issues = issues_early
                    _render_issues_block(issues_early, issues_placeholder)  # â† å…ˆã«è¡¨ç¤º

                    status.update(label="2/3 ã‚«ã‚¿ãƒ­ã‚°ã¨ç…§åˆã—ã¦ææ¡ˆå•†å“ã‚’é¸å®šä¸­")
                    # äº‹å‰è¨ˆç®—ã—ãŸèª²é¡Œã‚’æµç”¨ã—ã¦ææ¡ˆå•†å“ã‚’æ¤œç´¢
                    candidates = _search_product_candidates(
                        company=company_internal,
                        item_id=item_id,
                        meeting_notes=st.session_state.slide_meeting_notes or "",
                        top_k=int(st.session_state.slide_top_k),
                        history_n=int(st.session_state.slide_history_reference_count),
                        dataset=st.session_state.slide_products_dataset,
                        uploaded_files=st.session_state.uploaded_files_store,
                        issues_precomputed=issues_early,  # â† ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆ
                    )
                    st.session_state.product_candidates = candidates

                    status.update(label="3/3 ææ¡ˆå•†å“ã‚«ãƒ¼ãƒ‰ã‚’æç”»ã—ã¦ã„ã¾ã™")
                    _render_candidates_block(candidates, candidates_placeholder)

                    status.update(state="complete", label="ææ¡ˆå•†å“ã®æŠ½å‡ºãŒå®Œäº†ã—ã¾ã—ãŸ")

        # ï¼ˆä»»æ„ï¼‰ã“ã“ã§ãƒ‰ãƒ©ãƒ•ãƒˆä¿å­˜ã¯å¾“æ¥ã©ãŠã‚Š
        try:
            proposal_id = _save_proposal_to_db(
                project_item_id=item_id,
                company=company_internal,
                meeting_notes=st.session_state.slide_meeting_notes or "",
                overview=st.session_state.slide_overview or "",
                issues=st.session_state.analyzed_issues or [],
                products=st.session_state.product_candidates or [],
                created_at_iso=datetime.now().isoformat(timespec="seconds")
            )
            st.session_state["last_proposal_id"] = proposal_id
            st.info(f"ãƒ‰ãƒ©ãƒ•ãƒˆææ¡ˆã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ˆID: {proposal_id[:8]}â€¦ï¼‰")
        except Exception as e:
            st.warning(f"ãƒ‰ãƒ©ãƒ•ãƒˆä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


    if sidebar_clear:
        st.session_state.product_candidates = []
        st.session_state.analyzed_issues = []
        st.info("ææ¡ˆå•†å“ã¨èª²é¡Œåˆ†æè¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

    st.divider()


    # åˆæœŸè¡¨ç¤ºï¼ˆå‰å›ã®çŠ¶æ…‹ã‚’åæ˜ ï¼‰
    _render_issues_block(st.session_state.get("analyzed_issues") or [], issues_placeholder)
    _render_candidates_block(st.session_state.get("product_candidates") or [], candidates_placeholder)

    st.divider()

    # ====================== 2. ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ ======================
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æ·»ä»˜
    tmpl_file = st.file_uploader(
        "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆ.pptxï¼‰ã‚’æ·»ä»˜ï¼ˆä»»æ„ï¼‰",
        type=["pptx"],
        key="slide_template_uploader",
        help="æ·»ä»˜ãŒç„¡ã„å ´åˆã¯æ—¢å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™"
    )

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¸ä¿æŒ
    if tmpl_file is not None:
        st.session_state.slide_template_bytes = tmpl_file.getvalue()
        st.session_state.slide_template_name = tmpl_file.name
        st.success(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸï¼š{tmpl_file.name}")
    else:
        # æ—¢ã«å‰å›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰åå‰ã ã‘è¡¨ç¤º
        current = st.session_state.get("slide_template_name")
        if current:
            st.caption(f"ç¾åœ¨ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼š{current}ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‚’ä½¿ç”¨ï¼‰")
        else:
            st.caption("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæœªæ·»ä»˜ï¼šæ—¢å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")

    st.subheader("2. ææ¡ˆã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ")

    row_l, row_r = st.columns([8, 2], vertical_alignment="center")
    with row_l:
        st.session_state.slide_overview = st.text_input(
            "æ¦‚èª¬ï¼ˆä»»æ„ï¼‰",
            value=st.session_state.slide_overview or "",
            placeholder="ä¾‹ï¼šåœ¨åº«æœ€é©åŒ–ã‚’ä¸­å¿ƒã«ã€éœ€è¦äºˆæ¸¬ã¨è£œå……è¨ˆç”»ã®é€£æºã‚’ææ¡ˆâ€¦",
        )
    with row_r:
        gen_btn = st.button("ç”Ÿæˆ", type="primary", use_container_width=True)

    if gen_btn:
        if not company_internal.strip():
            st.error("ä¼æ¥­ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        elif not st.session_state.product_candidates:
            st.error("ææ¡ˆå•†å“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã€Œææ¡ˆå•†å“ã‚’å‡ºåŠ›ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        else:
            selected = list(st.session_state.product_candidates or [])  # å…¨ææ¡ˆå•†å“ã‚’æ¡ç”¨
            
            # ä¸‹æ›¸ãã®ä½œæˆ
            outline = _make_outline_preview(
                company_internal,
                st.session_state.slide_meeting_notes or "",
                selected,
                st.session_state.slide_overview or "",
            )
            st.session_state.slide_outline = outline
            
            # ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
            with st.spinner("AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆä¸­..."):
                try:
                    print("ğŸš€ Streamlit: ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆé–‹å§‹")
                    print(f"  ä¼æ¥­å: {company_internal}")
                    print(f"  è£½å“æ•°: {len(selected)}")
                    print(f"  GPT API: {st.session_state.slide_use_gpt_api}")
                    print(f"  TAVILY API: {st.session_state.slide_use_tavily_api}")
                    print(f"  TAVILYä½¿ç”¨å›æ•°: {st.session_state.slide_tavily_uses}")
                    
                    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®å–å¾—
                    print("ğŸ“š ãƒãƒ£ãƒƒãƒˆå±¥æ­´å–å¾—ä¸­...")
                    chat_history = _gather_messages_context(
                        item_id, 
                        st.session_state.slide_history_reference_count
                    )
                    print(f"  ãƒãƒ£ãƒƒãƒˆå±¥æ­´é•·: {len(chat_history)}æ–‡å­—")
                    print("ğŸ¤– NewSlideGeneratoråˆæœŸåŒ–ä¸­.")

                    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ†ãƒ³ãƒ—ãƒ¬ãŒã‚ã‚Œã°ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ä½¿ç”¨
                    uploaded_template_path = None
                    try:
                        if st.session_state.get("slide_template_bytes"):
                            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pptx")
                            tmp.write(st.session_state["slide_template_bytes"])
                            tmp.flush()
                            tmp.close()
                            uploaded_template_path = tmp.name
                            print(f"  ğŸ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ä½¿ç”¨: {uploaded_template_path}")

                        generator = (NewSlideGenerator(template_path=uploaded_template_path) if uploaded_template_path else NewSlideGenerator())
                        print("âœ… NewSlideGeneratoråˆæœŸåŒ–å®Œäº†")
                    finally:
                        if uploaded_template_path and os.path.exists(uploaded_template_path):
                            try:
                                os.remove(uploaded_template_path)
                            except Exception:
                                pass
                    
                    # ææ¡ˆèª²é¡Œã®å–å¾—
                    print("ğŸ” ææ¡ˆèª²é¡Œå–å¾—ä¸­...")
                    proposal_issues = []
                    if st.session_state.get("last_proposal_id"):
                        proposal_issues = _get_proposal_issues_from_db(st.session_state["last_proposal_id"])
                    else:
                        proposal_issues = st.session_state.get("analyzed_issues", [])
                    
                    print(f"  ææ¡ˆèª²é¡Œæ•°: {len(proposal_issues)}")
                    
                    print("ğŸ¯ ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Ÿè¡Œä¸­...")
                    pptx_data = generator.create_presentation(
                        project_name=company_internal,  # æ¡ˆä»¶åã¨ã—ã¦ä¼æ¥­åã‚’ä½¿ç”¨
                        company_name=company_internal,
                        meeting_notes=st.session_state.slide_meeting_notes or "",
                        chat_history=chat_history,
                        products=selected,
                        proposal_issues=proposal_issues,
                        use_tavily=st.session_state.slide_use_tavily_api,
                        use_gpt=st.session_state.slide_use_gpt_api,
                        tavily_uses=st.session_state.slide_tavily_uses
                    )
                    
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®è¡¨ç¤º
                    print("âœ… ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Œäº†")
                    print(f"  ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(pptx_data)} ãƒã‚¤ãƒˆ")
                    
                    st.success("ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"{company_internal}_ææ¡ˆæ›¸_{timestamp}.pptx"
                    
                    st.download_button(
                        label="ğŸ“¥ ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=pptx_data,
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                        use_container_width=True,
                        type="primary"
                    )
                    
                except Exception as e:
                    print(f"âŒ Streamlit: ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
                    print(f"âŒ Streamlit: ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
                    st.error(f"ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.info("ä¸‹æ›¸ãã®ã¿ä½œæˆã•ã‚Œã¾ã—ãŸã€‚")

    if st.session_state.slide_outline:
        with st.expander("ä¸‹æ›¸ããƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆJSONï¼‰", expanded=True):
            st.json(st.session_state.slide_outline)
